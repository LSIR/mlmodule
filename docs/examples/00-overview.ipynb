{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoZuMa overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mozuma.torch.callbacks import TorchRunnerCallbackType\n",
    "from mozuma.torch.datasets import ListDatasetIndexed, TorchDataset\n",
    "from mozuma.torch.modules import TorchModel\n",
    "from mozuma.torch.runners import TorchInferenceRunner\n",
    "from mozuma.torch.options import TorchRunnerOptions\n",
    "\n",
    "\n",
    "def run_torch_model_inference(\n",
    "    model: TorchModel,\n",
    "    callbacks: \"list[TorchRunnerCallbackType]\",\n",
    "    dataset: \"TorchDataset | None\" = None,\n",
    ") -> None:\n",
    "    \"\"\"Runs inference for a PyTorch model\"\"\"\n",
    "    # Setting the dataset to images if not defined\n",
    "    dataset = dataset or ListDatasetIndexed(indices=IMAGE_URLS, objects=images_objects)\n",
    "\n",
    "    runner = TorchInferenceRunner(\n",
    "        model=model,\n",
    "        dataset=dataset,\n",
    "        callbacks=callbacks,\n",
    "        options=TorchRunnerOptions(\n",
    "            device=torch.device(\"cpu\"), data_loader_options={\"batch_size\": 20}\n",
    "        ),\n",
    "    )\n",
    "    runner.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic function to compute features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mozuma.callbacks import CollectFeaturesInMemory\n",
    "\n",
    "\n",
    "def collect_features(\n",
    "    model: TorchModel, dataset: \"TorchDataset | None\" = None\n",
    ") -> npt.NDArray[np.float_]:\n",
    "    features = CollectFeaturesInMemory()\n",
    "    run_torch_model_inference(model=model, callbacks=[features], dataset=dataset)\n",
    "    if dataset is None:\n",
    "        assert features.indices == IMAGE_URLS, features.indices\n",
    "    return features.features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find an image from a text query\n",
    "\n",
    "<p style=\"text-align: center;\">A dog at the beach</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add noise with dog images\n",
    "from mozuma.models.clip.pretrained import (\n",
    "    torch_clip_image_encoder,\n",
    "    torch_clip_text_encoder,\n",
    ")\n",
    "from mozuma.torch.datasets import ListDataset\n",
    "\n",
    "# See https://mozuma.github.io/mozuma/examples/overview/\n",
    "clip_image_features = collect_features(model=torch_clip_image_encoder(\"ViT-B/32\"))\n",
    "clip_text_features = collect_features(\n",
    "    model=torch_clip_text_encoder(\"ViT-B/32\"),\n",
    "    dataset=ListDataset([\"a dog at the beach\"]),\n",
    ")\n",
    "display_images(\n",
    "    arg_rank_by_cosine_similarity(clip_text_features, clip_image_features, take=1), img_width=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic function to compute bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mozuma.callbacks import CollectBoundingBoxesInMemory\n",
    "\n",
    "\n",
    "def collect_bbox(\n",
    "    model: TorchModel,\n",
    ") -> \"tuple[npt.NDArray[np.str_], npt.NDArray[np.float_], npt.NDArray[np.float_]]\":\n",
    "    bbox = CollectBoundingBoxesInMemory()\n",
    "    run_torch_model_inference(model=model, callbacks=[bbox])\n",
    "    assert bbox.indices == IMAGE_URLS, bbox.indices\n",
    "    # Flattening the bounding boxes\n",
    "    indices: \"list[str]\" = []\n",
    "    features: \"list[npt.NDArray[np.float_]]\" = []\n",
    "    boxes: \"list[npt.NDArray[np.float_]]\" = []\n",
    "    for index, box_list in zip(bbox.indices, bbox.bounding_boxes):\n",
    "        indices += [index] * len(box_list.bounding_boxes)\n",
    "        boxes.append(box_list.bounding_boxes)\n",
    "        if box_list.features is None:\n",
    "            raise ValueError(\"This model does not returned features\")\n",
    "        features.append(box_list.features)\n",
    "    return np.array(indices, dtype=str), np.vstack(boxes), np.vstack(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find images with similar objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mozuma.models.vinvl.pretrained import torch_vinvl_detector\n",
    "\n",
    "bbox_indices, bbox_boxes, bbox_features = collect_bbox(model=torch_vinvl_detector())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Find an image of a paddle\n",
    "paddle_coordinates = np.array([ 899.95416,  581.6102 , 1105.5442 ,  640.5274 ])\n",
    "paddle_box_index = np.argmin(cdist(paddle_coordinates[np.newaxis], bbox_boxes[bbox_indices == KAYAK_IMAGE]))\n",
    "paddle_bounding_box = bbox_boxes[bbox_indices == KAYAK_IMAGE][paddle_box_index]\n",
    "paddle_features = bbox_features[bbox_indices == KAYAK_IMAGE][paddle_box_index]\n",
    "bbox_features[IMAGE_URLS]\n",
    "\n",
    "display_crops([IMAGE_URLS.index(KAYAK_IMAGE)], paddle_bounding_box[np.newaxis], img_width=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similar objects\n",
    "top_matching_objects = arg_rank_by_cosine_similarity(paddle_features, bbox_features, take=12)[1:]\n",
    "top_matching_objects_image_urls = bbox_indices[top_matching_objects]\n",
    "\n",
    "display_crops([IMAGE_URLS.index(img_url) for img_url in top_matching_objects_image_urls], bbox_boxes[top_matching_objects, :], img_width=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(cosine_similarity(paddle_features, bbox_features))[-12:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# places + kayak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mozuma.models.densenet.pretrained import torch_densenet_places365\n",
    "\n",
    "densenet_places_features = collect_features(model=torch_densenet_places365())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([IMAGE_URLS.index(SEA_CAVE_IMAGE)], img_width=500)\n",
    "# TODO add more cave images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find images with an object that looks like a paddle with threshold 0.5 on cosine similarity\n",
    "paddle_objects = cosine_similarity(paddle_features, bbox_features) > 0.5\n",
    "image_urls_with_paddles = set(bbox_indices[paddle_objects])\n",
    "\n",
    "# Ranking image with paddles with the places365 similarity\n",
    "display_images([next(\n",
    "    img_idx\n",
    "    for img_idx in arg_rank_by_cosine_similarity(\n",
    "        densenet_places_features[IMAGE_URLS.index(SEA_CAVE_IMAGE)],\n",
    "        densenet_places_features,\n",
    "    )\n",
    "    if IMAGE_URLS[img_idx] in image_urls_with_paddles\n",
    ")], img_width=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "446fa4ae14fc0db07deeb22d284bdd26ac4d0fa6060970ce5f4cfd209622d30f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
