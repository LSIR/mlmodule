{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from facenet_pytorch.models.utils.detect_face import crop_resize\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mlmodule.box import BBoxOutput\n",
    "from mlmodule.torch.data.box import BoundingBoxDataset\n",
    "from mlmodule.contrib.mtcnn import MTCNNDetector\n",
    "from mlmodule.contrib.magface import MagFaceFeatures\n",
    "from mlmodule.torch.data.images import ImageDataset\n",
    "from mlmodule.utils import list_files_in_dir\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%env AWS_ACCESS_KEY_ID = <please your key id here>\n",
    "%env AWS_SECRET_ACCESS_KEY = <please your secret access key here>"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load models\n",
    "device = torch.device('cuda:0')\n",
    "magface = MagFaceFeatures(device=device).load()\n",
    "mtcnn = MTCNNDetector(device=device, image_size=(720,1080), min_face_size=20).load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run face detection first\n",
    "base_path = \"../tests/fixtures/remi_faces\"\n",
    "file_names = list_files_in_dir(base_path, allowed_extensions=('jpg',))\n",
    "dataset = ImageDataset(file_names)\n",
    "# Detect faces first\n",
    "file_names, outputs = mtcnn.bulk_inference(dataset)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Flattening all detected faces\n",
    "bboxes: List[BBoxOutput]\n",
    "indices: List[str]\n",
    "indices, file_names, bboxes = zip(*[\n",
    "    (f'{fn}_{i}', fn, bbox) for fn, bbox_list in zip(file_names, outputs) for i, bbox in enumerate(bbox_list)\n",
    "])\n",
    "# Create a dataset for the bounding boxes\n",
    "bbox_features = BoundingBoxDataset(indices, file_names, bboxes)\n",
    "\n",
    "# Get face features\n",
    "d_indices, features = magface.bulk_inference(\n",
    "    bbox_features,\n",
    "    remove_bad_quality_faces=False,\n",
    "    data_loader_options={'batch_size': 12,\n",
    "                         'num_workers': 0, 'pin_memory': True},\n",
    "    tqdm_enabled=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show face quality using feature magnitudes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def image_grid(array, ncols=10):\n",
    "    index, height, width, channels = array.shape\n",
    "    nrows = index//ncols\n",
    "\n",
    "    img_grid = (array.reshape(nrows, ncols, height, width, channels)\n",
    "                .swapaxes(1, 2)\n",
    "                .reshape(height*nrows, width*ncols, channels))\n",
    "\n",
    "    return img_grid\n",
    "\n",
    "def display_faces_with_magnitude(features, file_names, bboxes, ncols=10):\n",
    "    # compute feature magnitudes\n",
    "    mags = torch.linalg.norm(torch.tensor(features), dim=1)\n",
    "    sort_idx = torch.argsort(mags)\n",
    "\n",
    "    img_arr = []\n",
    "    for ele in sort_idx:\n",
    "        img = Image.open(file_names[ele])\n",
    "        box = np.array([bboxes[ele].bounding_box[0].x,\n",
    "                    bboxes[ele].bounding_box[0].y, bboxes[ele].bounding_box[1].x,\n",
    "                    bboxes[ele].bounding_box[1].y])\n",
    "        cropped_face = np.asarray(crop_resize(img, box, image_size=112))\n",
    "        img_arr.append(cropped_face)\n",
    "\n",
    "    if len(img_arr)%ncols:\n",
    "        for i in range(len(img_arr), (len(img_arr)//ncols+1)*ncols):\n",
    "            img_arr.append(255 * np.ones((112, 112, 3), np.uint8))\n",
    "\n",
    "    result = image_grid(np.array(img_arr), ncols=ncols)\n",
    "    fig = plt.figure(figsize=(20., 20.))\n",
    "    plt.imshow(result)\n",
    "    print('feature magnitude: {}'.format([float('{0:.2f}'.format(mags[idx_].item())) for idx_ in sort_idx]))\n",
    "    return sort_idx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sort_idx = display_faces_with_magnitude(features, file_names, bboxes, ncols=11)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face similarity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "normalized_features = torch.nn.functional.normalize(torch.tensor(features))[sort_idx]\n",
    "sim_mat = normalized_features @ normalized_features.T\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax = sns.heatmap(sim_mat, cmap=\"PuRd\", annot=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face quality analysis of faces from Office"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mtcnn = MTCNNDetector(device=device, image_size=(720, 720), min_face_size=20).load()  \n",
    "# run face detection first\n",
    "base_path = \"../tests/fixtures/faces\"\n",
    "file_names = list_files_in_dir(base_path, allowed_extensions=('jpg',))\n",
    "dataset = ImageDataset(file_names)\n",
    "# Detect faces first\n",
    "file_names, outputs = mtcnn.bulk_inference(dataset)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Flattening all detected faces\n",
    "bboxes: List[BBoxOutput]\n",
    "indices: List[str]\n",
    "indices, file_names, bboxes = zip(*[\n",
    "    (f'{fn}_{i}', fn, bbox) for fn, bbox_list in zip(file_names, outputs) for i, bbox in enumerate(bbox_list)\n",
    "])\n",
    "# Create a dataset for the bounding boxes\n",
    "bbox_features = BoundingBoxDataset(indices, file_names, bboxes)\n",
    "\n",
    "# Get face features\n",
    "d_indices, features = magface.bulk_inference(\n",
    "    bbox_features,\n",
    "    remove_bad_quality_faces=False,\n",
    "    data_loader_options={'batch_size': 12,\n",
    "                         'num_workers': 0, 'pin_memory': True},\n",
    "    tqdm_enabled=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sort_idx = display_faces_with_magnitude(features, file_names, bboxes, ncols=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "normalized_features = torch.nn.functional.normalize(torch.tensor(features))[sort_idx]\n",
    "sim_mat = normalized_features @ normalized_features.T\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax = sns.heatmap(sim_mat, cmap=\"PuRd\", annot=True, cbar=False, fmt=\".1f\")\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "355aa20d4ad8dc39dec281b3934cd3d34f60aab58844993f6361175a2b6f9a83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}