{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Overview MLModule is a library containing a collection of machine learning models with standardised interface to load/save weights, run inference and training. It aims at providing high-level abstractions called runners on top of inference and training loops while allowing extensions via callbacks . These callbacks control the way the output of a runner is handled (i.e. features, labels, model weights...). We also try to keep as few dependencies as possible. Meaning models will be mostly implemented from modules available in deep learning frameworks (such as PyTorch or torchvision ). Go ahead to the getting started guide for an overview of MLModule. Features Model zoo PyTorch inference PyTorch training Multi-GPU support Installation From the git repository: pip install git+ssh://git@github.com/LSIR/mlmodule.git For convenience, we ship a base Docker image https://hub.docker.com/repository/docker/lsirepfl/mlmodulekit which contains dependencies that can be hard to install (for instance PyTorch or MMCV). See MLModuleKit .","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#overview","text":"MLModule is a library containing a collection of machine learning models with standardised interface to load/save weights, run inference and training. It aims at providing high-level abstractions called runners on top of inference and training loops while allowing extensions via callbacks . These callbacks control the way the output of a runner is handled (i.e. features, labels, model weights...). We also try to keep as few dependencies as possible. Meaning models will be mostly implemented from modules available in deep learning frameworks (such as PyTorch or torchvision ). Go ahead to the getting started guide for an overview of MLModule.","title":"Overview"},{"location":"#features","text":"Model zoo PyTorch inference PyTorch training Multi-GPU support","title":"Features"},{"location":"#installation","text":"From the git repository: pip install git+ssh://git@github.com/LSIR/mlmodule.git For convenience, we ship a base Docker image https://hub.docker.com/repository/docker/lsirepfl/mlmodulekit which contains dependencies that can be hard to install (for instance PyTorch or MMCV). See MLModuleKit .","title":"Installation"},{"location":"1-getting-started/","text":"Getting started Code-based usage This guide runs through the inference of a PyTorch ResNet model pretrained on imagenet. First, we need to create a dataset of images, for this we will be using the OpenFileDataset . from mlmodule.v2.torch.datasets import OpenImageFileDataset # Getting a dataset of images (1) dataset = OpenImageFileDataset ( paths = [ \"tests/fixtures/cats_dogs/cat_0.jpg\" , \"tests/fixtures/cats_dogs/cat_90.jpg\" ] ) See Datasets for a list of available datasets. Next, we need to load the ResNet PyTorch module specifying the resnet18 architecture. The model is initialised with weights provided by the MLModuleModelStore . from mlmodule.contrib.resnet import TorchResNetModule from mlmodule.v2.states import StateKey from mlmodule.v2.stores import MLModuleModelStore # Model definition (1) resnet = TorchResNetModule ( \"resnet18\" ) # Getting pre-trained model (2) store = MLModuleModelStore () # Getting the state pre-trained on ImageNet (3) store . load ( resnet , StateKey ( state_type = resnet . state_type , training_id = \"imagenet\" ) ) List of all models List of all stores Description of how states are handled is available is state's reference Once we have a model initialized, we need to define what we want to do with it. In this case, we'll run an inference loop using the TorchInferenceRunner . Note that we pass two callbacks to the runner: CollectFeaturesInMemory and CollectLabelsInMemory . They will be called to collect the resulting features and labels for each batch. from mlmodule.v2.helpers.callbacks import ( CollectFeaturesInMemory , CollectLabelsInMemory ) from mlmodule.v2.torch.options import TorchRunnerOptions from mlmodule.v2.torch.runners import TorchInferenceRunner # Creating the callback to collect data (1) features = CollectFeaturesInMemory () labels = CollectLabelsInMemory () # Getting the torch runner for inference (2) runner = TorchInferenceRunner ( model = resnet , dataset = dataset , callbacks = [ features , labels ], options = TorchRunnerOptions ( tqdm_enabled = True ), ) List of available callbacks . List of available runners When defined, the runner can be run and the callback objects will contain the features and labels. # Executing inference runner . run () # Printing the features print ( features . indices , features . features ) # Printing labels print ( labels . indices , labels . labels ) Command-line interface This only works for models accepting images for now. For instance, to run CLIP on all images in a folder: python -m mlmodule.cli run clip.CLIPViTB32ImageEncoder folder/* --batch-size 256 --num-workers 12","title":"Getting started"},{"location":"1-getting-started/#getting-started","text":"","title":"Getting started"},{"location":"1-getting-started/#code-based-usage","text":"This guide runs through the inference of a PyTorch ResNet model pretrained on imagenet. First, we need to create a dataset of images, for this we will be using the OpenFileDataset . from mlmodule.v2.torch.datasets import OpenImageFileDataset # Getting a dataset of images (1) dataset = OpenImageFileDataset ( paths = [ \"tests/fixtures/cats_dogs/cat_0.jpg\" , \"tests/fixtures/cats_dogs/cat_90.jpg\" ] ) See Datasets for a list of available datasets. Next, we need to load the ResNet PyTorch module specifying the resnet18 architecture. The model is initialised with weights provided by the MLModuleModelStore . from mlmodule.contrib.resnet import TorchResNetModule from mlmodule.v2.states import StateKey from mlmodule.v2.stores import MLModuleModelStore # Model definition (1) resnet = TorchResNetModule ( \"resnet18\" ) # Getting pre-trained model (2) store = MLModuleModelStore () # Getting the state pre-trained on ImageNet (3) store . load ( resnet , StateKey ( state_type = resnet . state_type , training_id = \"imagenet\" ) ) List of all models List of all stores Description of how states are handled is available is state's reference Once we have a model initialized, we need to define what we want to do with it. In this case, we'll run an inference loop using the TorchInferenceRunner . Note that we pass two callbacks to the runner: CollectFeaturesInMemory and CollectLabelsInMemory . They will be called to collect the resulting features and labels for each batch. from mlmodule.v2.helpers.callbacks import ( CollectFeaturesInMemory , CollectLabelsInMemory ) from mlmodule.v2.torch.options import TorchRunnerOptions from mlmodule.v2.torch.runners import TorchInferenceRunner # Creating the callback to collect data (1) features = CollectFeaturesInMemory () labels = CollectLabelsInMemory () # Getting the torch runner for inference (2) runner = TorchInferenceRunner ( model = resnet , dataset = dataset , callbacks = [ features , labels ], options = TorchRunnerOptions ( tqdm_enabled = True ), ) List of available callbacks . List of available runners When defined, the runner can be run and the callback objects will contain the features and labels. # Executing inference runner . run () # Printing the features print ( features . indices , features . features ) # Printing labels print ( labels . indices , labels . labels )","title":"Code-based usage"},{"location":"1-getting-started/#command-line-interface","text":"This only works for models accepting images for now. For instance, to run CLIP on all images in a folder: python -m mlmodule.cli run clip.CLIPViTB32ImageEncoder folder/* --batch-size 256 --num-workers 12","title":"Command-line interface"},{"location":"2-develop/","text":"Developer guide Installation Using tox This method requires conda and tox to be installed. Create a development environment: # CPU development tox --devenv venv -e py39 # or CUDA 11.1 tox --devenv venv -e cuda111-py39 The environment can be activated with: conda activate ./venv Using pip This method requires pip to be installed pip install -r requirements.txt # To install MLModule in development mode with all dependencies pip install -e . Testing Testing can be done using tox # CPU testing tox -e py39 # or CUDA 11.1 tox -e cuda111-py39 or with directly using pytest on an environment with all dependencies installed pytest Code quality We use black as formatter. Install pre commit hooks for systematic styling on commits: pip install pre-commit pre-commit install Packaging MLModules is distributed via wheels on the LSIR public assets bucket. A wheel can be created with the build module MLMODULE_BUILD_VERSION = x.y.z python -m build --wheel Requirements Updating requirements should be done in setup.cfg . To update the requirement.txt file run: pip-compile --extra full --upgrade Publish a new version Push the new version to the master branch Add a tag on the branch with the format vX.Y.Z . For instance, v0.1.1 . Follow the guide in dataplatform-infra/build-ml-module to build and upload the new release","title":"Developer guide"},{"location":"2-develop/#developer-guide","text":"","title":"Developer guide"},{"location":"2-develop/#installation","text":"","title":"Installation"},{"location":"2-develop/#using-tox","text":"This method requires conda and tox to be installed. Create a development environment: # CPU development tox --devenv venv -e py39 # or CUDA 11.1 tox --devenv venv -e cuda111-py39 The environment can be activated with: conda activate ./venv","title":"Using tox"},{"location":"2-develop/#using-pip","text":"This method requires pip to be installed pip install -r requirements.txt # To install MLModule in development mode with all dependencies pip install -e .","title":"Using pip"},{"location":"2-develop/#testing","text":"Testing can be done using tox # CPU testing tox -e py39 # or CUDA 11.1 tox -e cuda111-py39 or with directly using pytest on an environment with all dependencies installed pytest","title":"Testing"},{"location":"2-develop/#code-quality","text":"We use black as formatter. Install pre commit hooks for systematic styling on commits: pip install pre-commit pre-commit install","title":"Code quality"},{"location":"2-develop/#packaging","text":"MLModules is distributed via wheels on the LSIR public assets bucket. A wheel can be created with the build module MLMODULE_BUILD_VERSION = x.y.z python -m build --wheel","title":"Packaging"},{"location":"2-develop/#requirements","text":"Updating requirements should be done in setup.cfg . To update the requirement.txt file run: pip-compile --extra full --upgrade","title":"Requirements"},{"location":"2-develop/#publish-a-new-version","text":"Push the new version to the master branch Add a tag on the branch with the format vX.Y.Z . For instance, v0.1.1 . Follow the guide in dataplatform-infra/build-ml-module to build and upload the new release","title":"Publish a new version"},{"location":"3-mlmodulekit/","text":"MLModule Kit Docker images with PyTorch related libraries pre-installed to use MLModule. docker pull lsirepfl/mlmodulekit:<version> Versions List of image tags and installed libraries. All versions are available with the suffix -ca35 . These images are compiled for the CUDA compute ability version 3.5 (see https://en.wikipedia.org/wiki/CUDA), they must be used when using the LSIR PC32 machine. 1 , 1-ca35 Python 3.7 CUDA 11.0 PyTorch 1.7.1 TorchVision 0.8.2 MMCV full 1.3.11 2 Python 3.7 CUDA 11.1 PyTorch 1.9.1 TorchVision 0.10.1 MMCV full 1.3.14 3 Python 3.7 CUDA 11.1 PyTorch 1.9.1 TorchVision 0.10.1","title":"MLModule Kit"},{"location":"3-mlmodulekit/#mlmodule-kit","text":"Docker images with PyTorch related libraries pre-installed to use MLModule. docker pull lsirepfl/mlmodulekit:<version>","title":"MLModule Kit"},{"location":"3-mlmodulekit/#versions","text":"List of image tags and installed libraries. All versions are available with the suffix -ca35 . These images are compiled for the CUDA compute ability version 3.5 (see https://en.wikipedia.org/wiki/CUDA), they must be used when using the LSIR PC32 machine.","title":"Versions"},{"location":"3-mlmodulekit/#1-1-ca35","text":"Python 3.7 CUDA 11.0 PyTorch 1.7.1 TorchVision 0.8.2 MMCV full 1.3.11","title":"1, 1-ca35"},{"location":"3-mlmodulekit/#2","text":"Python 3.7 CUDA 11.1 PyTorch 1.9.1 TorchVision 0.10.1 MMCV full 1.3.14","title":"2"},{"location":"3-mlmodulekit/#3","text":"Python 3.7 CUDA 11.1 PyTorch 1.9.1 TorchVision 0.10.1","title":"3"},{"location":"models/","text":"Models Tip See the the menu on the left for a list of available models The sections below will help with defining a new model. Each section contains the functions a MLModule model class should define to implement a feature. Models state management A model with internal state (weights) should at least implement the ModelWithState protocol. mlmodule.v2.base.models.ModelWithState Protocol of a model with internal state (weights) It defines two functions set_state and get_state . Attributes: Name Type Description state_type StateType Type of the model state, see states for more information. state_type : StateType property readonly Type of the model state See states for more information. get_state ( self ) -> bytes Get the model internal state Returns: Type Description bytes Serialised state as bytes set_state ( self , state : bytes ) -> None Set the model internal state Parameters: Name Type Description Default state bytes Serialised state as bytes required Labels When a model returns label scores, it must define a LabelSet . This should be defined by implementing the ModelWithLabels protocol. mlmodule.v2.base.models.ModelWithLabels Model that predicts scores for labels It defines the get_labels function get_labels ( self ) -> LabelSet Getter for the model's LabelSet Returns: Type Description LabelSet The label set corresponding to returned label scores PyTorch models PyTorch models should be a subclass of TorchMlModule . Note PyTorch models already implement the ModelWithState protocol by default. mlmodule.v2.torch.modules.TorchMlModule Base torch.nn.Module for PyTorch models implemented in MLModule. A valid subclass of TorchMlModule must implement the following method: forward_predictions state_type And can optionally implement: get_dataset_transforms Attributes: Name Type Description device torch.device Mandatory PyTorch device attribute to initialise model. Examples: This would define a simple PyTorch model consisting of fully connected layer. from mlmodule.v2.states import StateType from mlmodule.v2.torch.modules import TorchMlModule from torchvision import transforms class FC ( TorchMlModule [ torch . Tensor ]): def __init__ ( self , device : torch . device = torch . device ( \"cpu\" )): super () . __init__ ( device = device ) self . fc = nn . Linear ( 512 , 512 ) def forward_predictions ( self , batch : torch . Tensor ) -> BatchModelPrediction [ torch . Tensor ]: return BatchModelPrediction ( features = self . fc ( batch )) @property def state_type ( self ) -> StateType : return StateType ( backend = \"pytorch\" , architecture = \"fc512x512\" , ) def get_dataset_transforms ( self ) -> List [ Callable ]: return [ transforms . ToTensor ()] Note This is a generic class taking a _BatchType type argument. This corresponds to the type of data the forward_predictions will receive. It is most likely torch.Tensor state_type : StateType property readonly Identifier for the current's model state architecture Note PyTorch's model architecture should have the pytorch backend Returns: Type Description StateType State architecture object Note This method must be implemented in subclasses forward_predictions ( self , batch : ~ _BatchType ) -> mlmodule . v2 . base . predictions . BatchModelPrediction [ ~ _BatchPredictionArrayType ] Forward pass of the model Applies the module on a batch and returns all potentially interesting data point (features, labels...) Parameters: Name Type Description Default batch _BatchType the batch of data to process required Returns: Type Description BatchModelPrediction[_BatchPredictionArrayType] Prediction object with the keys features , label_scores ... Note This method must be implemented in subclasses get_dataset_transforms ( self ) -> List [ Callable ] Transforms to apply to the input dataset . Note By default, this method returns an empty list (meaning no transformation) but in most cases, this will need to be overridden. Returns: Type Description List[Callable] A list of callables that will be used to transform the input data.","title":"Models"},{"location":"models/#models","text":"Tip See the the menu on the left for a list of available models The sections below will help with defining a new model. Each section contains the functions a MLModule model class should define to implement a feature.","title":"Models"},{"location":"models/#models-state-management","text":"A model with internal state (weights) should at least implement the ModelWithState protocol.","title":"Models state management"},{"location":"models/#mlmodule.v2.base.models.ModelWithState","text":"Protocol of a model with internal state (weights) It defines two functions set_state and get_state . Attributes: Name Type Description state_type StateType Type of the model state, see states for more information.","title":"ModelWithState"},{"location":"models/#mlmodule.v2.base.models.ModelWithState.state_type","text":"Type of the model state See states for more information.","title":"state_type"},{"location":"models/#mlmodule.v2.base.models.ModelWithState.get_state","text":"Get the model internal state Returns: Type Description bytes Serialised state as bytes","title":"get_state()"},{"location":"models/#mlmodule.v2.base.models.ModelWithState.set_state","text":"Set the model internal state Parameters: Name Type Description Default state bytes Serialised state as bytes required","title":"set_state()"},{"location":"models/#labels","text":"When a model returns label scores, it must define a LabelSet . This should be defined by implementing the ModelWithLabels protocol.","title":"Labels"},{"location":"models/#mlmodule.v2.base.models.ModelWithLabels","text":"Model that predicts scores for labels It defines the get_labels function","title":"ModelWithLabels"},{"location":"models/#mlmodule.v2.base.models.ModelWithLabels.get_labels","text":"Getter for the model's LabelSet Returns: Type Description LabelSet The label set corresponding to returned label scores","title":"get_labels()"},{"location":"models/#pytorch-models","text":"PyTorch models should be a subclass of TorchMlModule . Note PyTorch models already implement the ModelWithState protocol by default.","title":"PyTorch models"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule","text":"Base torch.nn.Module for PyTorch models implemented in MLModule. A valid subclass of TorchMlModule must implement the following method: forward_predictions state_type And can optionally implement: get_dataset_transforms Attributes: Name Type Description device torch.device Mandatory PyTorch device attribute to initialise model. Examples: This would define a simple PyTorch model consisting of fully connected layer. from mlmodule.v2.states import StateType from mlmodule.v2.torch.modules import TorchMlModule from torchvision import transforms class FC ( TorchMlModule [ torch . Tensor ]): def __init__ ( self , device : torch . device = torch . device ( \"cpu\" )): super () . __init__ ( device = device ) self . fc = nn . Linear ( 512 , 512 ) def forward_predictions ( self , batch : torch . Tensor ) -> BatchModelPrediction [ torch . Tensor ]: return BatchModelPrediction ( features = self . fc ( batch )) @property def state_type ( self ) -> StateType : return StateType ( backend = \"pytorch\" , architecture = \"fc512x512\" , ) def get_dataset_transforms ( self ) -> List [ Callable ]: return [ transforms . ToTensor ()] Note This is a generic class taking a _BatchType type argument. This corresponds to the type of data the forward_predictions will receive. It is most likely torch.Tensor","title":"TorchMlModule"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.state_type","text":"Identifier for the current's model state architecture Note PyTorch's model architecture should have the pytorch backend Returns: Type Description StateType State architecture object Note This method must be implemented in subclasses","title":"state_type"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.forward_predictions","text":"Forward pass of the model Applies the module on a batch and returns all potentially interesting data point (features, labels...) Parameters: Name Type Description Default batch _BatchType the batch of data to process required Returns: Type Description BatchModelPrediction[_BatchPredictionArrayType] Prediction object with the keys features , label_scores ... Note This method must be implemented in subclasses","title":"forward_predictions()"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.get_dataset_transforms","text":"Transforms to apply to the input dataset . Note By default, this method returns an empty list (meaning no transformation) but in most cases, this will need to be overridden. Returns: Type Description List[Callable] A list of callables that will be used to transform the input data.","title":"get_dataset_transforms()"},{"location":"models/ArcFace/","text":"ArcFace Deprecated This model needs to be migrated to the latest version We are using an implementation of InsightFace in PyTorch (https://github.com/TreB1eN/InsightFace_Pytorch) Usage import os from typing import List import torch from mlmodule.box import BBoxOutput from mlmodule.contrib.arcface import ArcFaceFeatures from mlmodule.contrib.mtcnn import MTCNNDetector from mlmodule.torch.data.box import BoundingBoxDataset from mlmodule.torch.data.images import ImageDataset from mlmodule.utils import list_files_in_dir torch_device = torch . device ( 'cpu' ) base_path = os . path . join ( \"tests\" , \"fixtures\" , \"berset\" ) file_names = list_files_in_dir ( base_path , allowed_extensions = ( 'jpg' ,)) # Load image dataset dataset = ImageDataset ( file_names ) # Getting face detection model mtcnn = MTCNNDetector ( device = torch_device , image_size = 720 , min_face_size = 20 ) mtcnn . load () # Detect faces first file_names , outputs = mtcnn . bulk_inference ( dataset ) # Flattening all detected faces bboxes : List [ BBoxOutput ] indices : List [ str ] indices , file_names , bboxes = zip ( * [ ( f ' { fn } _ { i } ' , fn , bbox ) for fn , bbox_list in zip ( file_names , outputs ) for i , bbox in enumerate ( bbox_list ) ]) # Create a dataset for the bounding boxes bbox_features = BoundingBoxDataset ( indices , file_names , bboxes ) arcface = ArcFaceFeatures ( device = torch_device ) arcface . load () # Get face features indices , new_outputs = arcface . bulk_inference ( bbox_features , data_loader_options = { 'batch_size' : 3 }) Download the model weights on PC32 export PUBLIC_ASSETS = /mnt/storage01/lsir-public-assets/pretrained-models # For text encoders python -m mlmodule.cli download arcface.ArcFaceFeatures --outdir \" $PUBLIC_ASSETS /face-detection/\"","title":"ArcFace"},{"location":"models/ArcFace/#arcface","text":"Deprecated This model needs to be migrated to the latest version We are using an implementation of InsightFace in PyTorch (https://github.com/TreB1eN/InsightFace_Pytorch)","title":"ArcFace"},{"location":"models/ArcFace/#usage","text":"import os from typing import List import torch from mlmodule.box import BBoxOutput from mlmodule.contrib.arcface import ArcFaceFeatures from mlmodule.contrib.mtcnn import MTCNNDetector from mlmodule.torch.data.box import BoundingBoxDataset from mlmodule.torch.data.images import ImageDataset from mlmodule.utils import list_files_in_dir torch_device = torch . device ( 'cpu' ) base_path = os . path . join ( \"tests\" , \"fixtures\" , \"berset\" ) file_names = list_files_in_dir ( base_path , allowed_extensions = ( 'jpg' ,)) # Load image dataset dataset = ImageDataset ( file_names ) # Getting face detection model mtcnn = MTCNNDetector ( device = torch_device , image_size = 720 , min_face_size = 20 ) mtcnn . load () # Detect faces first file_names , outputs = mtcnn . bulk_inference ( dataset ) # Flattening all detected faces bboxes : List [ BBoxOutput ] indices : List [ str ] indices , file_names , bboxes = zip ( * [ ( f ' { fn } _ { i } ' , fn , bbox ) for fn , bbox_list in zip ( file_names , outputs ) for i , bbox in enumerate ( bbox_list ) ]) # Create a dataset for the bounding boxes bbox_features = BoundingBoxDataset ( indices , file_names , bboxes ) arcface = ArcFaceFeatures ( device = torch_device ) arcface . load () # Get face features indices , new_outputs = arcface . bulk_inference ( bbox_features , data_loader_options = { 'batch_size' : 3 })","title":"Usage"},{"location":"models/ArcFace/#download-the-model-weights-on-pc32","text":"export PUBLIC_ASSETS = /mnt/storage01/lsir-public-assets/pretrained-models # For text encoders python -m mlmodule.cli download arcface.ArcFaceFeatures --outdir \" $PUBLIC_ASSETS /face-detection/\"","title":"Download the model weights on PC32"},{"location":"models/CLIP/","text":"CLIP See OpenAI/CLIP for the source code and original models. This model has extra requirements: pip install git+ssh://git@github.com/LSIR/mlmodule.git#egg = mlmodule [ torch,clip ] # or pip install mlmodule [ torch,clip ] Models CLIP comes with image ( CLIPImageModule ) and a text ( CLIPTextModule ) encoders. These modules are an implementation of TorchMlModule . mlmodule.contrib.clip.image.CLIPImageModule Image encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . mlmodule.contrib.clip.text.CLIPTextModule Text encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . Pre-trained states from CLIP See the stores documentation for usage. mlmodule.contrib.clip.stores.CLIPStore Pre-trained model states by OpenAI CLIP These are identified by training_id=clip . List models and parameters There is a command line utility to list all available models from CLIP with their associated parameters in JSON format: python -m mlmodule.contrib.clip.list The output is used to fill the mlmodule.contrib.clip.parameters file.","title":"CLIP"},{"location":"models/CLIP/#clip","text":"See OpenAI/CLIP for the source code and original models. This model has extra requirements: pip install git+ssh://git@github.com/LSIR/mlmodule.git#egg = mlmodule [ torch,clip ] # or pip install mlmodule [ torch,clip ]","title":"CLIP"},{"location":"models/CLIP/#models","text":"CLIP comes with image ( CLIPImageModule ) and a text ( CLIPTextModule ) encoders. These modules are an implementation of TorchMlModule .","title":"Models"},{"location":"models/CLIP/#mlmodule.contrib.clip.image.CLIPImageModule","text":"Image encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"CLIPImageModule"},{"location":"models/CLIP/#mlmodule.contrib.clip.text.CLIPTextModule","text":"Text encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"CLIPTextModule"},{"location":"models/CLIP/#pre-trained-states-from-clip","text":"See the stores documentation for usage.","title":"Pre-trained states from CLIP"},{"location":"models/CLIP/#mlmodule.contrib.clip.stores.CLIPStore","text":"Pre-trained model states by OpenAI CLIP These are identified by training_id=clip .","title":"CLIPStore"},{"location":"models/CLIP/#list-models-and-parameters","text":"There is a command line utility to list all available models from CLIP with their associated parameters in JSON format: python -m mlmodule.contrib.clip.list The output is used to fill the mlmodule.contrib.clip.parameters file.","title":"List models and parameters"},{"location":"models/DenseNet/","text":"DenseNet Deprecated This model needs to be migrated to the latest version We are using torchvision to load pretrained models, see https://pytorch.org/docs/stable/torchvision/models.html. Feature Models TODO Pretrained Classifier TODO","title":"DenseNet"},{"location":"models/DenseNet/#densenet","text":"Deprecated This model needs to be migrated to the latest version We are using torchvision to load pretrained models, see https://pytorch.org/docs/stable/torchvision/models.html.","title":"DenseNet"},{"location":"models/DenseNet/#feature-models","text":"TODO","title":"Feature Models"},{"location":"models/DenseNet/#pretrained-classifier","text":"TODO","title":"Pretrained Classifier"},{"location":"models/MTCNN/","text":"MTCNN We are using facenet-pytorch to load pre-trained MTCNN model, see https://github.com/timesler/facenet-pytorch . Requirements This needs mlmodule with the mtcnn and torch extra requirements: pip install git+ssh://git@github.com/LSIR/mlmodule.git#egg = mlmodule [ torch,mtcnn ] # or pip install mlmodule [ torch,mtcnn ] Model The MTCNN model is an implementation of a TorchMlModule . mlmodule.contrib.mtcnn.modules.TorchMTCNNModule MTCNN face detection module Attributes: Name Type Description thresholds Tuple[float, float, float] MTCNN threshold hyperparameters image_size Tuple[int, int] Image size after pre-preprocessing min_face_size int Minimum face size in pixels device torch.device Torch device to initialise the model weights Provider store See the stores documentation for usage. mlmodule.contrib.mtcnn.stores.FaceNetMTCNNStore Pre-trained model states by Facenet for MTCNN These are identified by training_id=facenet .","title":"MTCNN"},{"location":"models/MTCNN/#mtcnn","text":"We are using facenet-pytorch to load pre-trained MTCNN model, see https://github.com/timesler/facenet-pytorch .","title":"MTCNN"},{"location":"models/MTCNN/#requirements","text":"This needs mlmodule with the mtcnn and torch extra requirements: pip install git+ssh://git@github.com/LSIR/mlmodule.git#egg = mlmodule [ torch,mtcnn ] # or pip install mlmodule [ torch,mtcnn ]","title":"Requirements"},{"location":"models/MTCNN/#model","text":"The MTCNN model is an implementation of a TorchMlModule .","title":"Model"},{"location":"models/MTCNN/#mlmodule.contrib.mtcnn.modules.TorchMTCNNModule","text":"MTCNN face detection module Attributes: Name Type Description thresholds Tuple[float, float, float] MTCNN threshold hyperparameters image_size Tuple[int, int] Image size after pre-preprocessing min_face_size int Minimum face size in pixels device torch.device Torch device to initialise the model weights","title":"TorchMTCNNModule"},{"location":"models/MTCNN/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/MTCNN/#mlmodule.contrib.mtcnn.stores.FaceNetMTCNNStore","text":"Pre-trained model states by Facenet for MTCNN These are identified by training_id=facenet .","title":"FaceNetMTCNNStore"},{"location":"models/MagFace/","text":"MagFace Deprecated This model needs to be migrated to the latest version We are using the official implementation of MagFace in Pytorch (https://github.com/IrvingMeng/MagFace) Usage import os from typing import List import torch from mlmodule.box import BBoxOutput from mlmodule.contrib.arcface import MagFaceFeatures from mlmodule.contrib.mtcnn import MTCNNDetector from mlmodule.torch.data.box import BoundingBoxDataset from mlmodule.torch.data.images import ImageDataset from mlmodule.utils import list_files_in_dir torch_device = torch . device ( 'cpu' ) base_path = os . path . join ( \"tests\" , \"fixtures\" , \"berset\" ) file_names = list_files_in_dir ( base_path , allowed_extensions = ( 'jpg' ,)) # Load image dataset dataset = ImageDataset ( file_names ) # Getting face detection model mtcnn = MTCNNDetector ( device = torch_device , image_size = 720 , min_face_size = 20 ) mtcnn . load () # Detect faces first file_names , outputs = mtcnn . bulk_inference ( dataset ) # Flattening all detected faces bboxes : List [ BBoxOutput ] indices : List [ str ] indices , file_names , bboxes = zip ( * [ ( f ' { fn } _ { i } ' , fn , bbox ) for fn , bbox_list in zip ( file_names , outputs ) for i , bbox in enumerate ( bbox_list ) ]) # Create a dataset for the bounding boxes bbox_features = BoundingBoxDataset ( indices , file_names , bboxes ) magface = MagFaceFeatures ( device = torch_device ) magface . load () # Get face features indices , new_outputs = magface . bulk_inference ( bbox_features , data_loader_options = { 'batch_size' : 3 }) Download the model weights on PC32 export PUBLIC_ASSETS = /mnt/storage01/lsir-public-assets/pretrained-models # For text encoders python -m mlmodule.cli download magface.MagFaceFeatures --outdir \" $PUBLIC_ASSETS /face-detection/\"","title":"MagFace"},{"location":"models/MagFace/#magface","text":"Deprecated This model needs to be migrated to the latest version We are using the official implementation of MagFace in Pytorch (https://github.com/IrvingMeng/MagFace)","title":"MagFace"},{"location":"models/MagFace/#usage","text":"import os from typing import List import torch from mlmodule.box import BBoxOutput from mlmodule.contrib.arcface import MagFaceFeatures from mlmodule.contrib.mtcnn import MTCNNDetector from mlmodule.torch.data.box import BoundingBoxDataset from mlmodule.torch.data.images import ImageDataset from mlmodule.utils import list_files_in_dir torch_device = torch . device ( 'cpu' ) base_path = os . path . join ( \"tests\" , \"fixtures\" , \"berset\" ) file_names = list_files_in_dir ( base_path , allowed_extensions = ( 'jpg' ,)) # Load image dataset dataset = ImageDataset ( file_names ) # Getting face detection model mtcnn = MTCNNDetector ( device = torch_device , image_size = 720 , min_face_size = 20 ) mtcnn . load () # Detect faces first file_names , outputs = mtcnn . bulk_inference ( dataset ) # Flattening all detected faces bboxes : List [ BBoxOutput ] indices : List [ str ] indices , file_names , bboxes = zip ( * [ ( f ' { fn } _ { i } ' , fn , bbox ) for fn , bbox_list in zip ( file_names , outputs ) for i , bbox in enumerate ( bbox_list ) ]) # Create a dataset for the bounding boxes bbox_features = BoundingBoxDataset ( indices , file_names , bboxes ) magface = MagFaceFeatures ( device = torch_device ) magface . load () # Get face features indices , new_outputs = magface . bulk_inference ( bbox_features , data_loader_options = { 'batch_size' : 3 })","title":"Usage"},{"location":"models/MagFace/#download-the-model-weights-on-pc32","text":"export PUBLIC_ASSETS = /mnt/storage01/lsir-public-assets/pretrained-models # For text encoders python -m mlmodule.cli download magface.MagFaceFeatures --outdir \" $PUBLIC_ASSETS /face-detection/\"","title":"Download the model weights on PC32"},{"location":"models/ResNet/","text":"ResNet PyTorch implementation of ResNet as defined in Deep Residual Learning for Image Recognition and Torchvision . Model The ResNet model is an implementation of a TorchMlModule . mlmodule.contrib.resnet.modules.TorchResNetModule PyTorch ResNet architecture for ImageNet classification. See PyTorch's documentation . Attributes: Name Type Description resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights Pre-trained state origins See the stores documentation for usage. mlmodule.contrib.resnet.stores.ResNetTorchVisionStore Model store to load ResNet weights pretrained on ImageNet from TorchVision","title":"ResNet"},{"location":"models/ResNet/#resnet","text":"PyTorch implementation of ResNet as defined in Deep Residual Learning for Image Recognition and Torchvision .","title":"ResNet"},{"location":"models/ResNet/#model","text":"The ResNet model is an implementation of a TorchMlModule .","title":"Model"},{"location":"models/ResNet/#mlmodule.contrib.resnet.modules.TorchResNetModule","text":"PyTorch ResNet architecture for ImageNet classification. See PyTorch's documentation . Attributes: Name Type Description resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights","title":"TorchResNetModule"},{"location":"models/ResNet/#pre-trained-state-origins","text":"See the stores documentation for usage.","title":"Pre-trained state origins"},{"location":"models/ResNet/#mlmodule.contrib.resnet.stores.ResNetTorchVisionStore","text":"Model store to load ResNet weights pretrained on ImageNet from TorchVision","title":"ResNetTorchVisionStore"},{"location":"models/VinVL/","text":"VinVL: Revisiting Visual Representations in Vision-Language Models Deprecated This model needs to be migrated to the latest version Pre-trained large-scale object-attribute detection (OD) model is based on the ResNeXt-152 C4 architecture. The OD model has been firstly trained on much larger amounts of data, combining multiple public object detection datasets, including COCO , OpenImages (OI) , Objects365 , and Visual Genome (VG) . Then it is fine-tuned on VG dataset alone, since VG is the only dataset with label attributes (see issue #120 ). It predicts objects from 1594 classes with attributes from 524 classes. See the code and the paper for details. Requirements The model needs the configuration system YACS to be installed. pip install yacs == 0 .1.8 Usage from mlmodule.utils import list_files_in_dir from mlmodule.torch.data.images import ImageDataset from mlmodule.contrib.vinvl import VinVLDetector from mlmodule.contrib.vinvl.utils import postprocess_attr import torch import os # Load VinVL model torch_device = torch . device ( 'cuda' ) vinvl = VinVLDetector ( device = torch_device , score_threshold = 0.5 ) # Pretrained model vinvl . load () # Getting data base_path = os . path . join ( \"tests\" , \"fixtures\" , \"objects\" ) file_names = list_files_in_dir ( base_path , allowed_extensions = ( 'jpg' ,))[: 50 ] dataset = ImageDataset ( file_names ) # Get the detections indices , detections = vinvl . bulk_inference ( dataset , data_loader_options = { 'batch_size' : 10 }) # Get labels and attributes labels = vinvl . get_labels () attribute_labels = vinvl . get_attribute_labels () # Print out detected object for i , img_path in enumerate ( indices ): print ( f 'Object with attributes detected for { img_path } ' ) for k , det in enumerate ( detections [ i ]): label = labels [ det . labels [ 0 ]] attr_labels = det . attributes [ det . attr_scores > 0.5 ] attr_scores = det . attr_scores [ det . attr_scores > 0.5 ] attributes = postprocess_attr ( attribute_labels , attr_labels , attr_scores ) print ( f ' { k + 1 } : { \",\" . join ( list ( attributes [ 0 ])) } { label } ( { det . probability : .2f } )' ) Download the model weights on PC32 export PUBLIC_ASSETS = /mnt/storage01/lsir-public-assets/pretrained-models # For text encoders python -m mlmodule.cli download vinvl.VinVLDetector --outdir \" $PUBLIC_ASSETS /object-detection/\"","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"location":"models/VinVL/#vinvl-revisiting-visual-representations-in-vision-language-models","text":"Deprecated This model needs to be migrated to the latest version Pre-trained large-scale object-attribute detection (OD) model is based on the ResNeXt-152 C4 architecture. The OD model has been firstly trained on much larger amounts of data, combining multiple public object detection datasets, including COCO , OpenImages (OI) , Objects365 , and Visual Genome (VG) . Then it is fine-tuned on VG dataset alone, since VG is the only dataset with label attributes (see issue #120 ). It predicts objects from 1594 classes with attributes from 524 classes. See the code and the paper for details.","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"location":"models/VinVL/#requirements","text":"The model needs the configuration system YACS to be installed. pip install yacs == 0 .1.8","title":"Requirements"},{"location":"models/VinVL/#usage","text":"from mlmodule.utils import list_files_in_dir from mlmodule.torch.data.images import ImageDataset from mlmodule.contrib.vinvl import VinVLDetector from mlmodule.contrib.vinvl.utils import postprocess_attr import torch import os # Load VinVL model torch_device = torch . device ( 'cuda' ) vinvl = VinVLDetector ( device = torch_device , score_threshold = 0.5 ) # Pretrained model vinvl . load () # Getting data base_path = os . path . join ( \"tests\" , \"fixtures\" , \"objects\" ) file_names = list_files_in_dir ( base_path , allowed_extensions = ( 'jpg' ,))[: 50 ] dataset = ImageDataset ( file_names ) # Get the detections indices , detections = vinvl . bulk_inference ( dataset , data_loader_options = { 'batch_size' : 10 }) # Get labels and attributes labels = vinvl . get_labels () attribute_labels = vinvl . get_attribute_labels () # Print out detected object for i , img_path in enumerate ( indices ): print ( f 'Object with attributes detected for { img_path } ' ) for k , det in enumerate ( detections [ i ]): label = labels [ det . labels [ 0 ]] attr_labels = det . attributes [ det . attr_scores > 0.5 ] attr_scores = det . attr_scores [ det . attr_scores > 0.5 ] attributes = postprocess_attr ( attribute_labels , attr_labels , attr_scores ) print ( f ' { k + 1 } : { \",\" . join ( list ( attributes [ 0 ])) } { label } ( { det . probability : .2f } )' )","title":"Usage"},{"location":"models/VinVL/#download-the-model-weights-on-pc32","text":"export PUBLIC_ASSETS = /mnt/storage01/lsir-public-assets/pretrained-models # For text encoders python -m mlmodule.cli download vinvl.VinVLDetector --outdir \" $PUBLIC_ASSETS /object-detection/\"","title":"Download the model weights on PC32"},{"location":"models/keyframes/","text":"Video key-frames extractor This model implements two types of modules: a video frames encoder and the key-frames module. These models are an implementation of a TorchMlModule . Key-frames selector models These models allow to extract key-frames from a video. mlmodule.contrib.keyframes.selectors.KeyFrameSelector Video key-frames selector Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor, torch.Tensor] The PyTorch module to encode frames. fps int The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . Video frames encoder mlmodule.contrib.keyframes.encoders.VideoFramesEncoder Video frames encoder This module will extract and encode frames of a video using an image_encoder . Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor] The PyTorch module to encode frames fps int The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"Video key-frames extractor"},{"location":"models/keyframes/#video-key-frames-extractor","text":"This model implements two types of modules: a video frames encoder and the key-frames module. These models are an implementation of a TorchMlModule .","title":"Video key-frames extractor"},{"location":"models/keyframes/#key-frames-selector-models","text":"These models allow to extract key-frames from a video.","title":"Key-frames selector models"},{"location":"models/keyframes/#mlmodule.contrib.keyframes.selectors.KeyFrameSelector","text":"Video key-frames selector Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor, torch.Tensor] The PyTorch module to encode frames. fps int The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"KeyFrameSelector"},{"location":"models/keyframes/#video-frames-encoder","text":"","title":"Video frames encoder"},{"location":"models/keyframes/#mlmodule.contrib.keyframes.encoders.VideoFramesEncoder","text":"Video frames encoder This module will extract and encode frames of a video using an image_encoder . Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor] The PyTorch module to encode frames fps int The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"VideoFramesEncoder"},{"location":"references/callbacks/","text":"Callbacks Callbacks are used to control to control how a runner handles the results (features, labels or model weights). They are classes implementing a pre-defined set of functions: save_features save_label_scores save_bounding_boxes save_frames In memory callbacks These callbacks accumulate results in-memory. They expose their results via object attributes. mlmodule.v2.helpers.callbacks.CollectFeaturesInMemory dataclass Callback to collect features in memory Attributes: Name Type Description indices list List of dataset indices features numpy.ndarray Array of features. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_features ( self , model : Any , indices : Sequence , features : Union [ torch . Tensor , numpy . ndarray ]) -> None Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required mlmodule.v2.helpers.callbacks.CollectLabelsInMemory dataclass Callback to collect labels in memory Attributes: Name Type Description indices list List of dataset indices label_scores numpy.ndarray Array of label scores. The first dimension correspond to self.indices values. labels list[str] List of matching labels (label with maximum score) Note This callback works with any array-like features save_label_scores ( self , model : ModelWithLabels , indices : Sequence , labels_scores : Union [ torch . Tensor , numpy . ndarray ]) -> None Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required mlmodule.v2.helpers.callbacks.CollectBoundingBoxesInMemory dataclass Callback to collect bounding boxes predictions in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_bounding_boxes ( self , model : Any , indices : Sequence , bounding_boxes : Sequence [ mlmodule . v2 . base . predictions . BatchBoundingBoxesPrediction [ Union [ torch . Tensor , numpy . ndarray ]]]) -> None Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required mlmodule.v2.helpers.callbacks.CollectVideoFramesInMemory dataclass Callback to collect video frames in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_frames ( self , model : Any , indices : Sequence , frames : Sequence [ mlmodule . v2 . base . predictions . BatchVideoFramesPrediction [ Union [ torch . Tensor , numpy . ndarray ]]]) -> None Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required Write your own callbacks mlmodule.v2.base.callbacks.BaseSaveFeaturesCallback save_features ( self , model : Any , indices : Sequence , features : - _ContraArrayType ) -> None Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required mlmodule.v2.base.callbacks.BaseSaveLabelsCallback save_label_scores ( self , model : ModelWithLabels , indices : Sequence , labels_scores : - _ContraArrayType ) -> None Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required mlmodule.v2.base.callbacks.BaseSaveBoundingBoxCallback save_bounding_boxes ( self , model : Any , indices : Sequence , bounding_boxes : Sequence [ mlmodule . v2 . base . predictions . BatchBoundingBoxesPrediction [ - _ContraArrayType ]]) -> None Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required mlmodule.v2.base.callbacks.BaseSaveVideoFramesCallback save_frames ( self , model : Any , indices : Sequence , frames : Sequence [ mlmodule . v2 . base . predictions . BatchVideoFramesPrediction [ - _ContraArrayType ]]) -> None Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required","title":"Callbacks"},{"location":"references/callbacks/#callbacks","text":"Callbacks are used to control to control how a runner handles the results (features, labels or model weights). They are classes implementing a pre-defined set of functions: save_features save_label_scores save_bounding_boxes save_frames","title":"Callbacks"},{"location":"references/callbacks/#in-memory-callbacks","text":"These callbacks accumulate results in-memory. They expose their results via object attributes.","title":"In memory callbacks"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectFeaturesInMemory","text":"Callback to collect features in memory Attributes: Name Type Description indices list List of dataset indices features numpy.ndarray Array of features. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectFeaturesInMemory"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectFeaturesInMemory.save_features","text":"Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required","title":"save_features()"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectLabelsInMemory","text":"Callback to collect labels in memory Attributes: Name Type Description indices list List of dataset indices label_scores numpy.ndarray Array of label scores. The first dimension correspond to self.indices values. labels list[str] List of matching labels (label with maximum score) Note This callback works with any array-like features","title":"CollectLabelsInMemory"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectLabelsInMemory.save_label_scores","text":"Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required","title":"save_label_scores()"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectBoundingBoxesInMemory","text":"Callback to collect bounding boxes predictions in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectBoundingBoxesInMemory"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectBoundingBoxesInMemory.save_bounding_boxes","text":"Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required","title":"save_bounding_boxes()"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectVideoFramesInMemory","text":"Callback to collect video frames in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectVideoFramesInMemory"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectVideoFramesInMemory.save_frames","text":"Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required","title":"save_frames()"},{"location":"references/callbacks/#write-your-own-callbacks","text":"","title":"Write your own callbacks"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveFeaturesCallback","text":"","title":"BaseSaveFeaturesCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveFeaturesCallback.save_features","text":"Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required","title":"save_features()"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveLabelsCallback","text":"","title":"BaseSaveLabelsCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveLabelsCallback.save_label_scores","text":"Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required","title":"save_label_scores()"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveBoundingBoxCallback","text":"","title":"BaseSaveBoundingBoxCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveBoundingBoxCallback.save_bounding_boxes","text":"Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required","title":"save_bounding_boxes()"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveVideoFramesCallback","text":"","title":"BaseSaveVideoFramesCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveVideoFramesCallback.save_frames","text":"Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required","title":"save_frames()"},{"location":"references/datasets/","text":"Datasets Torch datasets Torch dataset are implementing dataset as suggested in the Datasets & Dataloaders guide. These datasets are compatible with torch.utils.data.DataLoader . mlmodule.v2.torch.datasets.ListDataset dataclass Simple dataset that contains a list of objects in memory Attributes: Name Type Description objects List[Any] List of objects of the dataset mlmodule.v2.torch.datasets.OpenBinaryFileDataset dataclass Dataset that returns typing.BinaryIO from a list of local file names Attributes: Name Type Description paths List[str] List of paths to files mlmodule.v2.torch.datasets.OpenImageFileDataset dataclass Dataset that returns PIL.Image.Image from a list of local file names Attributes: Name Type Description paths List[str] List of paths to image files resize_image_size tuple[int, int] | None Optionally reduce the image size on load Write your own dataset Dataset types depends on the runner used, refer to the runner list to know which type to implement. Below is the list of dataset protocols that are specifyied by mlmodule mlmodule.v2.torch.datasets.TorchDataset PyTorch dataset protocol In order to be used with the PyTorch runners, a TorchDataset should expose two functions __getitem__ and __len__ . __getitem__ ( self , index : int ) -> Tuple [ + _IndicesType , + _DatasetType ] special Get an item of the dataset by index Parameters: Name Type Description Default index int The index of the element to get required Returns: Type Description Tuple[_IndicesType, _DatasetType] A tuple of dataset index of the element and value of the element __len__ ( self ) -> int special Length of the dataset Returns: Type Description int The length of the dataset","title":"Datasets"},{"location":"references/datasets/#datasets","text":"","title":"Datasets"},{"location":"references/datasets/#torch-datasets","text":"Torch dataset are implementing dataset as suggested in the Datasets & Dataloaders guide. These datasets are compatible with torch.utils.data.DataLoader .","title":"Torch datasets"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.ListDataset","text":"Simple dataset that contains a list of objects in memory Attributes: Name Type Description objects List[Any] List of objects of the dataset","title":"ListDataset"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.OpenBinaryFileDataset","text":"Dataset that returns typing.BinaryIO from a list of local file names Attributes: Name Type Description paths List[str] List of paths to files","title":"OpenBinaryFileDataset"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.OpenImageFileDataset","text":"Dataset that returns PIL.Image.Image from a list of local file names Attributes: Name Type Description paths List[str] List of paths to image files resize_image_size tuple[int, int] | None Optionally reduce the image size on load","title":"OpenImageFileDataset"},{"location":"references/datasets/#write-your-own-dataset","text":"Dataset types depends on the runner used, refer to the runner list to know which type to implement. Below is the list of dataset protocols that are specifyied by mlmodule","title":"Write your own dataset"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.TorchDataset","text":"PyTorch dataset protocol In order to be used with the PyTorch runners, a TorchDataset should expose two functions __getitem__ and __len__ .","title":"TorchDataset"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.TorchDataset.__getitem__","text":"Get an item of the dataset by index Parameters: Name Type Description Default index int The index of the element to get required Returns: Type Description Tuple[_IndicesType, _DatasetType] A tuple of dataset index of the element and value of the element","title":"__getitem__()"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.TorchDataset.__len__","text":"Length of the dataset Returns: Type Description int The length of the dataset","title":"__len__()"},{"location":"references/labels/","text":"mlmodule.labels.base.LabelSet dataclass Label set is an ordered list of labels used for classification tasks Attributes: Name Type Description label_set_unique_id str Unique identifier for a label set label_list List[str] Ordered list of labels Examples: LabelSet objects are used as classic lists: animal_labels = LabelSet ( label_set_unique_id = \"animals\" , label_list = [ \"cat\" , \"dog\" ] ) print ( animal_labels . label_set_unique_id ) # \"animals\" print ( animal_labels [ 0 ]) # \"cat\" print ( animal_labels [ 1 ]) # \"dog\" print ( len ( animal_labels )) # 2 Available label sets mlmodule.labels.imagenet.IMAGENET_LABELS mlmodule.labels.places_io.PLACES_IN_OUT_DOOR mlmodule.labels.places_io.PLACES_IO_LABELS mlmodule.labels.places.PLACES_LABELS mlmodule.labels.vinvl_attributes.VINVL_ATTRIBUTE_LABELS mlmodule.labels.vinvl.VINVL_LABELS","title":"Labels"},{"location":"references/labels/#mlmodule.labels.base.LabelSet","text":"Label set is an ordered list of labels used for classification tasks Attributes: Name Type Description label_set_unique_id str Unique identifier for a label set label_list List[str] Ordered list of labels Examples: LabelSet objects are used as classic lists: animal_labels = LabelSet ( label_set_unique_id = \"animals\" , label_list = [ \"cat\" , \"dog\" ] ) print ( animal_labels . label_set_unique_id ) # \"animals\" print ( animal_labels [ 0 ]) # \"cat\" print ( animal_labels [ 1 ]) # \"dog\" print ( len ( animal_labels )) # 2","title":"LabelSet"},{"location":"references/labels/#available-label-sets","text":"mlmodule.labels.imagenet.IMAGENET_LABELS mlmodule.labels.places_io.PLACES_IN_OUT_DOOR mlmodule.labels.places_io.PLACES_IO_LABELS mlmodule.labels.places.PLACES_LABELS mlmodule.labels.vinvl_attributes.VINVL_ATTRIBUTE_LABELS mlmodule.labels.vinvl.VINVL_LABELS","title":"Available label sets"},{"location":"references/runners/","text":"Runners Runners are used to execute inference or training of a model. They are initialised with a model , a dataset , callbacks and options . They are executed with the run function which takes no arguments. Inference mlmodule.v2.torch.runners.TorchInferenceRunner Runner for inference tasks on PyTorch models Supports CPU or single GPU inference. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchRunnerOptions PyTorch options run ( self ) -> None Runs inference Options mlmodule.v2.torch.options.TorchRunnerOptions dataclass Options for PyTorch runners Attributes: Name Type Description device torch.device Torch device data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar Write your own runner mlmodule.v2.base.runners.BaseRunner A runner takes a model and run an action on it (inference, training...) It must implement the run function and call the callbacks to save prediction or model weights. Attributes: Name Type Description model _ModelType The model object to run the action against dataset _DataType The input dataset callbacks List[_CallbackType] Callbacks to save model state or predictions options _OptionsType Options of the runner (devices...) Note The _ModelType , _DataType , _CallbackType and _OptionsType are generic types that should be specified when implementing a runner. Examples: This is an example of a runner that applies a function to each element of list dataset. It passes the returned data to the save_features callback. from mlmodule.v2.base.callbacks import ( BaseSaveFeaturesCallback , callbacks_caller ) from mlmodule.v2.base.runners import BaseRunner class NumpySumRunnerExample ( BaseRunner [ Callable , # _ModelType List , # _DataType BaseSaveFeaturesCallback , # _CallbackType None # _OptionType ]): def run ( self ): for index , data in enumerate ( self . dataset ): # Helper function to call all matching callbacks callbacks_caller ( self . callbacks , \"save_features\" , index , self . model ( data ) )","title":"Runners"},{"location":"references/runners/#runners","text":"Runners are used to execute inference or training of a model. They are initialised with a model , a dataset , callbacks and options . They are executed with the run function which takes no arguments.","title":"Runners"},{"location":"references/runners/#inference","text":"","title":"Inference"},{"location":"references/runners/#mlmodule.v2.torch.runners.TorchInferenceRunner","text":"Runner for inference tasks on PyTorch models Supports CPU or single GPU inference. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchRunnerOptions PyTorch options","title":"TorchInferenceRunner"},{"location":"references/runners/#mlmodule.v2.torch.runners.TorchInferenceRunner.run","text":"Runs inference","title":"run()"},{"location":"references/runners/#options","text":"","title":"Options"},{"location":"references/runners/#mlmodule.v2.torch.options.TorchRunnerOptions","text":"Options for PyTorch runners Attributes: Name Type Description device torch.device Torch device data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar","title":"TorchRunnerOptions"},{"location":"references/runners/#write-your-own-runner","text":"","title":"Write your own runner"},{"location":"references/runners/#mlmodule.v2.base.runners.BaseRunner","text":"A runner takes a model and run an action on it (inference, training...) It must implement the run function and call the callbacks to save prediction or model weights. Attributes: Name Type Description model _ModelType The model object to run the action against dataset _DataType The input dataset callbacks List[_CallbackType] Callbacks to save model state or predictions options _OptionsType Options of the runner (devices...) Note The _ModelType , _DataType , _CallbackType and _OptionsType are generic types that should be specified when implementing a runner. Examples: This is an example of a runner that applies a function to each element of list dataset. It passes the returned data to the save_features callback. from mlmodule.v2.base.callbacks import ( BaseSaveFeaturesCallback , callbacks_caller ) from mlmodule.v2.base.runners import BaseRunner class NumpySumRunnerExample ( BaseRunner [ Callable , # _ModelType List , # _DataType BaseSaveFeaturesCallback , # _CallbackType None # _OptionType ]): def run ( self ): for index , data in enumerate ( self . dataset ): # Helper function to call all matching callbacks callbacks_caller ( self . callbacks , \"save_features\" , index , self . model ( data ) )","title":"BaseRunner"},{"location":"references/states/","text":"States are managed with two concepts: StateType : Represents a family of states that are compatible with each other . In general, a model can be loaded with any pre-trained state if it matches its state_type attribute. StateKey : The identifier of a state instance, it should uniquely identify the result of a training activity for a model. mlmodule.v2.states.StateType dataclass Definition for a type of state A state type is used to identify states that can be re-used accross models. For instance, the weights from ResNet18 with 1000 classes pretrained on ImageNet can be reused to initialised the weights of a ResNet18 for binary classification. In this scenario, the state type of the ResNet trained on ImageNet can be re-used to partially initialize the binary classification ResNet18. This is also used for models like key-frames extraction. Key-frames extraction does not define a new weights architecture, it is a wrapper around an image encoder. Therefore, any state that can be loaded into the image encoder, can also be loaded into the key-frame extractor. They share the same state type. As a convention, two state types are compatible when backend and architecture attributes are the same. This is implemented in the is_compatible_with method. Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description backend str The model backend. For instance pytorch . architecture str Identifier for the architecture (e.g. torchresnet18 ...). extra Optional[Iterable[str]] Additional information to identify architecture variants (number of output classes...). is_compatible_with ( self , other : StateType ) -> bool Tells whether two architecture are compatible with each other. Parameters: Name Type Description Default other StateType The other architecture to compare required Returns: Type Description bool true if backend and architecture attributes match. mlmodule.v2.states.StateKey dataclass Identifier for a state of a trained model Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description state_type StateType Identifies the type of state training_id str Identifies the training activity that was used to get to this state. mlmodule . v2 . states . VALID_NAMES The pattern for valid architecture name in StateType.architecture . It must be alphanumeric characters separated with dashes (i.e. [\\w\\-\\d] )","title":"States"},{"location":"references/states/#mlmodule.v2.states.StateType","text":"Definition for a type of state A state type is used to identify states that can be re-used accross models. For instance, the weights from ResNet18 with 1000 classes pretrained on ImageNet can be reused to initialised the weights of a ResNet18 for binary classification. In this scenario, the state type of the ResNet trained on ImageNet can be re-used to partially initialize the binary classification ResNet18. This is also used for models like key-frames extraction. Key-frames extraction does not define a new weights architecture, it is a wrapper around an image encoder. Therefore, any state that can be loaded into the image encoder, can also be loaded into the key-frame extractor. They share the same state type. As a convention, two state types are compatible when backend and architecture attributes are the same. This is implemented in the is_compatible_with method. Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description backend str The model backend. For instance pytorch . architecture str Identifier for the architecture (e.g. torchresnet18 ...). extra Optional[Iterable[str]] Additional information to identify architecture variants (number of output classes...).","title":"StateType"},{"location":"references/states/#mlmodule.v2.states.StateType.is_compatible_with","text":"Tells whether two architecture are compatible with each other. Parameters: Name Type Description Default other StateType The other architecture to compare required Returns: Type Description bool true if backend and architecture attributes match.","title":"is_compatible_with()"},{"location":"references/states/#mlmodule.v2.states.StateKey","text":"Identifier for a state of a trained model Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description state_type StateType Identifies the type of state training_id str Identifies the training activity that was used to get to this state.","title":"StateKey"},{"location":"references/states/#mlmodule.v2.states.VALID_NAMES","text":"The pattern for valid architecture name in StateType.architecture . It must be alphanumeric characters separated with dashes (i.e. [\\w\\-\\d] )","title":"VALID_NAMES"},{"location":"references/stores/","text":"Stores Stores are used to load and save models state. They define 3 methods: get_state_keys , load and save . The get_state_keys method allows to list available pre-trained states for a given type of state. It usually called from a model . # List available model states in store for the model state type state_keys = store . get_state_keys ( model . state_type ) print ( state_keys ) # Prints for a ResNet pre-trained on ImageNet: # [ # StateKey( # state_type=StateType( # backend='pytorch', # architecture='resnet18', # extra=('cls1000',) # ), # training_id='imagenet' # ) # ] See states documentation for more information on how pre-trained states are identified. Then, a state can be loaded into the model . # Getting the first state key resnet_imagenet_state_key = state_keys [ 0 ] # Loading it into the model store . load ( model , state_key = resnet_imagenet_state_key ) A model can be saved by specifying a training_id which should uniquely identify the training activity that yielded this model's state. store . save ( model , training_id = \"2022-01-01-finetuned-imagenet\" ) See AbstractStateStore for more details on these methods. MLModule pre-trained models MLModule provides model weights for all defined models through the MlModule Store . mlmodule . v2 . stores . Store () -> S3StateStore MlModule model state store. Examples: The store can be used to list available pre-trained states for a model store = Store () states = store . get_state_keys ( model . state_type ) And load a given state to a model store . load ( model , state_key = states [ 0 ]) Alternative stores These stores can be used if you want to store model states locally or on S3 storage. mlmodule.v2.stores.local.LocalStateStore dataclass Local filebased store Attributes: Name Type Description folder str Path to the folder to save model's state mlmodule.v2.stores.s3.S3StateStore dataclass State store on top of S3 object storage Given the state keys, states are organised in in folders with the following structure: base_path/ \u251c\u2500 { backend } / \u2502 \u251c\u2500 { architecture } . { extra1 } . { extra2 } . { training_id } .pt \u251c\u2500 pytorch/ # e.g. for torch models \u2502 \u251c\u2500 resnet18.cls1000.imagenet.pt \u2502 \u251c\u2500 clip-image-rn50.clip.pt Attributes: Name Type Description bucket str Bucket to use to store states session_kwargs dict Arguments passed to initialise boto3.session.Session s3_endpoint_url str To connect to S3 compatible storage base_path str The base path to store states Write your own store A store should inherit AbstractStateStore and implement the save , load and get_state_keys methods. mlmodule.v2.stores.abstract.AbstractStateStore Interface to handle model state loading and saving See states reference for more information on state management. get_state_keys ( self , state_type : StateType ) -> List [ mlmodule . v2 . states . StateKey ] Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type ) load ( self , model : ~ _ModelType , state_key : StateKey ) -> None Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load save ( self , model : ~ _ModelType , training_id : str ) -> StateKey Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created","title":"Stores"},{"location":"references/stores/#stores","text":"Stores are used to load and save models state. They define 3 methods: get_state_keys , load and save . The get_state_keys method allows to list available pre-trained states for a given type of state. It usually called from a model . # List available model states in store for the model state type state_keys = store . get_state_keys ( model . state_type ) print ( state_keys ) # Prints for a ResNet pre-trained on ImageNet: # [ # StateKey( # state_type=StateType( # backend='pytorch', # architecture='resnet18', # extra=('cls1000',) # ), # training_id='imagenet' # ) # ] See states documentation for more information on how pre-trained states are identified. Then, a state can be loaded into the model . # Getting the first state key resnet_imagenet_state_key = state_keys [ 0 ] # Loading it into the model store . load ( model , state_key = resnet_imagenet_state_key ) A model can be saved by specifying a training_id which should uniquely identify the training activity that yielded this model's state. store . save ( model , training_id = \"2022-01-01-finetuned-imagenet\" ) See AbstractStateStore for more details on these methods.","title":"Stores"},{"location":"references/stores/#mlmodule-pre-trained-models","text":"MLModule provides model weights for all defined models through the MlModule Store .","title":"MLModule pre-trained models"},{"location":"references/stores/#mlmodule.v2.stores.Store","text":"MlModule model state store. Examples: The store can be used to list available pre-trained states for a model store = Store () states = store . get_state_keys ( model . state_type ) And load a given state to a model store . load ( model , state_key = states [ 0 ])","title":"Store()"},{"location":"references/stores/#alternative-stores","text":"These stores can be used if you want to store model states locally or on S3 storage.","title":"Alternative stores"},{"location":"references/stores/#mlmodule.v2.stores.local.LocalStateStore","text":"Local filebased store Attributes: Name Type Description folder str Path to the folder to save model's state","title":"LocalStateStore"},{"location":"references/stores/#mlmodule.v2.stores.s3.S3StateStore","text":"State store on top of S3 object storage Given the state keys, states are organised in in folders with the following structure: base_path/ \u251c\u2500 { backend } / \u2502 \u251c\u2500 { architecture } . { extra1 } . { extra2 } . { training_id } .pt \u251c\u2500 pytorch/ # e.g. for torch models \u2502 \u251c\u2500 resnet18.cls1000.imagenet.pt \u2502 \u251c\u2500 clip-image-rn50.clip.pt Attributes: Name Type Description bucket str Bucket to use to store states session_kwargs dict Arguments passed to initialise boto3.session.Session s3_endpoint_url str To connect to S3 compatible storage base_path str The base path to store states","title":"S3StateStore"},{"location":"references/stores/#write-your-own-store","text":"A store should inherit AbstractStateStore and implement the save , load and get_state_keys methods.","title":"Write your own store"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore","text":"Interface to handle model state loading and saving See states reference for more information on state management.","title":"AbstractStateStore"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore.get_state_keys","text":"Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type )","title":"get_state_keys()"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore.load","text":"Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load","title":"load()"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore.save","text":"Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created","title":"save()"}]}