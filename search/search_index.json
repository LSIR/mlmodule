{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction MLModule is a library containing a collection of machine learning models with standardised interface to run inference, train and manage model state files. It provides high-level abstractions called runners to run inference or training against a model. Saving the predictions or the model's state is controlled via easily extensible callbacks . MLModule should also be easy to install, therefore, we try to keep as few dependencies as possible. To start using MLModule, go ahead to the installation and getting started guides ! Features Model zoo PyTorch inference PyTorch training Multi-GPU support","title":"Introduction"},{"location":"#introduction","text":"MLModule is a library containing a collection of machine learning models with standardised interface to run inference, train and manage model state files. It provides high-level abstractions called runners to run inference or training against a model. Saving the predictions or the model's state is controlled via easily extensible callbacks . MLModule should also be easy to install, therefore, we try to keep as few dependencies as possible. To start using MLModule, go ahead to the installation and getting started guides !","title":"Introduction"},{"location":"#features","text":"Model zoo PyTorch inference PyTorch training Multi-GPU support","title":"Features"},{"location":"0-installation/","text":"Installation We support Python >= 3.7 and PyTorch >= 1.9 . However, it is likely that MLModule can be run on previous versions of PyTorch, we are simply not testing it for versions before 1.9 . Pre-requisites First, you need to install PyTorch and TorchVision. See the official PyTorch's documentation for installation instructions. We recommend using the conda package manager to install PyTorch. Installing with pip Once PyTorch and TorchVision are installed, MLModule can be directly installed from the git repository using pip . # To install the latest developments pip install git+https://github.com/LSIR/mlmodule # Or to install a specific version X.Y.Z pip install git+https://github.com/LSIR/mlmodule@vX.Y.Z Using Docker For convenience, we also ship a base Docker image which contains dependencies that can be hard to install (for instance PyTorch). See MLModuleKit for usage documentation.","title":"Installation"},{"location":"0-installation/#installation","text":"We support Python >= 3.7 and PyTorch >= 1.9 . However, it is likely that MLModule can be run on previous versions of PyTorch, we are simply not testing it for versions before 1.9 .","title":"Installation"},{"location":"0-installation/#pre-requisites","text":"First, you need to install PyTorch and TorchVision. See the official PyTorch's documentation for installation instructions. We recommend using the conda package manager to install PyTorch.","title":"Pre-requisites"},{"location":"0-installation/#installing-with-pip","text":"Once PyTorch and TorchVision are installed, MLModule can be directly installed from the git repository using pip . # To install the latest developments pip install git+https://github.com/LSIR/mlmodule # Or to install a specific version X.Y.Z pip install git+https://github.com/LSIR/mlmodule@vX.Y.Z","title":"Installing with pip"},{"location":"0-installation/#using-docker","text":"For convenience, we also ship a base Docker image which contains dependencies that can be hard to install (for instance PyTorch). See MLModuleKit for usage documentation.","title":"Using Docker"},{"location":"1-getting-started/","text":"Getting started Code-based usage This guide runs through the inference of a PyTorch ResNet model pre-trained on imagenet. First, we need to create a dataset of images, for this we will be using an ImageDataset . from mlmodule.torch.datasets import LocalBinaryFilesDataset , ImageDataset # Getting a dataset of images (1) dataset = ImageDataset ( LocalBinaryFilesDataset ( paths = [ \"../tests/fixtures/cats_dogs/cat_0.jpg\" , \"../tests/fixtures/cats_dogs/cat_90.jpg\" , ] ) ) See Datasets for a list of available datasets. Next, we need to load the ResNet PyTorch module specifying the resnet18 architecture. The model is initialised with weights pre-trained on ImageNet 1 . from mlmodule.models.resnet import torch_resnet_imagenet # Model definition (1) resnet = torch_resnet_imagenet ( \"resnet18\" ) List of all models Once we have the model initialized, we need to define what we want to do with it. In this case, we'll run an inference loop using the TorchInferenceRunner . Note that we pass two callbacks to the runner: CollectFeaturesInMemory and CollectLabelsInMemory . They will be called to save the features and labels in-memory. from mlmodule.callbacks import CollectFeaturesInMemory , CollectLabelsInMemory from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner # Creating the callback to collect data (1) features = CollectFeaturesInMemory () labels = CollectLabelsInMemory () # Getting the torch runner for inference (2) runner = TorchInferenceRunner ( model = resnet , dataset = dataset , callbacks = [ features , labels ], options = TorchRunnerOptions ( tqdm_enabled = True ), ) List of available callbacks . List of available runners Now that the runner is initialised, we run it with the method run . The callbacks have accumulated the features and labels in memory and we print their content. # Executing inference runner . run () # Printing the features print ( features . indices , features . features ) # Printing labels print ( labels . indices , labels . labels ) Command-line interface MLModule exposes a command-line interface. See python -m mlmodule -h for a list of available commands. For instance, one can run a ResNet model against a list of local images with the following command: python -m mlmodule run \".resnet.modules.TorchResNetModule(resnet18)\" *.jpg It prints the results (features and labels) in JSON format. Similarly, we can extract the key-frames from videos: python -m mlmodule run \".keyframes.selectors.resnet_key_frame_selector(resnet18, 10)\" *.mp4 --file-type vi --batch-size 1 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: a large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , 248\u2013255. Ieee, 2009. \u21a9","title":"Getting started"},{"location":"1-getting-started/#getting-started","text":"","title":"Getting started"},{"location":"1-getting-started/#code-based-usage","text":"This guide runs through the inference of a PyTorch ResNet model pre-trained on imagenet. First, we need to create a dataset of images, for this we will be using an ImageDataset . from mlmodule.torch.datasets import LocalBinaryFilesDataset , ImageDataset # Getting a dataset of images (1) dataset = ImageDataset ( LocalBinaryFilesDataset ( paths = [ \"../tests/fixtures/cats_dogs/cat_0.jpg\" , \"../tests/fixtures/cats_dogs/cat_90.jpg\" , ] ) ) See Datasets for a list of available datasets. Next, we need to load the ResNet PyTorch module specifying the resnet18 architecture. The model is initialised with weights pre-trained on ImageNet 1 . from mlmodule.models.resnet import torch_resnet_imagenet # Model definition (1) resnet = torch_resnet_imagenet ( \"resnet18\" ) List of all models Once we have the model initialized, we need to define what we want to do with it. In this case, we'll run an inference loop using the TorchInferenceRunner . Note that we pass two callbacks to the runner: CollectFeaturesInMemory and CollectLabelsInMemory . They will be called to save the features and labels in-memory. from mlmodule.callbacks import CollectFeaturesInMemory , CollectLabelsInMemory from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner # Creating the callback to collect data (1) features = CollectFeaturesInMemory () labels = CollectLabelsInMemory () # Getting the torch runner for inference (2) runner = TorchInferenceRunner ( model = resnet , dataset = dataset , callbacks = [ features , labels ], options = TorchRunnerOptions ( tqdm_enabled = True ), ) List of available callbacks . List of available runners Now that the runner is initialised, we run it with the method run . The callbacks have accumulated the features and labels in memory and we print their content. # Executing inference runner . run () # Printing the features print ( features . indices , features . features ) # Printing labels print ( labels . indices , labels . labels )","title":"Code-based usage"},{"location":"1-getting-started/#command-line-interface","text":"MLModule exposes a command-line interface. See python -m mlmodule -h for a list of available commands. For instance, one can run a ResNet model against a list of local images with the following command: python -m mlmodule run \".resnet.modules.TorchResNetModule(resnet18)\" *.jpg It prints the results (features and labels) in JSON format. Similarly, we can extract the key-frames from videos: python -m mlmodule run \".keyframes.selectors.resnet_key_frame_selector(resnet18, 10)\" *.mp4 --file-type vi --batch-size 1 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: a large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , 248\u2013255. Ieee, 2009. \u21a9","title":"Command-line interface"},{"location":"2-develop/","text":"Developer guide Installation This section covers the installation of a development environment for contributing to MLModule. If you want to use MLModule in your project, see installation instructions instead. We are developing on Python 3.7 and PyTorch 1.9. Using tox (recommended) This method is the recommended method as it will install a complete environment with PyTorch. It requires conda and tox to be installed. Create a development environment: # CPU development tox --devenv venv -e py37-pt19 # or with GPU support tox --devenv venv -e py37-pt19-cuda111 The environment can be activated with: conda activate ./venv Using pip This method requires you to install PyTorch and TorchVision (see PyTorch documentation ). pip install -r requirements.txt # To install MLModule in development mode with all dependencies pip install -e . Testing Testing can be done using tox : # CPU testing tox -e py37-pt19 # or with GPU support tox -e py37-pt19-cuda111 or with directly using pytest on an environment with all dependencies installed pytest Code quality We use black as formatter. Install pre commit hooks for systematic styling on commits: pip install pre-commit pre-commit install Publish a new version Push the new version to the master branch Test the release on all supported Python / PyTorch versions with the command. tox -e \"{py37,py38,py39}{-pt19,-pt110}{-cuda111,}\" Create a GitHUB release on the branch master with format vX.Y.Z . For instance, v0.1.1 . Upload new model weights Note You will need to set the GH_TOKEN environment variable to the token given by gh auth status -t of the GitHUB CLI . Add the model and provider stores to the scripts/update_public_store.py . They should be added in a function that returns a list of tuple with model and provider store. Update the get_all_models_stores to iterate over your new function. Run the update_public_store scripts. This script accepts a --dry-run argument to see changes before actually uploading models. Update documentation Install requirements to build the documentation: pip install -r docs/requirements.txt We are using mkdocs-material . It allows for previewing the changes with hot-reload: mkdocs serve The documentation can then be deployed to GitHUB pages manually with: mkdocs gh-deploy Or automatically when merging a Pull Request into the main branch with GitHUB actions. Adding notebooks to the documentation When adding new notebooks in the documentation, the bash script docs/pair-notebooks.sh should be executed to create the notebooks in Markdown version. Once the notebooks are paired to a markdown version, they can be updated with the docs/sync-notebooks.sh script.","title":"Developer guide"},{"location":"2-develop/#developer-guide","text":"","title":"Developer guide"},{"location":"2-develop/#installation","text":"This section covers the installation of a development environment for contributing to MLModule. If you want to use MLModule in your project, see installation instructions instead. We are developing on Python 3.7 and PyTorch 1.9.","title":"Installation"},{"location":"2-develop/#using-tox-recommended","text":"This method is the recommended method as it will install a complete environment with PyTorch. It requires conda and tox to be installed. Create a development environment: # CPU development tox --devenv venv -e py37-pt19 # or with GPU support tox --devenv venv -e py37-pt19-cuda111 The environment can be activated with: conda activate ./venv","title":"Using tox (recommended)"},{"location":"2-develop/#using-pip","text":"This method requires you to install PyTorch and TorchVision (see PyTorch documentation ). pip install -r requirements.txt # To install MLModule in development mode with all dependencies pip install -e .","title":"Using pip"},{"location":"2-develop/#testing","text":"Testing can be done using tox : # CPU testing tox -e py37-pt19 # or with GPU support tox -e py37-pt19-cuda111 or with directly using pytest on an environment with all dependencies installed pytest","title":"Testing"},{"location":"2-develop/#code-quality","text":"We use black as formatter. Install pre commit hooks for systematic styling on commits: pip install pre-commit pre-commit install","title":"Code quality"},{"location":"2-develop/#publish-a-new-version","text":"Push the new version to the master branch Test the release on all supported Python / PyTorch versions with the command. tox -e \"{py37,py38,py39}{-pt19,-pt110}{-cuda111,}\" Create a GitHUB release on the branch master with format vX.Y.Z . For instance, v0.1.1 .","title":"Publish a new version"},{"location":"2-develop/#upload-new-model-weights","text":"Note You will need to set the GH_TOKEN environment variable to the token given by gh auth status -t of the GitHUB CLI . Add the model and provider stores to the scripts/update_public_store.py . They should be added in a function that returns a list of tuple with model and provider store. Update the get_all_models_stores to iterate over your new function. Run the update_public_store scripts. This script accepts a --dry-run argument to see changes before actually uploading models.","title":"Upload new model weights"},{"location":"2-develop/#update-documentation","text":"Install requirements to build the documentation: pip install -r docs/requirements.txt We are using mkdocs-material . It allows for previewing the changes with hot-reload: mkdocs serve The documentation can then be deployed to GitHUB pages manually with: mkdocs gh-deploy Or automatically when merging a Pull Request into the main branch with GitHUB actions.","title":"Update documentation"},{"location":"2-develop/#adding-notebooks-to-the-documentation","text":"When adding new notebooks in the documentation, the bash script docs/pair-notebooks.sh should be executed to create the notebooks in Markdown version. Once the notebooks are paired to a markdown version, they can be updated with the docs/sync-notebooks.sh script.","title":"Adding notebooks to the documentation"},{"location":"3-mlmodulekit/","text":"MLModule Kit MLModuleKit is a collection of docker images with a pre-configured environment to use MLModule. You can pull the image for a specific <version> with: docker pull lsirepfl/mlmodulekit:<version> Usage This example will go through the process to run a python script called main.py in the MLModuleKit docker image. As a first step, we need to create a Dockerfile that: Imports mlmodulekit image Installs the latest version of MLModule Copies the script we want to run. FROM lsirepfl/mlmodulekit:3 WORKDIR /app RUN pip install git+https://github.com/LSIR/mlmodule ADD main.py . ENTRYPOINT [ \"conda\" , \"run\" , \"-n\" , \"app\" , \"--no-capture-output\" ] Then, we need to build a docker image from the Dockerfile : docker build . -t my-mlmodule-job The previous command has created a docker container that we can run with the following command: docker run my-mlmodule-job python main.py That's it ! You should see you script output in the terminal. Available versions Description of available image tags. 3 Python 3.7 CUDA 11.1 PyTorch 1.9.1 TorchVision 0.10.1","title":"MLModule Kit"},{"location":"3-mlmodulekit/#mlmodule-kit","text":"MLModuleKit is a collection of docker images with a pre-configured environment to use MLModule. You can pull the image for a specific <version> with: docker pull lsirepfl/mlmodulekit:<version>","title":"MLModule Kit"},{"location":"3-mlmodulekit/#usage","text":"This example will go through the process to run a python script called main.py in the MLModuleKit docker image. As a first step, we need to create a Dockerfile that: Imports mlmodulekit image Installs the latest version of MLModule Copies the script we want to run. FROM lsirepfl/mlmodulekit:3 WORKDIR /app RUN pip install git+https://github.com/LSIR/mlmodule ADD main.py . ENTRYPOINT [ \"conda\" , \"run\" , \"-n\" , \"app\" , \"--no-capture-output\" ] Then, we need to build a docker image from the Dockerfile : docker build . -t my-mlmodule-job The previous command has created a docker container that we can run with the following command: docker run my-mlmodule-job python main.py That's it ! You should see you script output in the terminal.","title":"Usage"},{"location":"3-mlmodulekit/#available-versions","text":"Description of available image tags.","title":"Available versions"},{"location":"3-mlmodulekit/#3","text":"Python 3.7 CUDA 11.1 PyTorch 1.9.1 TorchVision 0.10.1","title":"3"},{"location":"examples/arcface/","text":"Face Similarity Search Import mlmodule modules from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , CollectFeaturesInMemory , ) from mlmodule.torch.datasets import ( LocalBinaryFilesDataset , ImageDataset , ImageBoundingBoxDataset , ) from mlmodule.helpers.files import list_files_in_dir from mlmodule.models.mtcnn.pretrained import torch_mtcnn from mlmodule.models.arcface.pretrained import torch_arcface_insightface import torch import matplotlib.pyplot as plt import numpy as np from facenet_pytorch.models.utils.detect_face import crop_resize from PIL import Image import seaborn as sns sns . set ( style = \"white\" ) % matplotlib inline Load images base_path = \"../../tests/fixtures/berset\" file_names = list_files_in_dir ( base_path , allowed_extensions = ( \"jpg\" ,)) # Load image dataset berset_dataset = ImageDataset ( LocalBinaryFilesDataset ( file_names )) Run face detection with torch_mtcnn torch_device = torch . device ( \"cpu\" ) face_detector = torch_mtcnn ( device = torch_device ) # Callbacks bb = CollectBoundingBoxesInMemory () # Runner runner = TorchInferenceRunner ( model = face_detector , dataset = berset_dataset , callbacks = [ bb ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Extract face features with TorchArcFaceModule arcface = torch_arcface_insightface ( device = torch_device ) # Dataset dataset = ImageBoundingBoxDataset ( image_dataset = ImageDataset ( LocalBinaryFilesDataset ( bb . indices )), bounding_boxes = bb . bounding_boxes , ) # Callbacks ff = CollectFeaturesInMemory () # Runner runner = TorchInferenceRunner ( model = arcface , dataset = dataset , callbacks = [ ff ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 3 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Display the cropped faces def image_grid ( array , ncols = 10 ): index , height , width , channels = array . shape nrows = index // ncols img_grid = ( array . reshape ( nrows , ncols , height , width , channels ) . swapaxes ( 1 , 2 ) . reshape ( height * nrows , width * ncols , channels ) ) return img_grid img_arr = [] for k , file_name in enumerate ( bb . indices ): img = Image . open ( file_name ) . convert ( \"RGB\" ) bboxes = bb . bounding_boxes [ k ] for box in bboxes . bounding_boxes : cropped_face = np . asarray ( crop_resize ( img , box , image_size = 112 )) img_arr . append ( cropped_face ) result = image_grid ( np . array ( img_arr ), ncols = len ( img_arr )) fig = plt . figure ( figsize = ( 20.0 , 20.0 )) plt . imshow ( result ) Compute face similarity sim_mat = ff . features @ ff . features . T fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax = sns . heatmap ( sim_mat , cmap = \"PuRd\" , annot = True ) From this heatmap, we can see that the cosine similarities between the three faces of Alain Berset is quite high (from 0.7 to 0.74) while they are very low between all other faces.","title":"Face Similarity Search"},{"location":"examples/arcface/#face-similarity-search","text":"Import mlmodule modules from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , CollectFeaturesInMemory , ) from mlmodule.torch.datasets import ( LocalBinaryFilesDataset , ImageDataset , ImageBoundingBoxDataset , ) from mlmodule.helpers.files import list_files_in_dir from mlmodule.models.mtcnn.pretrained import torch_mtcnn from mlmodule.models.arcface.pretrained import torch_arcface_insightface import torch import matplotlib.pyplot as plt import numpy as np from facenet_pytorch.models.utils.detect_face import crop_resize from PIL import Image import seaborn as sns sns . set ( style = \"white\" ) % matplotlib inline Load images base_path = \"../../tests/fixtures/berset\" file_names = list_files_in_dir ( base_path , allowed_extensions = ( \"jpg\" ,)) # Load image dataset berset_dataset = ImageDataset ( LocalBinaryFilesDataset ( file_names )) Run face detection with torch_mtcnn torch_device = torch . device ( \"cpu\" ) face_detector = torch_mtcnn ( device = torch_device ) # Callbacks bb = CollectBoundingBoxesInMemory () # Runner runner = TorchInferenceRunner ( model = face_detector , dataset = berset_dataset , callbacks = [ bb ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Extract face features with TorchArcFaceModule arcface = torch_arcface_insightface ( device = torch_device ) # Dataset dataset = ImageBoundingBoxDataset ( image_dataset = ImageDataset ( LocalBinaryFilesDataset ( bb . indices )), bounding_boxes = bb . bounding_boxes , ) # Callbacks ff = CollectFeaturesInMemory () # Runner runner = TorchInferenceRunner ( model = arcface , dataset = dataset , callbacks = [ ff ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 3 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Display the cropped faces def image_grid ( array , ncols = 10 ): index , height , width , channels = array . shape nrows = index // ncols img_grid = ( array . reshape ( nrows , ncols , height , width , channels ) . swapaxes ( 1 , 2 ) . reshape ( height * nrows , width * ncols , channels ) ) return img_grid img_arr = [] for k , file_name in enumerate ( bb . indices ): img = Image . open ( file_name ) . convert ( \"RGB\" ) bboxes = bb . bounding_boxes [ k ] for box in bboxes . bounding_boxes : cropped_face = np . asarray ( crop_resize ( img , box , image_size = 112 )) img_arr . append ( cropped_face ) result = image_grid ( np . array ( img_arr ), ncols = len ( img_arr )) fig = plt . figure ( figsize = ( 20.0 , 20.0 )) plt . imshow ( result ) Compute face similarity sim_mat = ff . features @ ff . features . T fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax = sns . heatmap ( sim_mat , cmap = \"PuRd\" , annot = True ) From this heatmap, we can see that the cosine similarities between the three faces of Alain Berset is quite high (from 0.7 to 0.74) while they are very low between all other faces.","title":"Face Similarity Search"},{"location":"examples/keyframes/","text":"Video Key-frames Extraction Model Import mlmodule modules from mlmodule.torch.datasets import ( LocalBinaryFilesDataset , ) from mlmodule.models.keyframes.pretrained import torch_keyframes_resnet_imagenet from mlmodule.models.keyframes.datasets import ( BinaryVideoCapture , extract_video_frames , ) from mlmodule.callbacks.memory import CollectVideoFramesInMemory from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner import torch import os Load a test video dataset = LocalBinaryFilesDataset ( [ os . path . join ( \"../../tests\" , \"fixtures\" , \"video\" , \"test.mp4\" )] ) Extract key-frames with torch_keyframes_resnet_imagenet torch_device = torch . device ( \"cpu\" ) # define model for keyframes extractor model = torch_keyframes_resnet_imagenet ( \"resnet18\" , device = torch_device ) features = CollectVideoFramesInMemory () runner = TorchInferenceRunner ( model = model , dataset = dataset , callbacks = [ features ], options = TorchRunnerOptions ( device = torch_device , data_loader_options = { \"batch_size\" : 1 }, tqdm_enabled = True ), ) runner . run () Visualise the extracted key-frames First install ipyplot # Install a pip package in the current Jupyter kernel import sys ! { sys . executable } - m pip install ipyplot Then extract the selected key-frames from the video with open ( os . path . join ( \"../../tests\" , \"fixtures\" , \"video\" , \"test.mp4\" ), mode = \"rb\" ) as video_file : with BinaryVideoCapture ( video_file ) as capture : video_frames = dict ( extract_video_frames ( capture )) frame_positions = sorted ( kf for kf in features . frames [ 0 ] . frame_indices ) selected_frames = [ video_frames [ i ] for i in frame_positions ] Finally, display the frames import ipyplot ipyplot . plot_images ( selected_frames , frame_positions , img_width = 250 )","title":"Video Key-frames Extraction Model"},{"location":"examples/keyframes/#video-key-frames-extraction-model","text":"Import mlmodule modules from mlmodule.torch.datasets import ( LocalBinaryFilesDataset , ) from mlmodule.models.keyframes.pretrained import torch_keyframes_resnet_imagenet from mlmodule.models.keyframes.datasets import ( BinaryVideoCapture , extract_video_frames , ) from mlmodule.callbacks.memory import CollectVideoFramesInMemory from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner import torch import os Load a test video dataset = LocalBinaryFilesDataset ( [ os . path . join ( \"../../tests\" , \"fixtures\" , \"video\" , \"test.mp4\" )] ) Extract key-frames with torch_keyframes_resnet_imagenet torch_device = torch . device ( \"cpu\" ) # define model for keyframes extractor model = torch_keyframes_resnet_imagenet ( \"resnet18\" , device = torch_device ) features = CollectVideoFramesInMemory () runner = TorchInferenceRunner ( model = model , dataset = dataset , callbacks = [ features ], options = TorchRunnerOptions ( device = torch_device , data_loader_options = { \"batch_size\" : 1 }, tqdm_enabled = True ), ) runner . run () Visualise the extracted key-frames First install ipyplot # Install a pip package in the current Jupyter kernel import sys ! { sys . executable } - m pip install ipyplot Then extract the selected key-frames from the video with open ( os . path . join ( \"../../tests\" , \"fixtures\" , \"video\" , \"test.mp4\" ), mode = \"rb\" ) as video_file : with BinaryVideoCapture ( video_file ) as capture : video_frames = dict ( extract_video_frames ( capture )) frame_positions = sorted ( kf for kf in features . frames [ 0 ] . frame_indices ) selected_frames = [ video_frames [ i ] for i in frame_positions ] Finally, display the frames import ipyplot ipyplot . plot_images ( selected_frames , frame_positions , img_width = 250 )","title":"Video Key-frames Extraction Model"},{"location":"examples/magface/","text":"Face Quality Assessment with MagFace Import mlmodule modules from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , CollectFeaturesInMemory , ) from mlmodule.torch.datasets import ( LocalBinaryFilesDataset , ImageDataset , ImageBoundingBoxDataset , ) from mlmodule.helpers.files import list_files_in_dir from mlmodule.models.mtcnn.pretrained import torch_mtcnn from mlmodule.models.magface.pretrained import torch_magface import torch import matplotlib.pyplot as plt import numpy as np from facenet_pytorch.models.utils.detect_face import crop_resize from PIL import Image import seaborn as sns sns . set ( style = \"white\" ) % matplotlib inline Load images base_path = \"../../tests/fixtures/remi_faces\" file_names = list_files_in_dir ( base_path , allowed_extensions = ( \"jpg\" ,)) remi_dataset = ImageDataset ( LocalBinaryFilesDataset ( file_names )) Run face detection with torch_mtcnn torch_device = \"cpu\" face_detector = torch_mtcnn ( device = torch_device ) # Callbacks bb = CollectBoundingBoxesInMemory () # Runner runner = TorchInferenceRunner ( model = face_detector , dataset = remi_dataset , callbacks = [ bb ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Extract face features with torch_magface magface = torch_magface ( device = torch_device ) # Dataset dataset = ImageBoundingBoxDataset ( image_dataset = ImageDataset ( LocalBinaryFilesDataset ( bb . indices )), bounding_boxes = bb . bounding_boxes , ) # Callbacks ff = CollectFeaturesInMemory () # Runner runner = TorchInferenceRunner ( model = magface , dataset = dataset , callbacks = [ ff ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 3 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Show face quality using feature magnitudes def image_grid ( array , ncols = 10 ): index , height , width , channels = array . shape nrows = index // ncols img_grid = ( array . reshape ( nrows , ncols , height , width , channels ) . swapaxes ( 1 , 2 ) . reshape ( height * nrows , width * ncols , channels ) ) return img_grid def display_faces_with_magnitude ( features , bounding_boxes , ncols = 10 ): # compute feature magnitudes mags = torch . linalg . norm ( torch . tensor ( features . features ), dim = 1 ) sort_idx = torch . argsort ( mags ) img_arr = [] for k in sort_idx : filename , face_index = features . indices [ k ] img = Image . open ( filename ) bbox_index = bounding_boxes . indices . index ( filename ) box = bounding_boxes . bounding_boxes [ bbox_index ] . bounding_boxes [ face_index ] cropped_face = np . asarray ( crop_resize ( img , box , image_size = 112 )) img_arr . append ( cropped_face ) if len ( img_arr ) % ncols : for i in range ( len ( img_arr ), ( len ( img_arr ) // ncols + 1 ) * ncols ): img_arr . append ( 255 * np . ones (( 112 , 112 , 3 ), np . uint8 )) result = image_grid ( np . array ( img_arr ), ncols = ncols ) fig = plt . figure ( figsize = ( 20.0 , 20.0 )) plt . imshow ( result ) print ( \"feature magnitude: {} \" . format ( [ float ( \" {0:.2f} \" . format ( mags [ idx_ ] . item ())) for idx_ in sort_idx ] ) ) return sort_idx sort_idx = display_faces_with_magnitude ( ff , bb , ncols = 11 ) We can see that there exists a positive correlation between the quality of the faces and the magnitude of the MagFace feature vectors. Display face similarity normalized_features = torch . nn . functional . normalize ( torch . tensor ( ff . features ))[ sort_idx ] sim_mat = normalized_features @ normalized_features . T fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax = sns . heatmap ( sim_mat , cmap = \"PuRd\" , annot = True ) As we can see on the heatmap below, face quality strongly affects face recognition performance as the cosine similarities between bad quality faces and high quality faces are much smaller.","title":"Face Quality Assessment with MagFace"},{"location":"examples/magface/#face-quality-assessment-with-magface","text":"Import mlmodule modules from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , CollectFeaturesInMemory , ) from mlmodule.torch.datasets import ( LocalBinaryFilesDataset , ImageDataset , ImageBoundingBoxDataset , ) from mlmodule.helpers.files import list_files_in_dir from mlmodule.models.mtcnn.pretrained import torch_mtcnn from mlmodule.models.magface.pretrained import torch_magface import torch import matplotlib.pyplot as plt import numpy as np from facenet_pytorch.models.utils.detect_face import crop_resize from PIL import Image import seaborn as sns sns . set ( style = \"white\" ) % matplotlib inline Load images base_path = \"../../tests/fixtures/remi_faces\" file_names = list_files_in_dir ( base_path , allowed_extensions = ( \"jpg\" ,)) remi_dataset = ImageDataset ( LocalBinaryFilesDataset ( file_names )) Run face detection with torch_mtcnn torch_device = \"cpu\" face_detector = torch_mtcnn ( device = torch_device ) # Callbacks bb = CollectBoundingBoxesInMemory () # Runner runner = TorchInferenceRunner ( model = face_detector , dataset = remi_dataset , callbacks = [ bb ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Extract face features with torch_magface magface = torch_magface ( device = torch_device ) # Dataset dataset = ImageBoundingBoxDataset ( image_dataset = ImageDataset ( LocalBinaryFilesDataset ( bb . indices )), bounding_boxes = bb . bounding_boxes , ) # Callbacks ff = CollectFeaturesInMemory () # Runner runner = TorchInferenceRunner ( model = magface , dataset = dataset , callbacks = [ ff ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 3 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Show face quality using feature magnitudes def image_grid ( array , ncols = 10 ): index , height , width , channels = array . shape nrows = index // ncols img_grid = ( array . reshape ( nrows , ncols , height , width , channels ) . swapaxes ( 1 , 2 ) . reshape ( height * nrows , width * ncols , channels ) ) return img_grid def display_faces_with_magnitude ( features , bounding_boxes , ncols = 10 ): # compute feature magnitudes mags = torch . linalg . norm ( torch . tensor ( features . features ), dim = 1 ) sort_idx = torch . argsort ( mags ) img_arr = [] for k in sort_idx : filename , face_index = features . indices [ k ] img = Image . open ( filename ) bbox_index = bounding_boxes . indices . index ( filename ) box = bounding_boxes . bounding_boxes [ bbox_index ] . bounding_boxes [ face_index ] cropped_face = np . asarray ( crop_resize ( img , box , image_size = 112 )) img_arr . append ( cropped_face ) if len ( img_arr ) % ncols : for i in range ( len ( img_arr ), ( len ( img_arr ) // ncols + 1 ) * ncols ): img_arr . append ( 255 * np . ones (( 112 , 112 , 3 ), np . uint8 )) result = image_grid ( np . array ( img_arr ), ncols = ncols ) fig = plt . figure ( figsize = ( 20.0 , 20.0 )) plt . imshow ( result ) print ( \"feature magnitude: {} \" . format ( [ float ( \" {0:.2f} \" . format ( mags [ idx_ ] . item ())) for idx_ in sort_idx ] ) ) return sort_idx sort_idx = display_faces_with_magnitude ( ff , bb , ncols = 11 ) We can see that there exists a positive correlation between the quality of the faces and the magnitude of the MagFace feature vectors. Display face similarity normalized_features = torch . nn . functional . normalize ( torch . tensor ( ff . features ))[ sort_idx ] sim_mat = normalized_features @ normalized_features . T fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax = sns . heatmap ( sim_mat , cmap = \"PuRd\" , annot = True ) As we can see on the heatmap below, face quality strongly affects face recognition performance as the cosine similarities between bad quality faces and high quality faces are much smaller.","title":"Face Quality Assessment with MagFace"},{"location":"examples/test_distiluse_multilingual/","text":"Testing multilingual semantic text similarity from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.torch.options import TorchRunnerOptions from mlmodule.models.sentences import torch_distiluse_base_multilingual_v2 from mlmodule.torch.datasets import ListDataset from mlmodule.callbacks.memory import CollectFeaturesInMemory import torch import matplotlib.pyplot as plt import seaborn as sns sns . set ( style = \"white\" ) % matplotlib inline Create lists of sentences in multiple languages Random sentences selected from https://tatoeba.org/en/ sentences1 = [ \"We were preparing food.\" , \"Ni preparis man\u011da\u0135on.\" , \"Wij maakten eten klaar.\" , \"Nous pr\u00e9par\u00e2mes de la nourriture.\" , \"\u305f\u3061\u306f\u98df\u4e8b\u306e\u6e96\u5099\u3092\u3057\u305f\u3002\" , \"Preparamos comida.\" , ] sentences2 = [ \"An\u0125izo interpretas la orakolon, kaj konvinkas la Trojanojn, ke temas pri la insulo Kreto, el kiu eliris unu el la unuatempaj fondintoj de Trojo.\" , \"Anchise explique l'oracle, et persuade aux Troyens qu'il s'agit de l'\u00eele de Cr\u00e8te, d'o\u00f9 est sorti un des anciens fondateurs de Troie.\" , \"Anquises interpreta o or\u00e1culo e convence os troianos de que se trata da ilha de Creta, da qual saiu um dos antigos fundadores de Troia.\" , ] sentences3 = [ \"Mi pensas, ke mi devus foriri, \u0109ar jam estas malfrue.\" , \"Je crois que je devrais partir car il se fait tard.\" , \"I think I must be leaving since it is getting late.\" , ] Define a ListDataset dset = ListDataset ( sentences1 + sentences2 + sentences3 ) Extract sentence feature vectors # define sentence embedding model torch_device = torch . device ( \"cpu\" ) model = torch_distiluse_base_multilingual_v2 ( device = torch_device ) # define callback for collecting features sentence_features = CollectFeaturesInMemory () # define the runner runner = TorchInferenceRunner ( dataset = dset , model = model , callbacks = [ sentence_features ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = model . device , tqdm_enabled = True ), ) runner . run () Compute sentence similarities cos_sim = sentence_features . features @ sentence_features . features . T fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax = sns . heatmap ( cos_sim , cmap = \"PuRd\" , annot = True , fmt = \".1\" ) We can see on the heatmap below that similar sentences in multiple languages have high cosine similarity scores, while other sentences have low similarities.","title":"Testing multilingual semantic text similarity"},{"location":"examples/test_distiluse_multilingual/#testing-multilingual-semantic-text-similarity","text":"from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.torch.options import TorchRunnerOptions from mlmodule.models.sentences import torch_distiluse_base_multilingual_v2 from mlmodule.torch.datasets import ListDataset from mlmodule.callbacks.memory import CollectFeaturesInMemory import torch import matplotlib.pyplot as plt import seaborn as sns sns . set ( style = \"white\" ) % matplotlib inline Create lists of sentences in multiple languages Random sentences selected from https://tatoeba.org/en/ sentences1 = [ \"We were preparing food.\" , \"Ni preparis man\u011da\u0135on.\" , \"Wij maakten eten klaar.\" , \"Nous pr\u00e9par\u00e2mes de la nourriture.\" , \"\u305f\u3061\u306f\u98df\u4e8b\u306e\u6e96\u5099\u3092\u3057\u305f\u3002\" , \"Preparamos comida.\" , ] sentences2 = [ \"An\u0125izo interpretas la orakolon, kaj konvinkas la Trojanojn, ke temas pri la insulo Kreto, el kiu eliris unu el la unuatempaj fondintoj de Trojo.\" , \"Anchise explique l'oracle, et persuade aux Troyens qu'il s'agit de l'\u00eele de Cr\u00e8te, d'o\u00f9 est sorti un des anciens fondateurs de Troie.\" , \"Anquises interpreta o or\u00e1culo e convence os troianos de que se trata da ilha de Creta, da qual saiu um dos antigos fundadores de Troia.\" , ] sentences3 = [ \"Mi pensas, ke mi devus foriri, \u0109ar jam estas malfrue.\" , \"Je crois que je devrais partir car il se fait tard.\" , \"I think I must be leaving since it is getting late.\" , ] Define a ListDataset dset = ListDataset ( sentences1 + sentences2 + sentences3 ) Extract sentence feature vectors # define sentence embedding model torch_device = torch . device ( \"cpu\" ) model = torch_distiluse_base_multilingual_v2 ( device = torch_device ) # define callback for collecting features sentence_features = CollectFeaturesInMemory () # define the runner runner = TorchInferenceRunner ( dataset = dset , model = model , callbacks = [ sentence_features ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = model . device , tqdm_enabled = True ), ) runner . run () Compute sentence similarities cos_sim = sentence_features . features @ sentence_features . features . T fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax = sns . heatmap ( cos_sim , cmap = \"PuRd\" , annot = True , fmt = \".1\" ) We can see on the heatmap below that similar sentences in multiple languages have high cosine similarity scores, while other sentences have low similarities.","title":"Testing multilingual semantic text similarity"},{"location":"examples/text_to_retrieval_with_clip/","text":"Text-to-image retrieval with CLIP This is an example of a text-to-Image retrieval engine based on OpenAI CLIP model Import mlmodule modules for the task from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.torch.options import TorchRunnerOptions from mlmodule.callbacks.memory import ( CollectFeaturesInMemory , ) from mlmodule.torch.datasets import ( ImageDataset , ListDataset , LocalBinaryFilesDataset , ) from mlmodule.helpers.files import list_files_in_dir from mlmodule.models.clip.text import CLIPTextModule from mlmodule.models.clip.image import CLIPImageModule from mlmodule.states import StateKey from mlmodule.stores import Store import torch Load CLIP Image Encoder image_encoder = CLIPImageModule ( clip_model_name = \"ViT-B/32\" , device = torch . device ( \"cuda\" )) store = Store () store . load ( image_encoder , StateKey ( image_encoder . state_type , \"clip\" )) Extract CLIP image features of FlickR30k dataset It might take a few minutes for extracting the features... path_to_flickr30k_images = \"/mnt/storage01/datasets/flickr30k/full/images\" file_names = list_files_in_dir ( path_to_flickr30k_images , allowed_extensions = ( \"jpg\" ,)) dataset = ImageDataset ( LocalBinaryFilesDataset ( file_names )) image_features = CollectFeaturesInMemory () runner = TorchInferenceRunner ( dataset = dataset , model = image_encoder , callbacks = [ image_features ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 128 }, device = image_encoder . device , tqdm_enabled = True , ), ) runner . run () Load CLIP Text Encoder text_encoder = CLIPTextModule ( image_encoder . clip_model_name , device = torch . device ( \"cpu\" )) store . load ( text_encoder , StateKey ( text_encoder . state_type , \"clip\" )) Extract CLIP text features of a given query text_queries = [ \"Workers look down from up above on a piece of equipment .\" , \"Ballet dancers in a studio practice jumping with wonderful form .\" , ] dataset = ListDataset ( text_queries ) text_features = CollectFeaturesInMemory () runner = TorchInferenceRunner ( dataset = dataset , model = text_encoder , callbacks = [ text_features ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = text_encoder . device , tqdm_enabled = True , ), ) runner . run () Text-to-image retrieval engine Pick the top 5 most similar images for the text query img_feat = torch . tensor ( image_features . features ) . type ( torch . float32 ) img_feat /= img_feat . norm ( dim =- 1 , keepdim = True ) txt_feat = torch . tensor ( text_features . features ) txt_feat /= txt_feat . norm ( dim =- 1 , keepdim = True ) similarity = ( 100.0 * txt_feat @ img_feat . T ) . softmax ( dim =- 1 ) values , indices = similarity . topk ( 5 ) Display the results # Install a pip package in the current Jupyter kernel import sys ! { sys . executable } - m pip install ipyplot import ipyplot from PIL import Image for k , text in enumerate ( text_queries ): print ( f \"Query: { text } \" ) print ( f \"Top 5 images:\" ) ipyplot . plot_images ( [ Image . open ( image_features . indices [ i ]) for i in indices [ k ]], [ f \" { v * 100 : .1f } %\" for v in values [ k ]], img_width = 250 , )","title":"Text-to-image retrieval with CLIP"},{"location":"examples/text_to_retrieval_with_clip/#text-to-image-retrieval-with-clip","text":"This is an example of a text-to-Image retrieval engine based on OpenAI CLIP model Import mlmodule modules for the task from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.torch.options import TorchRunnerOptions from mlmodule.callbacks.memory import ( CollectFeaturesInMemory , ) from mlmodule.torch.datasets import ( ImageDataset , ListDataset , LocalBinaryFilesDataset , ) from mlmodule.helpers.files import list_files_in_dir from mlmodule.models.clip.text import CLIPTextModule from mlmodule.models.clip.image import CLIPImageModule from mlmodule.states import StateKey from mlmodule.stores import Store import torch Load CLIP Image Encoder image_encoder = CLIPImageModule ( clip_model_name = \"ViT-B/32\" , device = torch . device ( \"cuda\" )) store = Store () store . load ( image_encoder , StateKey ( image_encoder . state_type , \"clip\" )) Extract CLIP image features of FlickR30k dataset It might take a few minutes for extracting the features... path_to_flickr30k_images = \"/mnt/storage01/datasets/flickr30k/full/images\" file_names = list_files_in_dir ( path_to_flickr30k_images , allowed_extensions = ( \"jpg\" ,)) dataset = ImageDataset ( LocalBinaryFilesDataset ( file_names )) image_features = CollectFeaturesInMemory () runner = TorchInferenceRunner ( dataset = dataset , model = image_encoder , callbacks = [ image_features ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 128 }, device = image_encoder . device , tqdm_enabled = True , ), ) runner . run () Load CLIP Text Encoder text_encoder = CLIPTextModule ( image_encoder . clip_model_name , device = torch . device ( \"cpu\" )) store . load ( text_encoder , StateKey ( text_encoder . state_type , \"clip\" )) Extract CLIP text features of a given query text_queries = [ \"Workers look down from up above on a piece of equipment .\" , \"Ballet dancers in a studio practice jumping with wonderful form .\" , ] dataset = ListDataset ( text_queries ) text_features = CollectFeaturesInMemory () runner = TorchInferenceRunner ( dataset = dataset , model = text_encoder , callbacks = [ text_features ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = text_encoder . device , tqdm_enabled = True , ), ) runner . run () Text-to-image retrieval engine Pick the top 5 most similar images for the text query img_feat = torch . tensor ( image_features . features ) . type ( torch . float32 ) img_feat /= img_feat . norm ( dim =- 1 , keepdim = True ) txt_feat = torch . tensor ( text_features . features ) txt_feat /= txt_feat . norm ( dim =- 1 , keepdim = True ) similarity = ( 100.0 * txt_feat @ img_feat . T ) . softmax ( dim =- 1 ) values , indices = similarity . topk ( 5 ) Display the results # Install a pip package in the current Jupyter kernel import sys ! { sys . executable } - m pip install ipyplot import ipyplot from PIL import Image for k , text in enumerate ( text_queries ): print ( f \"Query: { text } \" ) print ( f \"Top 5 images:\" ) ipyplot . plot_images ( [ Image . open ( image_features . indices [ i ]) for i in indices [ k ]], [ f \" { v * 100 : .1f } %\" for v in values [ k ]], img_width = 250 , )","title":"Text-to-image retrieval with CLIP"},{"location":"examples/train_cifar10/","text":"CIFAR10 Image Classification training In this notebook, we are training CIFAR10 image classification on top of ResNet18 features from ImageNet Import mlmodule modules for the task from mlmodule.models.classification import LinearClassifierTorchModule from mlmodule.models.resnet.pretrained import torch_resnet_imagenet from mlmodule.models.densenet.pretrained import torch_densenet_imagenet from mlmodule.torch.runners import TorchTrainingRunner from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.torch.runners import TorchInferenceMultiGPURunner from mlmodule.torch.options import TorchTrainingOptions from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.options import TorchMultiGPURunnerOptions from mlmodule.labels.base import LabelSet from mlmodule.torch.datasets import ( ListDataset , ListDatasetIndexed , TorchTrainingDataset , ) from mlmodule.callbacks.memory import CollectFeaturesInMemory , CollectLabelsInMemory from mlmodule.callbacks.states import SaveModelState from mlmodule.stores.local import LocalStateStore from mlmodule.states import StateKey import torch import torch.nn.functional as F import torch.optim as optim from torchvision.datasets import CIFAR10 from ignite.metrics import Precision , Recall , Loss , Accuracy from sklearn.metrics import classification_report import os Enable logging into notebook import logging import sys logging . basicConfig ( format = \" %(asctime)s | %(levelname)s : %(message)s \" , level = logging . INFO , stream = sys . stdout , ) Load CIFAR10 dataset from torchvision root_dir = os . path . join ( os . environ [ \"HOME\" ], \"torchvision-datasets\" ) train_cifar10 = CIFAR10 ( root = root_dir , train = True , download = True , transform = None ) Format inputs and labels for mlmodule labels_dict = { 0 : \"airplane\" , 1 : \"automobile\" , 2 : \"bird\" , 3 : \"cat\" , 4 : \"deer\" , 5 : \"dog\" , 6 : \"frog\" , 7 : \"horse\" , 8 : \"ship\" , 9 : \"truck\" , } train_samples = [( img , labels_dict [ label ]) for img , label in train_cifar10 ] train_images , train_labels = zip ( * train_samples ) Load resnet18 pre-trained model torch_device = \"cuda\" resnet = torch_resnet_imagenet ( resnet_arch = \"resnet18\" , device = torch_device , training_mode = \"features\" ) Extract image features from ResNet with a single GPU # Callbacks ff_train_resnet = CollectFeaturesInMemory () # Runner runner = TorchInferenceRunner ( model = resnet , dataset = ListDataset ( train_images ), callbacks = [ ff_train_resnet ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Create train and validation splits # define the set of labels label_set = LabelSet ( label_set_unique_id = \"cifar10\" , label_list = list ( labels_dict . values ()) ) # split samples into train and valid sets train_indices , valid_indices = torch . split ( torch . randperm ( len ( ff_train_resnet . indices )), int ( len ( ff_train_resnet . indices ) * 0.9 ), ) # define training set train_dset = TorchTrainingDataset ( dataset = ListDatasetIndexed ( train_indices , ff_train_resnet . features [ train_indices ]), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in train_indices ]), ) # define valid set valid_dset = TorchTrainingDataset ( dataset = ListDatasetIndexed ( valid_indices , ff_train_resnet . features [ valid_indices ]), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in valid_indices ]), ) Train the image classifier using TorchTrainingRunner module # define a classifier on top of resnet features classifier_resnet = LinearClassifierTorchModule ( in_features = ff_train_resnet . features . shape [ 1 ], label_set = label_set ) # define the evaluation metrics precision = Precision ( average = False ) recall = Recall ( average = False ) F1 = ( precision * recall * 2 / ( precision + recall )) . mean () # Callbacks exp_dir = os . path . join ( os . environ [ \"HOME\" ], \"mlmodule-training\" ) os . makedirs ( exp_dir , exist_ok = True ) resnet_state = SaveModelState ( store = LocalStateStore ( exp_dir ), state_key = StateKey ( classifier_resnet . state_type , \"train-resnet\" ), ) # define a loss function loss_fn = F . cross_entropy # define the trainer trainer = TorchTrainingRunner ( model = classifier_resnet , dataset = ( train_dset , valid_dset ), callbacks = [ resnet_state ], options = TorchTrainingOptions ( data_loader_options = { \"batch_size\" : 32 }, criterion = loss_fn , optimizer = optim . Adam ( classifier_resnet . parameters (), lr = 1e-3 ), metrics = { \"pre\" : precision , \"recall\" : recall , \"f1\" : F1 , \"acc\" : Accuracy (), \"ce_loss\" : Loss ( loss_fn ), }, validate_every = 1 , checkpoint_every = 3 , num_epoch = 3 , tqdm_enabled = True , ), ) trainer . run () Do evaluation on the test set test_cifar10 = CIFAR10 ( root = root_dir , train = False , download = True , transform = None ) test_samples = [( img , labels_dict [ label ]) for img , label in test_cifar10 ] test_images , test_labels = zip ( * test_samples ) # Callbacks ff_test_resnet = CollectFeaturesInMemory () score_test_resnet = CollectLabelsInMemory () # Extract the image features runner = TorchInferenceRunner ( model = resnet , dataset = ListDataset ( test_images ), callbacks = [ ff_test_resnet ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () # Do the predictions runner = TorchInferenceRunner ( model = classifier_resnet , dataset = ListDataset ( ff_test_resnet . features ), callbacks = [ score_test_resnet ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Print classification report print ( classification_report ( test_labels , score_test_resnet . labels )) Compare the classification performance with a deeper DenseNet model Load densenet201 model torch_device = \"cuda\" densenet = torch_densenet_imagenet ( densenet_arch = \"densenet201\" , device = torch_device , ) Extract image features with multiple gpus # Callbacks ff_train_densenet = CollectFeaturesInMemory () # Runner runner = TorchInferenceMultiGPURunner ( model = densenet , dataset = ListDataset ( train_images ), callbacks = [ ff_train_densenet ], options = TorchMultiGPURunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, tqdm_enabled = True ), ) runner . run () Train a new classifier on top of densenet201 features # define training set train_dset_densenet = TorchTrainingDataset ( dataset = ListDatasetIndexed ( train_indices , ff_train_densenet . features [ train_indices ] ), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in train_indices ]), ) # define valid set valid_dset_densenet = TorchTrainingDataset ( dataset = ListDatasetIndexed ( valid_indices , ff_train_densenet . features [ valid_indices ] ), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in valid_indices ]), ) # define a classifier on top of resnet features classifier_densenet = LinearClassifierTorchModule ( in_features = ff_train_densenet . features . shape [ 1 ], label_set = label_set ) # save state of the classifier on top of densenet features densenet_state = SaveModelState ( store = LocalStateStore ( exp_dir ), state_key = StateKey ( classifier_densenet . state_type , \"train-densenet\" ), ) # define the trainer trainer = TorchTrainingRunner ( model = classifier_densenet , dataset = ( train_dset_densenet , valid_dset_densenet ), callbacks = [ densenet_state ], options = TorchTrainingOptions ( data_loader_options = { \"batch_size\" : 32 }, criterion = loss_fn , optimizer = optim . Adam ( classifier_densenet . parameters (), lr = 1e-3 ), metrics = { \"pre\" : precision , \"recall\" : recall , \"f1\" : F1 , \"acc\" : Accuracy (), \"ce_loss\" : Loss ( loss_fn ), }, validate_every = 1 , checkpoint_every = 3 , num_epoch = 3 , tqdm_enabled = True , ), ) trainer . run () # Callbacks ff_test_densenet = CollectFeaturesInMemory () score_test_densenet = CollectLabelsInMemory () # Extract the image features runner = TorchInferenceMultiGPURunner ( model = densenet , dataset = ListDataset ( test_images ), callbacks = [ ff_test_densenet ], options = TorchMultiGPURunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, tqdm_enabled = True ), ) runner . run () # Do the predictions runner = TorchInferenceMultiGPURunner ( model = classifier_densenet , dataset = ListDataset ( ff_test_densenet . features ), callbacks = [ score_test_densenet ], options = TorchMultiGPURunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, tqdm_enabled = True ), ) runner . run () print ( classification_report ( test_labels , score_test_densenet . labels )) From the classification report we can see that performance with densenet201 are much better than the performance with resnet18.","title":"CIFAR10 Image Classification training"},{"location":"examples/train_cifar10/#cifar10-image-classification-training","text":"In this notebook, we are training CIFAR10 image classification on top of ResNet18 features from ImageNet Import mlmodule modules for the task from mlmodule.models.classification import LinearClassifierTorchModule from mlmodule.models.resnet.pretrained import torch_resnet_imagenet from mlmodule.models.densenet.pretrained import torch_densenet_imagenet from mlmodule.torch.runners import TorchTrainingRunner from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.torch.runners import TorchInferenceMultiGPURunner from mlmodule.torch.options import TorchTrainingOptions from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.options import TorchMultiGPURunnerOptions from mlmodule.labels.base import LabelSet from mlmodule.torch.datasets import ( ListDataset , ListDatasetIndexed , TorchTrainingDataset , ) from mlmodule.callbacks.memory import CollectFeaturesInMemory , CollectLabelsInMemory from mlmodule.callbacks.states import SaveModelState from mlmodule.stores.local import LocalStateStore from mlmodule.states import StateKey import torch import torch.nn.functional as F import torch.optim as optim from torchvision.datasets import CIFAR10 from ignite.metrics import Precision , Recall , Loss , Accuracy from sklearn.metrics import classification_report import os Enable logging into notebook import logging import sys logging . basicConfig ( format = \" %(asctime)s | %(levelname)s : %(message)s \" , level = logging . INFO , stream = sys . stdout , ) Load CIFAR10 dataset from torchvision root_dir = os . path . join ( os . environ [ \"HOME\" ], \"torchvision-datasets\" ) train_cifar10 = CIFAR10 ( root = root_dir , train = True , download = True , transform = None ) Format inputs and labels for mlmodule labels_dict = { 0 : \"airplane\" , 1 : \"automobile\" , 2 : \"bird\" , 3 : \"cat\" , 4 : \"deer\" , 5 : \"dog\" , 6 : \"frog\" , 7 : \"horse\" , 8 : \"ship\" , 9 : \"truck\" , } train_samples = [( img , labels_dict [ label ]) for img , label in train_cifar10 ] train_images , train_labels = zip ( * train_samples ) Load resnet18 pre-trained model torch_device = \"cuda\" resnet = torch_resnet_imagenet ( resnet_arch = \"resnet18\" , device = torch_device , training_mode = \"features\" ) Extract image features from ResNet with a single GPU # Callbacks ff_train_resnet = CollectFeaturesInMemory () # Runner runner = TorchInferenceRunner ( model = resnet , dataset = ListDataset ( train_images ), callbacks = [ ff_train_resnet ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Create train and validation splits # define the set of labels label_set = LabelSet ( label_set_unique_id = \"cifar10\" , label_list = list ( labels_dict . values ()) ) # split samples into train and valid sets train_indices , valid_indices = torch . split ( torch . randperm ( len ( ff_train_resnet . indices )), int ( len ( ff_train_resnet . indices ) * 0.9 ), ) # define training set train_dset = TorchTrainingDataset ( dataset = ListDatasetIndexed ( train_indices , ff_train_resnet . features [ train_indices ]), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in train_indices ]), ) # define valid set valid_dset = TorchTrainingDataset ( dataset = ListDatasetIndexed ( valid_indices , ff_train_resnet . features [ valid_indices ]), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in valid_indices ]), ) Train the image classifier using TorchTrainingRunner module # define a classifier on top of resnet features classifier_resnet = LinearClassifierTorchModule ( in_features = ff_train_resnet . features . shape [ 1 ], label_set = label_set ) # define the evaluation metrics precision = Precision ( average = False ) recall = Recall ( average = False ) F1 = ( precision * recall * 2 / ( precision + recall )) . mean () # Callbacks exp_dir = os . path . join ( os . environ [ \"HOME\" ], \"mlmodule-training\" ) os . makedirs ( exp_dir , exist_ok = True ) resnet_state = SaveModelState ( store = LocalStateStore ( exp_dir ), state_key = StateKey ( classifier_resnet . state_type , \"train-resnet\" ), ) # define a loss function loss_fn = F . cross_entropy # define the trainer trainer = TorchTrainingRunner ( model = classifier_resnet , dataset = ( train_dset , valid_dset ), callbacks = [ resnet_state ], options = TorchTrainingOptions ( data_loader_options = { \"batch_size\" : 32 }, criterion = loss_fn , optimizer = optim . Adam ( classifier_resnet . parameters (), lr = 1e-3 ), metrics = { \"pre\" : precision , \"recall\" : recall , \"f1\" : F1 , \"acc\" : Accuracy (), \"ce_loss\" : Loss ( loss_fn ), }, validate_every = 1 , checkpoint_every = 3 , num_epoch = 3 , tqdm_enabled = True , ), ) trainer . run () Do evaluation on the test set test_cifar10 = CIFAR10 ( root = root_dir , train = False , download = True , transform = None ) test_samples = [( img , labels_dict [ label ]) for img , label in test_cifar10 ] test_images , test_labels = zip ( * test_samples ) # Callbacks ff_test_resnet = CollectFeaturesInMemory () score_test_resnet = CollectLabelsInMemory () # Extract the image features runner = TorchInferenceRunner ( model = resnet , dataset = ListDataset ( test_images ), callbacks = [ ff_test_resnet ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () # Do the predictions runner = TorchInferenceRunner ( model = classifier_resnet , dataset = ListDataset ( ff_test_resnet . features ), callbacks = [ score_test_resnet ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Print classification report print ( classification_report ( test_labels , score_test_resnet . labels )) Compare the classification performance with a deeper DenseNet model Load densenet201 model torch_device = \"cuda\" densenet = torch_densenet_imagenet ( densenet_arch = \"densenet201\" , device = torch_device , ) Extract image features with multiple gpus # Callbacks ff_train_densenet = CollectFeaturesInMemory () # Runner runner = TorchInferenceMultiGPURunner ( model = densenet , dataset = ListDataset ( train_images ), callbacks = [ ff_train_densenet ], options = TorchMultiGPURunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, tqdm_enabled = True ), ) runner . run () Train a new classifier on top of densenet201 features # define training set train_dset_densenet = TorchTrainingDataset ( dataset = ListDatasetIndexed ( train_indices , ff_train_densenet . features [ train_indices ] ), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in train_indices ]), ) # define valid set valid_dset_densenet = TorchTrainingDataset ( dataset = ListDatasetIndexed ( valid_indices , ff_train_densenet . features [ valid_indices ] ), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in valid_indices ]), ) # define a classifier on top of resnet features classifier_densenet = LinearClassifierTorchModule ( in_features = ff_train_densenet . features . shape [ 1 ], label_set = label_set ) # save state of the classifier on top of densenet features densenet_state = SaveModelState ( store = LocalStateStore ( exp_dir ), state_key = StateKey ( classifier_densenet . state_type , \"train-densenet\" ), ) # define the trainer trainer = TorchTrainingRunner ( model = classifier_densenet , dataset = ( train_dset_densenet , valid_dset_densenet ), callbacks = [ densenet_state ], options = TorchTrainingOptions ( data_loader_options = { \"batch_size\" : 32 }, criterion = loss_fn , optimizer = optim . Adam ( classifier_densenet . parameters (), lr = 1e-3 ), metrics = { \"pre\" : precision , \"recall\" : recall , \"f1\" : F1 , \"acc\" : Accuracy (), \"ce_loss\" : Loss ( loss_fn ), }, validate_every = 1 , checkpoint_every = 3 , num_epoch = 3 , tqdm_enabled = True , ), ) trainer . run () # Callbacks ff_test_densenet = CollectFeaturesInMemory () score_test_densenet = CollectLabelsInMemory () # Extract the image features runner = TorchInferenceMultiGPURunner ( model = densenet , dataset = ListDataset ( test_images ), callbacks = [ ff_test_densenet ], options = TorchMultiGPURunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, tqdm_enabled = True ), ) runner . run () # Do the predictions runner = TorchInferenceMultiGPURunner ( model = classifier_densenet , dataset = ListDataset ( ff_test_densenet . features ), callbacks = [ score_test_densenet ], options = TorchMultiGPURunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, tqdm_enabled = True ), ) runner . run () print ( classification_report ( test_labels , score_test_densenet . labels )) From the classification report we can see that performance with densenet201 are much better than the performance with resnet18.","title":"CIFAR10 Image Classification training"},{"location":"examples/train_face_age_detection/","text":"Train an age detection model based on ArcFace features Import MLModule modules from mlmodule.models.arcface.pretrained import torch_arcface_insightface from mlmodule.models.mtcnn.pretrained import torch_mtcnn from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.models.classification import LinearClassifierTorchModule from mlmodule.torch.datasets import TorchTrainingDataset from mlmodule.torch.runners import TorchTrainingRunner from mlmodule.torch.options import TorchTrainingOptions from mlmodule.torch.options import TorchRunnerOptions from mlmodule.labels.base import LabelSet from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , CollectFeaturesInMemory , CollectLabelsInMemory , ) from mlmodule.torch.datasets import ( ImageBoundingBoxDataset , ListDataset , LocalBinaryFilesDataset , ImageDataset , ) from mlmodule.helpers.files import list_files_in_dir from ignite.metrics import Precision , Recall , Loss , Accuracy import torch.nn.functional as F import torch.optim as optim from sklearn.metrics import classification_report import os import re Enable logging inside notebook import logging import sys logging . basicConfig ( format = \" %(asctime)s | %(levelname)s : %(message)s \" , level = logging . INFO , stream = sys . stdout , ) First download UTKFace_inthewild dataset from https://drive.google.com/drive/folders/1HROmgviy4jUUUaCdvvrQ8PcqtNg2jn3G Download and extract the 3 parts and place the folder UTKFace_inthewild in your home directory. part1 will serve as training set, while part2 will be our test set and part3 our valid set. path_to_utkface = os . path . join ( os . environ [ \"HOME\" ], \"UTKFace_inthewild\" ) train_filenames = list_files_in_dir ( os . path . join ( path_to_utkface , \"part1\" ), allowed_extensions = ( \"jpg\" ,) ) test_filenames = list_files_in_dir ( os . path . join ( path_to_utkface , \"part2\" ), allowed_extensions = ( \"jpg\" ,) ) valid_filenames = list_files_in_dir ( os . path . join ( path_to_utkface , \"part3\" ), allowed_extensions = ( \"jpg\" ,) ) train_dataset = ImageDataset ( LocalBinaryFilesDataset ( train_filenames )) test_dataset = ImageDataset ( LocalBinaryFilesDataset ( test_filenames )) valid_dataset = ImageDataset ( LocalBinaryFilesDataset ( valid_filenames )) Extract pretrained ArcFace features for all images torch_device = \"cuda\" face_detector = torch_mtcnn ( device = torch_device ) face_extractor = torch_arcface_insightface ( device = torch_device ) def get_face_features ( dset ): # Callbacks bb = CollectBoundingBoxesInMemory () ff = CollectFeaturesInMemory () # Face detection runner runner = TorchInferenceRunner ( model = face_detector , dataset = dset , callbacks = [ bb ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = torch_device , tqdm_enabled = True , ), ) runner . run () # Dataset with detected faces dataset = ImageBoundingBoxDataset ( image_dataset = ImageDataset ( LocalBinaryFilesDataset ( bb . indices )), bounding_boxes = bb . bounding_boxes , ) # Face extraction runner runner = TorchInferenceRunner ( model = face_extractor , dataset = dataset , callbacks = [ ff ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True , ), ) runner . run () return bb , ff train_bb , train_ff = get_face_features ( train_dataset ) test_bb , test_ff = get_face_features ( test_dataset ) valid_bb , valid_ff = get_face_features ( valid_dataset ) Discretising the age into six categories childhood = { i : \"childhood\" for i in range ( 0 , 8 )} puberty = { i : \"puberty\" for i in range ( 8 , 13 )} adolescence = { i : \"adolescence\" for i in range ( 13 , 18 )} adulthood = { i : \"adulthood\" for i in range ( 18 , 35 )} middle_age = { i : \"middle_age\" for i in range ( 35 , 50 )} seniority = { i : \"seniority\" for i in range ( 50 , 120 )} age2label = { ** childhood , ** puberty , ** adolescence , ** adulthood , ** middle_age , ** seniority , } def discretize_age ( filenames ): labels = {} for img_path in filenames : m = re . search ( \"(\\d+)_.*[.jpg]\" , img_path ) if m : age = int ( m . group ( 1 )) labels [ img_path ] = age2label [ age ] else : print ( f \" { img_path } failed\" ) assert len ( labels ) == len ( filenames ) return labels train_labels = discretize_age ( train_filenames ) test_labels = discretize_age ( test_filenames ) valid_labels = discretize_age ( valid_filenames ) label_set = LabelSet ( label_set_unique_id = \"age\" , label_list = [ \"childhood\" , \"puberty\" , \"adolescence\" , \"adulthood\" , \"middle_age\" , \"seniority\" , ], ) Define train and validation set for training the classifier # define train set train_dset = TorchTrainingDataset ( dataset = ListDataset ( train_ff . features ), targets = label_set . get_label_ids ( [ train_labels [ img_path ] for img_path , _ in train_ff . indices ] ), ) # define valid set valid_dset = TorchTrainingDataset ( dataset = ListDataset ( valid_ff . features ), targets = label_set . get_label_ids ( [ valid_labels [ img_path ] for img_path , _ in valid_ff . indices ] ), ) Define a linear classifier on top of the extracted features age_classifier = LinearClassifierTorchModule ( in_features = train_ff . features . shape [ 1 ], label_set = label_set ) Training runner precision = Precision ( average = False ) recall = Recall ( average = False ) F1 = ( precision * recall * 2 / ( precision + recall )) . mean () loss_fn = F . cross_entropy trainer = TorchTrainingRunner ( model = age_classifier , dataset = ( train_dset , valid_dset ), callbacks = [], options = TorchTrainingOptions ( data_loader_options = { \"batch_size\" : 32 }, criterion = loss_fn , optimizer = optim . Adam ( age_classifier . parameters (), lr = 1e-3 ), metrics = { \"pre\" : precision , \"recall\" : recall , \"f1\" : F1 , \"acc\" : Accuracy (), \"ce_loss\" : Loss ( loss_fn ), }, validate_every = 1 , num_epoch = 3 , tqdm_enabled = True , ), ) trainer . run () Get predictions on the test set # Callbacks score_test = CollectLabelsInMemory () # Do the predictions runner = TorchInferenceRunner ( model = age_classifier , dataset = ListDataset ( test_ff . features ), callbacks = [ score_test ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Print the classification report test_ff_labels = [ test_labels [ img_path ] for img_path , _ in test_ff . indices ] print ( classification_report ( test_ff_labels , score_test . labels ))","title":"Train an age detection model based on ArcFace features"},{"location":"examples/train_face_age_detection/#train-an-age-detection-model-based-on-arcface-features","text":"Import MLModule modules from mlmodule.models.arcface.pretrained import torch_arcface_insightface from mlmodule.models.mtcnn.pretrained import torch_mtcnn from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.models.classification import LinearClassifierTorchModule from mlmodule.torch.datasets import TorchTrainingDataset from mlmodule.torch.runners import TorchTrainingRunner from mlmodule.torch.options import TorchTrainingOptions from mlmodule.torch.options import TorchRunnerOptions from mlmodule.labels.base import LabelSet from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , CollectFeaturesInMemory , CollectLabelsInMemory , ) from mlmodule.torch.datasets import ( ImageBoundingBoxDataset , ListDataset , LocalBinaryFilesDataset , ImageDataset , ) from mlmodule.helpers.files import list_files_in_dir from ignite.metrics import Precision , Recall , Loss , Accuracy import torch.nn.functional as F import torch.optim as optim from sklearn.metrics import classification_report import os import re Enable logging inside notebook import logging import sys logging . basicConfig ( format = \" %(asctime)s | %(levelname)s : %(message)s \" , level = logging . INFO , stream = sys . stdout , ) First download UTKFace_inthewild dataset from https://drive.google.com/drive/folders/1HROmgviy4jUUUaCdvvrQ8PcqtNg2jn3G Download and extract the 3 parts and place the folder UTKFace_inthewild in your home directory. part1 will serve as training set, while part2 will be our test set and part3 our valid set. path_to_utkface = os . path . join ( os . environ [ \"HOME\" ], \"UTKFace_inthewild\" ) train_filenames = list_files_in_dir ( os . path . join ( path_to_utkface , \"part1\" ), allowed_extensions = ( \"jpg\" ,) ) test_filenames = list_files_in_dir ( os . path . join ( path_to_utkface , \"part2\" ), allowed_extensions = ( \"jpg\" ,) ) valid_filenames = list_files_in_dir ( os . path . join ( path_to_utkface , \"part3\" ), allowed_extensions = ( \"jpg\" ,) ) train_dataset = ImageDataset ( LocalBinaryFilesDataset ( train_filenames )) test_dataset = ImageDataset ( LocalBinaryFilesDataset ( test_filenames )) valid_dataset = ImageDataset ( LocalBinaryFilesDataset ( valid_filenames )) Extract pretrained ArcFace features for all images torch_device = \"cuda\" face_detector = torch_mtcnn ( device = torch_device ) face_extractor = torch_arcface_insightface ( device = torch_device ) def get_face_features ( dset ): # Callbacks bb = CollectBoundingBoxesInMemory () ff = CollectFeaturesInMemory () # Face detection runner runner = TorchInferenceRunner ( model = face_detector , dataset = dset , callbacks = [ bb ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 1 }, device = torch_device , tqdm_enabled = True , ), ) runner . run () # Dataset with detected faces dataset = ImageBoundingBoxDataset ( image_dataset = ImageDataset ( LocalBinaryFilesDataset ( bb . indices )), bounding_boxes = bb . bounding_boxes , ) # Face extraction runner runner = TorchInferenceRunner ( model = face_extractor , dataset = dataset , callbacks = [ ff ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True , ), ) runner . run () return bb , ff train_bb , train_ff = get_face_features ( train_dataset ) test_bb , test_ff = get_face_features ( test_dataset ) valid_bb , valid_ff = get_face_features ( valid_dataset ) Discretising the age into six categories childhood = { i : \"childhood\" for i in range ( 0 , 8 )} puberty = { i : \"puberty\" for i in range ( 8 , 13 )} adolescence = { i : \"adolescence\" for i in range ( 13 , 18 )} adulthood = { i : \"adulthood\" for i in range ( 18 , 35 )} middle_age = { i : \"middle_age\" for i in range ( 35 , 50 )} seniority = { i : \"seniority\" for i in range ( 50 , 120 )} age2label = { ** childhood , ** puberty , ** adolescence , ** adulthood , ** middle_age , ** seniority , } def discretize_age ( filenames ): labels = {} for img_path in filenames : m = re . search ( \"(\\d+)_.*[.jpg]\" , img_path ) if m : age = int ( m . group ( 1 )) labels [ img_path ] = age2label [ age ] else : print ( f \" { img_path } failed\" ) assert len ( labels ) == len ( filenames ) return labels train_labels = discretize_age ( train_filenames ) test_labels = discretize_age ( test_filenames ) valid_labels = discretize_age ( valid_filenames ) label_set = LabelSet ( label_set_unique_id = \"age\" , label_list = [ \"childhood\" , \"puberty\" , \"adolescence\" , \"adulthood\" , \"middle_age\" , \"seniority\" , ], ) Define train and validation set for training the classifier # define train set train_dset = TorchTrainingDataset ( dataset = ListDataset ( train_ff . features ), targets = label_set . get_label_ids ( [ train_labels [ img_path ] for img_path , _ in train_ff . indices ] ), ) # define valid set valid_dset = TorchTrainingDataset ( dataset = ListDataset ( valid_ff . features ), targets = label_set . get_label_ids ( [ valid_labels [ img_path ] for img_path , _ in valid_ff . indices ] ), ) Define a linear classifier on top of the extracted features age_classifier = LinearClassifierTorchModule ( in_features = train_ff . features . shape [ 1 ], label_set = label_set ) Training runner precision = Precision ( average = False ) recall = Recall ( average = False ) F1 = ( precision * recall * 2 / ( precision + recall )) . mean () loss_fn = F . cross_entropy trainer = TorchTrainingRunner ( model = age_classifier , dataset = ( train_dset , valid_dset ), callbacks = [], options = TorchTrainingOptions ( data_loader_options = { \"batch_size\" : 32 }, criterion = loss_fn , optimizer = optim . Adam ( age_classifier . parameters (), lr = 1e-3 ), metrics = { \"pre\" : precision , \"recall\" : recall , \"f1\" : F1 , \"acc\" : Accuracy (), \"ce_loss\" : Loss ( loss_fn ), }, validate_every = 1 , num_epoch = 3 , tqdm_enabled = True , ), ) trainer . run () Get predictions on the test set # Callbacks score_test = CollectLabelsInMemory () # Do the predictions runner = TorchInferenceRunner ( model = age_classifier , dataset = ListDataset ( test_ff . features ), callbacks = [ score_test ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Print the classification report test_ff_labels = [ test_labels [ img_path ] for img_path , _ in test_ff . indices ] print ( classification_report ( test_ff_labels , score_test . labels ))","title":"Train an age detection model based on ArcFace features"},{"location":"examples/train_face_emotion_recognizer/","text":"Face emotion recognition training This notebooks shows how to train a face emotion recognition model on top of ArcFace face features Import MTCNN and ArcFace modules from mlmodule from mlmodule.models.arcface.pretrained import torch_arcface_insightface from mlmodule.models.mtcnn.pretrained import torch_mtcnn from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , CollectFeaturesInMemory , ) from mlmodule.torch.datasets import ( ImageBoundingBoxDataset , ListDataset , ListDatasetIndexed , ) from torchvision.datasets import FER2013 import os Enable logging inside notebook import logging import sys logging . basicConfig ( format = \" %(asctime)s | %(levelname)s : %(message)s \" , level = logging . INFO , stream = sys . stdout , ) Load a dataset containing images of faces annotated with emotion labels We should first download FER2013 dataset from Kaggle and unzip train.csv and test.csv files. path_to_fer2013 = os . path . join ( os . environ [ \"HOME\" ], \"torchvision-datasets\" ) train_set = FER2013 ( root = path_to_fer2013 , split = \"train\" ) # Training images labels_dict = { 0 : \"Angry\" , 1 : \"Disgust\" , 2 : \"Fear\" , 3 : \"Happy\" , 4 : \"Sad\" , 5 : \"Surprise\" , 6 : \"Neutral\" , } train_samples = [( img . convert ( \"RGB\" ), labels_dict [ label ]) for img , label in train_set ] train_images , train_labels = zip ( * train_samples ) Run face detection on the images with TorchMTCNNModule torch_device = \"cuda\" model = torch_mtcnn ( device = torch_device ) # Callbacks bb = CollectBoundingBoxesInMemory () # Runner runner = TorchInferenceRunner ( model = model , dataset = ListDataset ( train_images ), callbacks = [ bb ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Extract face features with TorchArcFaceModule arcface = torch_arcface_insightface ( device = torch_device ) # Dataset dataset = ImageBoundingBoxDataset ( image_dataset = ListDatasetIndexed ( indices = bb . indices , objects = train_images ), bounding_boxes = bb . bounding_boxes , ) # Callbacks ff = CollectFeaturesInMemory () # Runner runner = TorchInferenceRunner ( model = arcface , dataset = dataset , callbacks = [ ff ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Training of a linear classifier on top of the face features Import the module for training from mlmodule.models.classification import LinearClassifierTorchModule from mlmodule.torch.datasets import TorchTrainingDataset from mlmodule.torch.runners import TorchTrainingRunner from mlmodule.torch.options import TorchTrainingOptions from mlmodule.labels.base import LabelSet import torch import torch.nn.functional as F import torch.optim as optim Define the training dataset Define the labels labels = list ( labels_dict . values ()) label_set = LabelSet ( label_set_unique_id = \"emotion\" , label_list = labels ) # split samples into train and valid sets train_indices , valid_indices = torch . split ( torch . randperm ( len ( ff . indices )), int ( len ( ff . indices ) * 0.9 ) ) # define training set train_dset = TorchTrainingDataset ( dataset = ListDatasetIndexed ( train_indices , ff . features [ train_indices ]), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in train_indices ]), ) # define valid set valid_dset = TorchTrainingDataset ( dataset = ListDatasetIndexed ( valid_indices , ff . features [ valid_indices ]), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in valid_indices ]), ) Define the linear classifier in_features = len ( ff . features [ 0 ]) classifier = LinearClassifierTorchModule ( in_features = in_features , label_set = label_set ) Define the trainer from ignite.metrics import Precision , Recall , Loss , Accuracy precision = Precision ( average = False ) recall = Recall ( average = False ) F1 = ( precision * recall * 2 / ( precision + recall )) . mean () loss_fn = F . cross_entropy trainer = TorchTrainingRunner ( model = classifier , dataset = ( train_dset , valid_dset ), callbacks = [], options = TorchTrainingOptions ( data_loader_options = { \"batch_size\" : 32 }, criterion = loss_fn , optimizer = optim . Adam ( classifier . parameters (), lr = 1e-2 ), metrics = { \"pre\" : precision , \"recall\" : recall , \"f1\" : F1 , \"acc\" : Accuracy (), \"ce_loss\" : Loss ( loss_fn ), }, validate_every = 1 , num_epoch = 5 , tqdm_enabled = True , ), ) trainer . run ()","title":"Face emotion recognition training"},{"location":"examples/train_face_emotion_recognizer/#face-emotion-recognition-training","text":"This notebooks shows how to train a face emotion recognition model on top of ArcFace face features Import MTCNN and ArcFace modules from mlmodule from mlmodule.models.arcface.pretrained import torch_arcface_insightface from mlmodule.models.mtcnn.pretrained import torch_mtcnn from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , CollectFeaturesInMemory , ) from mlmodule.torch.datasets import ( ImageBoundingBoxDataset , ListDataset , ListDatasetIndexed , ) from torchvision.datasets import FER2013 import os Enable logging inside notebook import logging import sys logging . basicConfig ( format = \" %(asctime)s | %(levelname)s : %(message)s \" , level = logging . INFO , stream = sys . stdout , ) Load a dataset containing images of faces annotated with emotion labels We should first download FER2013 dataset from Kaggle and unzip train.csv and test.csv files. path_to_fer2013 = os . path . join ( os . environ [ \"HOME\" ], \"torchvision-datasets\" ) train_set = FER2013 ( root = path_to_fer2013 , split = \"train\" ) # Training images labels_dict = { 0 : \"Angry\" , 1 : \"Disgust\" , 2 : \"Fear\" , 3 : \"Happy\" , 4 : \"Sad\" , 5 : \"Surprise\" , 6 : \"Neutral\" , } train_samples = [( img . convert ( \"RGB\" ), labels_dict [ label ]) for img , label in train_set ] train_images , train_labels = zip ( * train_samples ) Run face detection on the images with TorchMTCNNModule torch_device = \"cuda\" model = torch_mtcnn ( device = torch_device ) # Callbacks bb = CollectBoundingBoxesInMemory () # Runner runner = TorchInferenceRunner ( model = model , dataset = ListDataset ( train_images ), callbacks = [ bb ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Extract face features with TorchArcFaceModule arcface = torch_arcface_insightface ( device = torch_device ) # Dataset dataset = ImageBoundingBoxDataset ( image_dataset = ListDatasetIndexed ( indices = bb . indices , objects = train_images ), bounding_boxes = bb . bounding_boxes , ) # Callbacks ff = CollectFeaturesInMemory () # Runner runner = TorchInferenceRunner ( model = arcface , dataset = dataset , callbacks = [ ff ], options = TorchRunnerOptions ( data_loader_options = { \"batch_size\" : 32 }, device = torch_device , tqdm_enabled = True ), ) runner . run () Training of a linear classifier on top of the face features Import the module for training from mlmodule.models.classification import LinearClassifierTorchModule from mlmodule.torch.datasets import TorchTrainingDataset from mlmodule.torch.runners import TorchTrainingRunner from mlmodule.torch.options import TorchTrainingOptions from mlmodule.labels.base import LabelSet import torch import torch.nn.functional as F import torch.optim as optim Define the training dataset Define the labels labels = list ( labels_dict . values ()) label_set = LabelSet ( label_set_unique_id = \"emotion\" , label_list = labels ) # split samples into train and valid sets train_indices , valid_indices = torch . split ( torch . randperm ( len ( ff . indices )), int ( len ( ff . indices ) * 0.9 ) ) # define training set train_dset = TorchTrainingDataset ( dataset = ListDatasetIndexed ( train_indices , ff . features [ train_indices ]), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in train_indices ]), ) # define valid set valid_dset = TorchTrainingDataset ( dataset = ListDatasetIndexed ( valid_indices , ff . features [ valid_indices ]), targets = label_set . get_label_ids ([ train_labels [ idx ] for idx in valid_indices ]), ) Define the linear classifier in_features = len ( ff . features [ 0 ]) classifier = LinearClassifierTorchModule ( in_features = in_features , label_set = label_set ) Define the trainer from ignite.metrics import Precision , Recall , Loss , Accuracy precision = Precision ( average = False ) recall = Recall ( average = False ) F1 = ( precision * recall * 2 / ( precision + recall )) . mean () loss_fn = F . cross_entropy trainer = TorchTrainingRunner ( model = classifier , dataset = ( train_dset , valid_dset ), callbacks = [], options = TorchTrainingOptions ( data_loader_options = { \"batch_size\" : 32 }, criterion = loss_fn , optimizer = optim . Adam ( classifier . parameters (), lr = 1e-2 ), metrics = { \"pre\" : precision , \"recall\" : recall , \"f1\" : F1 , \"acc\" : Accuracy (), \"ce_loss\" : Loss ( loss_fn ), }, validate_every = 1 , num_epoch = 5 , tqdm_enabled = True , ), ) trainer . run ()","title":"Face emotion recognition training"},{"location":"examples/vinvl/","text":"Object Detection with VinVL Import mlmodule modules from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , ) from mlmodule.helpers.files import list_files_in_dir from mlmodule.torch.datasets import LocalBinaryFilesDataset , ImageDataset from mlmodule.models.vinvl.pretrained import torch_vinvl_detector import torch import matplotlib.pyplot as plt from matplotlib.patches import Rectangle from PIL import Image import os % matplotlib inline Load images base_path = os . path . join ( \"../../tests\" , \"fixtures\" , \"objects\" ) file_names = list_files_in_dir ( base_path , allowed_extensions = ( \"jpg\" ,))[: 50 ] dataset = ImageDataset ( LocalBinaryFilesDataset ( file_names )) Run object detection with torch_vinvl_detector # Load VinVL model (it might take a few minutes.) torch_device = torch . device ( \"cuda\" ) vinvl = torch_vinvl_detector ( device = torch_device , score_threshold = 0.5 ) bb = CollectBoundingBoxesInMemory () # Runner runner = TorchInferenceRunner ( model = vinvl , dataset = dataset , callbacks = [ bb ], options = TorchRunnerOptions ( device = torch_device , data_loader_options = { \"batch_size\" : 10 }, tqdm_enabled = True ), ) runner . run () Visualise the detected objects First get labels and attributes for i , img_path in enumerate ( bb . indices ): print ( f \"Object detected for { img_path } \" ) img = Image . open ( img_path ) . convert ( \"RGB\" ) plt . figure () plt . imshow ( img ) bboxes = bb . bounding_boxes [ i ] . bounding_boxes scores = bb . bounding_boxes [ i ] . scores for k , bbox in enumerate ( bboxes ): bbox0 , bbox1 , bbox2 , bbox3 = bbox plt . gca () . add_patch ( Rectangle ( ( bbox0 , bbox1 ), bbox2 - bbox0 , bbox3 - bbox1 , fill = False , edgecolor = \"red\" , linewidth = 2 , alpha = 0.5 , ) ) plt . text ( bbox0 , bbox1 , f \" { scores [ k ] * 100 : .1f } %\" , color = \"blue\" , fontsize = 12 )","title":"Object Detection with VinVL"},{"location":"examples/vinvl/#object-detection-with-vinvl","text":"Import mlmodule modules from mlmodule.torch.options import TorchRunnerOptions from mlmodule.torch.runners import TorchInferenceRunner from mlmodule.callbacks.memory import ( CollectBoundingBoxesInMemory , ) from mlmodule.helpers.files import list_files_in_dir from mlmodule.torch.datasets import LocalBinaryFilesDataset , ImageDataset from mlmodule.models.vinvl.pretrained import torch_vinvl_detector import torch import matplotlib.pyplot as plt from matplotlib.patches import Rectangle from PIL import Image import os % matplotlib inline Load images base_path = os . path . join ( \"../../tests\" , \"fixtures\" , \"objects\" ) file_names = list_files_in_dir ( base_path , allowed_extensions = ( \"jpg\" ,))[: 50 ] dataset = ImageDataset ( LocalBinaryFilesDataset ( file_names )) Run object detection with torch_vinvl_detector # Load VinVL model (it might take a few minutes.) torch_device = torch . device ( \"cuda\" ) vinvl = torch_vinvl_detector ( device = torch_device , score_threshold = 0.5 ) bb = CollectBoundingBoxesInMemory () # Runner runner = TorchInferenceRunner ( model = vinvl , dataset = dataset , callbacks = [ bb ], options = TorchRunnerOptions ( device = torch_device , data_loader_options = { \"batch_size\" : 10 }, tqdm_enabled = True ), ) runner . run () Visualise the detected objects First get labels and attributes for i , img_path in enumerate ( bb . indices ): print ( f \"Object detected for { img_path } \" ) img = Image . open ( img_path ) . convert ( \"RGB\" ) plt . figure () plt . imshow ( img ) bboxes = bb . bounding_boxes [ i ] . bounding_boxes scores = bb . bounding_boxes [ i ] . scores for k , bbox in enumerate ( bboxes ): bbox0 , bbox1 , bbox2 , bbox3 = bbox plt . gca () . add_patch ( Rectangle ( ( bbox0 , bbox1 ), bbox2 - bbox0 , bbox3 - bbox1 , fill = False , edgecolor = \"red\" , linewidth = 2 , alpha = 0.5 , ) ) plt . text ( bbox0 , bbox1 , f \" { scores [ k ] * 100 : .1f } %\" , color = \"blue\" , fontsize = 12 )","title":"Object Detection with VinVL"},{"location":"models/","text":"Models Pre-trained models Reference Name License torch_arcface_insightface ArcFace pre-trained by InsightFace InsightFace torch_clip_image_encoder CLIP pre-trained image encoder CLIP torch_clip_text_encoder CLIP pre-trained text encoder CLIP torch_densenet_imagenet PyTorch DenseNet pre-trained on ImageNet TorchVision torch_densenet_places365 PyTorch DenseNet pre-trained on Places365 Places365 torch_keyframes_resnet_imagenet Keyframes with ResNet pre-trained on ImageNet TorchVision torch_keyframes_densenet_imagenet Keyframes with DenseNet pre-trained on ImageNet TorchVision torch_keyframes_densenet_places365 Keyframes with DenseNet pre-trained on Places365 Places365 torch_magface Pre-trained MagFace MagFace torch_mtcnn Pre-trained face detection MTCNN from FaceNet FaceNet torch_resnet_imagenet PyTorch ResNet pre-trained on ImageNet TorchVision torch_distiluse_base_multilingual_v2 SBERT's pre-trained DistilUse Multilingual Cased V2 SBERT torch_vinvl_detector VinVL's pre-trained object detector Microsoft scene graph benchmark Add a new model In MLModule, a model is usually implemented as a class. The model implementation details primarily depend on the type of runner used. For instance, the TorchInferenceRunner expects to receive a subclass of TorchMlModule . However, there are a few conventions to follow: Model predictions should implement the BatchModelPrediction class, this is required for callbacks to work properly. If the model's state needs to be saved, the model should follow the ModelWithState protocol. If the model predicts labels, it should follow the ModelWithLabels protocol. Predictions Note The ArrayLike type is expected to be a np.ndarray or a torch.Tensor . mlmodule.predictions.BatchModelPrediction dataclass Class defining the accepted types for model's predictions Attributes: Name Type Description features ArrayLike | None Features or embeddings. Dimensions= (dataset_size, feature_dims...) label_scores ArrayLike | None Score for each predicted label. Dimensions= (dataset_size, n_labels) frames Sequence[BatchVideoFramesPrediction[ArrayLike]] | None Prediction for each frame and video. See BatchVideoFramesPrediction . Sequence length= dataset_size bounding_boxes Sequence[BatchBoundingBoxesPrediction[ArrayLike]] | None Prediction for each bounding_box and image. See BatchBoundingBoxesPrediction . Sequence length= dataset_size mlmodule.predictions.BatchBoundingBoxesPrediction dataclass Results of bounding box extraction on an images Attributes: Name Type Description bounding_boxes ArrayLike Array of bounding box coordinates. Dimensions= (n_boxes, 4) . scores ArrayLike | None Array of scores for each bounding boxes. Dimensions= (n_boxes, 1) . features ArrayLike | None Array of features for each bounding boxes. Dimensions= (n_boxes, features_dims...) . mlmodule.predictions.BatchVideoFramesPrediction dataclass Result of video frames extraction on a video Attributes: Name Type Description frame_indices Sequence[int] The frame index in the source video features ArrayLike | None The features of the video frames. Dimensions= (n_frames, features_dims...) State management A model with internal state (weights) should implement the ModelWithState protocol to be compatible with state stores . mlmodule.models.types.ModelWithState Protocol of a model with internal state (weights) It defines two functions set_state and get_state . Attributes: Name Type Description state_type StateType Type of the model state, see states for more information. state_type : StateType property readonly Type of the model state See states for more information. get_state ( self ) -> bytes Get the model internal state Returns: Type Description bytes Serialised state as bytes set_state ( self , state : bytes ) -> None Set the model internal state Parameters: Name Type Description Default state bytes Serialised state as bytes required Labels When a model returns label scores, it must define a LabelSet . This should be defined by implementing the ModelWithLabels protocol. mlmodule.models.types.ModelWithLabels Model that predicts scores for labels It defines the get_labels function get_labels ( self ) -> LabelSet Getter for the model's LabelSet Returns: Type Description LabelSet The label set corresponding to returned label scores PyTorch models PyTorch models should be a subclass of TorchMlModule . Note PyTorch models already implement the ModelWithState protocol by default. mlmodule.torch.modules.TorchMlModule Base torch.nn.Module for PyTorch models implemented in MLModule. A valid subclass of TorchMlModule must implement the following method: forward to_predictions state_type And can optionally implement: get_dataset_transforms get_dataloader_collate_fn Attributes: Name Type Description device torch.device Mandatory PyTorch device attribute to initialise model. is_trainable bool Flag which indicates if the model is trainable. Default, True. Examples: This would define a simple PyTorch model consisting of fully connected layer. from mlmodule.states import StateType from mlmodule.torch.modules import TorchMlModule from torchvision import transforms class FC ( TorchMlModule [ torch . Tensor , torch . Tensor ]): def __init__ ( self , device : torch . device = torch . device ( \"cpu\" )): super () . __init__ ( device = device ) self . fc = nn . Linear ( 512 , 512 ) def forward ( self , batch : torch . Tensor ) -> torch . Tensor : return self . fc ( batch ) def to_predictions ( self , forward_output : torch . Tensor ) -> BatchModelPrediction [ torch . Tensor ]: return BatchModelPrediction ( features = forward_output ) @property def state_type ( self ) -> StateType : return StateType ( backend = \"pytorch\" , architecture = \"fc512x512\" , ) def get_dataset_transforms ( self ) -> List [ Callable ]: return [ transforms . ToTensor ()] Note This is a generic class taking a _BatchType and _ForwardOutputType type argument. This corresponds respectively to the type of data the forward will take as argument and return. It is most likely torch.Tensor Note By default, MLModule models are trainable. Set the is_trainable parameter to False when creating a subclass if it shouldn't be trained. state_type : StateType property readonly Identifier for the current's model state architecture Important This property must be implemented in subclasses Note PyTorch's model architecture should have the pytorch backend Returns: Type Description StateType State architecture object forward ( self , batch : ~ _BatchType ) -> ~ _ForwardOutputType Forward pass of the model Important This method must be implemented in subclasses Applies the module on a batch and returns all potentially interesting data point (features, labels...) Parameters: Name Type Description Default batch _BatchType the batch of data to process required Returns: Type Description _ForwardOutputType A tensor or a sequence of tensor with relevant information (features, labels, bounding boxes...) Note This method must be implemented in subclasses get_dataloader_collate_fn ( self ) -> Optional [ Callable [[ Any ], Any ]] Optionally returns a collate function to be passed to the data loader Note This collate function will be wrapped in mlmodule.torch.collate.TorchMlModuleCollateFn . This means that the first argument batch will not contain the indices of the dataset but only the data element. Returns: Type Description Callable[[Any], Any] | None The collate function to be passed to TorchMlModuleCollateFn . get_dataset_transforms ( self ) -> List [ Callable ] Transforms to apply to the input dataset . Note By default, this method returns an empty list (meaning no transformation) but in most cases, this will need to be overridden. Returns: Type Description List[Callable] A list of callables that will be used to transform the input data. to_predictions ( self , forward_output : ~ _ForwardOutputType ) -> mlmodule . predictions . BatchModelPrediction [ torch . Tensor ] Modifies the output of the forward pass to create the standard BatchModelPrediction object Important This method must be implemented in subclasses Parameters: Name Type Description Default forward_output _ForwardOutputType the batch of data to process required Returns: Type Description BatchModelPrediction[torch.Tensor] Prediction object with the keys features , label_scores ...","title":"Models"},{"location":"models/#models","text":"","title":"Models"},{"location":"models/#pre-trained-models","text":"Reference Name License torch_arcface_insightface ArcFace pre-trained by InsightFace InsightFace torch_clip_image_encoder CLIP pre-trained image encoder CLIP torch_clip_text_encoder CLIP pre-trained text encoder CLIP torch_densenet_imagenet PyTorch DenseNet pre-trained on ImageNet TorchVision torch_densenet_places365 PyTorch DenseNet pre-trained on Places365 Places365 torch_keyframes_resnet_imagenet Keyframes with ResNet pre-trained on ImageNet TorchVision torch_keyframes_densenet_imagenet Keyframes with DenseNet pre-trained on ImageNet TorchVision torch_keyframes_densenet_places365 Keyframes with DenseNet pre-trained on Places365 Places365 torch_magface Pre-trained MagFace MagFace torch_mtcnn Pre-trained face detection MTCNN from FaceNet FaceNet torch_resnet_imagenet PyTorch ResNet pre-trained on ImageNet TorchVision torch_distiluse_base_multilingual_v2 SBERT's pre-trained DistilUse Multilingual Cased V2 SBERT torch_vinvl_detector VinVL's pre-trained object detector Microsoft scene graph benchmark","title":"Pre-trained models"},{"location":"models/#add-a-new-model","text":"In MLModule, a model is usually implemented as a class. The model implementation details primarily depend on the type of runner used. For instance, the TorchInferenceRunner expects to receive a subclass of TorchMlModule . However, there are a few conventions to follow: Model predictions should implement the BatchModelPrediction class, this is required for callbacks to work properly. If the model's state needs to be saved, the model should follow the ModelWithState protocol. If the model predicts labels, it should follow the ModelWithLabels protocol.","title":"Add a new model"},{"location":"models/#predictions","text":"Note The ArrayLike type is expected to be a np.ndarray or a torch.Tensor .","title":"Predictions"},{"location":"models/#mlmodule.predictions.BatchModelPrediction","text":"Class defining the accepted types for model's predictions Attributes: Name Type Description features ArrayLike | None Features or embeddings. Dimensions= (dataset_size, feature_dims...) label_scores ArrayLike | None Score for each predicted label. Dimensions= (dataset_size, n_labels) frames Sequence[BatchVideoFramesPrediction[ArrayLike]] | None Prediction for each frame and video. See BatchVideoFramesPrediction . Sequence length= dataset_size bounding_boxes Sequence[BatchBoundingBoxesPrediction[ArrayLike]] | None Prediction for each bounding_box and image. See BatchBoundingBoxesPrediction . Sequence length= dataset_size","title":"BatchModelPrediction"},{"location":"models/#mlmodule.predictions.BatchBoundingBoxesPrediction","text":"Results of bounding box extraction on an images Attributes: Name Type Description bounding_boxes ArrayLike Array of bounding box coordinates. Dimensions= (n_boxes, 4) . scores ArrayLike | None Array of scores for each bounding boxes. Dimensions= (n_boxes, 1) . features ArrayLike | None Array of features for each bounding boxes. Dimensions= (n_boxes, features_dims...) .","title":"BatchBoundingBoxesPrediction"},{"location":"models/#mlmodule.predictions.BatchVideoFramesPrediction","text":"Result of video frames extraction on a video Attributes: Name Type Description frame_indices Sequence[int] The frame index in the source video features ArrayLike | None The features of the video frames. Dimensions= (n_frames, features_dims...)","title":"BatchVideoFramesPrediction"},{"location":"models/#state-management","text":"A model with internal state (weights) should implement the ModelWithState protocol to be compatible with state stores .","title":"State management"},{"location":"models/#mlmodule.models.types.ModelWithState","text":"Protocol of a model with internal state (weights) It defines two functions set_state and get_state . Attributes: Name Type Description state_type StateType Type of the model state, see states for more information.","title":"ModelWithState"},{"location":"models/#mlmodule.models.types.ModelWithState.state_type","text":"Type of the model state See states for more information.","title":"state_type"},{"location":"models/#mlmodule.models.types.ModelWithState.get_state","text":"Get the model internal state Returns: Type Description bytes Serialised state as bytes","title":"get_state()"},{"location":"models/#mlmodule.models.types.ModelWithState.set_state","text":"Set the model internal state Parameters: Name Type Description Default state bytes Serialised state as bytes required","title":"set_state()"},{"location":"models/#labels","text":"When a model returns label scores, it must define a LabelSet . This should be defined by implementing the ModelWithLabels protocol.","title":"Labels"},{"location":"models/#mlmodule.models.types.ModelWithLabels","text":"Model that predicts scores for labels It defines the get_labels function","title":"ModelWithLabels"},{"location":"models/#mlmodule.models.types.ModelWithLabels.get_labels","text":"Getter for the model's LabelSet Returns: Type Description LabelSet The label set corresponding to returned label scores","title":"get_labels()"},{"location":"models/#pytorch-models","text":"PyTorch models should be a subclass of TorchMlModule . Note PyTorch models already implement the ModelWithState protocol by default.","title":"PyTorch models"},{"location":"models/#mlmodule.torch.modules.TorchMlModule","text":"Base torch.nn.Module for PyTorch models implemented in MLModule. A valid subclass of TorchMlModule must implement the following method: forward to_predictions state_type And can optionally implement: get_dataset_transforms get_dataloader_collate_fn Attributes: Name Type Description device torch.device Mandatory PyTorch device attribute to initialise model. is_trainable bool Flag which indicates if the model is trainable. Default, True. Examples: This would define a simple PyTorch model consisting of fully connected layer. from mlmodule.states import StateType from mlmodule.torch.modules import TorchMlModule from torchvision import transforms class FC ( TorchMlModule [ torch . Tensor , torch . Tensor ]): def __init__ ( self , device : torch . device = torch . device ( \"cpu\" )): super () . __init__ ( device = device ) self . fc = nn . Linear ( 512 , 512 ) def forward ( self , batch : torch . Tensor ) -> torch . Tensor : return self . fc ( batch ) def to_predictions ( self , forward_output : torch . Tensor ) -> BatchModelPrediction [ torch . Tensor ]: return BatchModelPrediction ( features = forward_output ) @property def state_type ( self ) -> StateType : return StateType ( backend = \"pytorch\" , architecture = \"fc512x512\" , ) def get_dataset_transforms ( self ) -> List [ Callable ]: return [ transforms . ToTensor ()] Note This is a generic class taking a _BatchType and _ForwardOutputType type argument. This corresponds respectively to the type of data the forward will take as argument and return. It is most likely torch.Tensor Note By default, MLModule models are trainable. Set the is_trainable parameter to False when creating a subclass if it shouldn't be trained.","title":"TorchMlModule"},{"location":"models/#mlmodule.torch.modules.TorchMlModule.state_type","text":"Identifier for the current's model state architecture Important This property must be implemented in subclasses Note PyTorch's model architecture should have the pytorch backend Returns: Type Description StateType State architecture object","title":"state_type"},{"location":"models/#mlmodule.torch.modules.TorchMlModule.forward","text":"Forward pass of the model Important This method must be implemented in subclasses Applies the module on a batch and returns all potentially interesting data point (features, labels...) Parameters: Name Type Description Default batch _BatchType the batch of data to process required Returns: Type Description _ForwardOutputType A tensor or a sequence of tensor with relevant information (features, labels, bounding boxes...) Note This method must be implemented in subclasses","title":"forward()"},{"location":"models/#mlmodule.torch.modules.TorchMlModule.get_dataloader_collate_fn","text":"Optionally returns a collate function to be passed to the data loader Note This collate function will be wrapped in mlmodule.torch.collate.TorchMlModuleCollateFn . This means that the first argument batch will not contain the indices of the dataset but only the data element. Returns: Type Description Callable[[Any], Any] | None The collate function to be passed to TorchMlModuleCollateFn .","title":"get_dataloader_collate_fn()"},{"location":"models/#mlmodule.torch.modules.TorchMlModule.get_dataset_transforms","text":"Transforms to apply to the input dataset . Note By default, this method returns an empty list (meaning no transformation) but in most cases, this will need to be overridden. Returns: Type Description List[Callable] A list of callables that will be used to transform the input data.","title":"get_dataset_transforms()"},{"location":"models/#mlmodule.torch.modules.TorchMlModule.to_predictions","text":"Modifies the output of the forward pass to create the standard BatchModelPrediction object Important This method must be implemented in subclasses Parameters: Name Type Description Default forward_output _ForwardOutputType the batch of data to process required Returns: Type Description BatchModelPrediction[torch.Tensor] Prediction object with the keys features , label_scores ...","title":"to_predictions()"},{"location":"models/ArcFace/","text":"ArcFace Implementation of ArcFace 1 in PyTorch by InsightFace . Pre-trained models mlmodule . models . arcface . pretrained . torch_arcface_insightface ArcFace model pre-trained by InsightFace Parameters: Name Type Description Default device torch.device Torch device to initialise the model weights required remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . required bad_faces_threshold float The cosine similarity distance to reference faces for which we consider the face is of bad quality. required Returns: Type Description TorchArcFaceModule Pre-trained ArcFace Base models The MagFace model is an implementation of a TorchMlModule . mlmodule.models.arcface.modules.TorchArcFaceModule Creates face embeddings from MTCNN output Attributes: Name Type Description device torch.device Torch device to initialise the model weights remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . bad_faces_threshold float The cosine similarity distance to reference faces for which we consider the face is of bad quality. Provider store See the stores documentation for usage. mlmodule.models.arcface.stores.ArcFaceStore Gets the pretrained state dir from OneDrive URL: https://github.com/TreB1eN/InsightFace_Pytorch#2-pretrained-models--performance Model: IR-SE50 Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: additive angular margin loss for deep face recognition. In CVPR . 2019. \u21a9","title":"ArcFace"},{"location":"models/ArcFace/#arcface","text":"Implementation of ArcFace 1 in PyTorch by InsightFace .","title":"ArcFace"},{"location":"models/ArcFace/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/ArcFace/#mlmodule.models.arcface.pretrained.torch_arcface_insightface","text":"ArcFace model pre-trained by InsightFace Parameters: Name Type Description Default device torch.device Torch device to initialise the model weights required remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . required bad_faces_threshold float The cosine similarity distance to reference faces for which we consider the face is of bad quality. required Returns: Type Description TorchArcFaceModule Pre-trained ArcFace","title":"torch_arcface_insightface()"},{"location":"models/ArcFace/#base-models","text":"The MagFace model is an implementation of a TorchMlModule .","title":"Base models"},{"location":"models/ArcFace/#mlmodule.models.arcface.modules.TorchArcFaceModule","text":"Creates face embeddings from MTCNN output Attributes: Name Type Description device torch.device Torch device to initialise the model weights remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . bad_faces_threshold float The cosine similarity distance to reference faces for which we consider the face is of bad quality.","title":"TorchArcFaceModule"},{"location":"models/ArcFace/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/ArcFace/#mlmodule.models.arcface.stores.ArcFaceStore","text":"Gets the pretrained state dir from OneDrive URL: https://github.com/TreB1eN/InsightFace_Pytorch#2-pretrained-models--performance Model: IR-SE50 Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: additive angular margin loss for deep face recognition. In CVPR . 2019. \u21a9","title":"ArcFaceStore"},{"location":"models/CLIP/","text":"CLIP CLIP 1 model encodes text snippets and images into a common embedding space which enables zero-shot retrieval and prediction. See OpenAI/CLIP for the source code and original models. Pre-trained models mlmodule . models . clip . pretrained . torch_clip_image_encoder Pre-trained CLIP image encoder Parameters: Name Type Description Default clip_model_name str Name of the model to load (see CLIP doc ) required device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . required mlmodule . models . clip . pretrained . torch_clip_text_encoder Pre-trained CLIP text encoder Parameters: Name Type Description Default clip_model_name str Name of the model to load (see CLIP doc ) required device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . required Models CLIP comes with image ( CLIPImageModule ) and a text ( CLIPTextModule ) encoders. These modules are an implementation of TorchMlModule . mlmodule.models.clip.image.CLIPImageModule Image encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . mlmodule.models.clip.text.CLIPTextModule Text encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . Pre-trained states from CLIP See the stores documentation for usage. mlmodule.models.clip.stores.CLIPStore Pre-trained model states by OpenAI CLIP These are identified by training_id=clip . List models and parameters There is a command line utility to list all available models from CLIP with their associated parameters in JSON format: python -m mlmodule.models.clip.list The output is used to fill the mlmodule.models.clip.parameters file. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research, 8748\u20138763. PMLR, 18\u201324 Jul 2021. URL: https://proceedings.mlr.press/v139/radford21a.html . \u21a9","title":"CLIP"},{"location":"models/CLIP/#clip","text":"CLIP 1 model encodes text snippets and images into a common embedding space which enables zero-shot retrieval and prediction. See OpenAI/CLIP for the source code and original models.","title":"CLIP"},{"location":"models/CLIP/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/CLIP/#mlmodule.models.clip.pretrained.torch_clip_image_encoder","text":"Pre-trained CLIP image encoder Parameters: Name Type Description Default clip_model_name str Name of the model to load (see CLIP doc ) required device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . required","title":"torch_clip_image_encoder()"},{"location":"models/CLIP/#mlmodule.models.clip.pretrained.torch_clip_text_encoder","text":"Pre-trained CLIP text encoder Parameters: Name Type Description Default clip_model_name str Name of the model to load (see CLIP doc ) required device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . required","title":"torch_clip_text_encoder()"},{"location":"models/CLIP/#models","text":"CLIP comes with image ( CLIPImageModule ) and a text ( CLIPTextModule ) encoders. These modules are an implementation of TorchMlModule .","title":"Models"},{"location":"models/CLIP/#mlmodule.models.clip.image.CLIPImageModule","text":"Image encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"CLIPImageModule"},{"location":"models/CLIP/#mlmodule.models.clip.text.CLIPTextModule","text":"Text encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"CLIPTextModule"},{"location":"models/CLIP/#pre-trained-states-from-clip","text":"See the stores documentation for usage.","title":"Pre-trained states from CLIP"},{"location":"models/CLIP/#mlmodule.models.clip.stores.CLIPStore","text":"Pre-trained model states by OpenAI CLIP These are identified by training_id=clip .","title":"CLIPStore"},{"location":"models/CLIP/#list-models-and-parameters","text":"There is a command line utility to list all available models from CLIP with their associated parameters in JSON format: python -m mlmodule.models.clip.list The output is used to fill the mlmodule.models.clip.parameters file. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research, 8748\u20138763. PMLR, 18\u201324 Jul 2021. URL: https://proceedings.mlr.press/v139/radford21a.html . \u21a9","title":"List models and parameters"},{"location":"models/Classification/","text":"Classification Path: mlmodule.models.classification This module contains torch modules that can be used as classification heads on top of features. Models mlmodule.models.classification.modules.LinearClassifierTorchModule Linear classifier Attributes: Name Type Description in_features int Number of dimensions in the input label_set LabelSet The set of labels for this classifier mlmodule.models.classification.modules.MLPClassifierTorchModule Multi-layer perceptron classifier Attributes: Name Type Description in_features int The number of dimensions in the input hidden_layers Sequence[int] A sequence of width for the hidden layers label_set LabelSet The set of labels for this classifier activation str | None Name of an activation function in the torch.nn module. For instance ReLU . If None , no activation function is used.","title":"Classification"},{"location":"models/Classification/#classification","text":"Path: mlmodule.models.classification This module contains torch modules that can be used as classification heads on top of features.","title":"Classification"},{"location":"models/Classification/#models","text":"","title":"Models"},{"location":"models/Classification/#mlmodule.models.classification.modules.LinearClassifierTorchModule","text":"Linear classifier Attributes: Name Type Description in_features int Number of dimensions in the input label_set LabelSet The set of labels for this classifier","title":"LinearClassifierTorchModule"},{"location":"models/Classification/#mlmodule.models.classification.modules.MLPClassifierTorchModule","text":"Multi-layer perceptron classifier Attributes: Name Type Description in_features int The number of dimensions in the input hidden_layers Sequence[int] A sequence of width for the hidden layers label_set LabelSet The set of labels for this classifier activation str | None Name of an activation function in the torch.nn module. For instance ReLU . If None , no activation function is used.","title":"MLPClassifierTorchModule"},{"location":"models/DenseNet/","text":"DenseNet PyTorch implementation of DenseNet architecture 1 as defined in Torchvision . Pre-trained models mlmodule . models . densenet . pretrained . torch_densenet_imagenet PyTorch DenseNet model pretrained on ImageNet Parameters: Name Type Description Default densenet_arch DenseNetArch Identifier for the DenseNet architecture. Must be one of: - densenet121 - densenet161 - densenet169 - densenet201 required device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description TorchDenseNetModule PyTorch DenseNet model pretrained on ImageNet mlmodule . models . densenet . pretrained . torch_densenet_places365 PyTorch DenseNet model pretrained on Places365. See places365 documentation for more info. Parameters: Name Type Description Default device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description TorchDenseNetModule PyTorch DenseNet model pretrained on Places365 Base model The DenseNet model is an implementation of a TorchMlModule . mlmodule.models.densenet.modules.TorchDenseNetModule PyTorch implementation of DenseNet See TorchVision source code . Attributes: Name Type Description densenet_arch DenseNetArch Identifier for the DenseNet architecture. Must be one of: - densenet121 - densenet161 - densenet169 - densenet201 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights Pre-trained state origins See the stores documentation for usage. mlmodule.models.densenet.stores.DenseNetTorchVisionStore Model store to load DenseNet weights pretrained on ImageNet from TorchVision mlmodule.models.densenet.stores.DenseNetPlaces365Store Model store to load DenseNet weights pretrained on Places365 See places365 documentation for more info. Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . July 2017. \u21a9","title":"DenseNet"},{"location":"models/DenseNet/#densenet","text":"PyTorch implementation of DenseNet architecture 1 as defined in Torchvision .","title":"DenseNet"},{"location":"models/DenseNet/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/DenseNet/#mlmodule.models.densenet.pretrained.torch_densenet_imagenet","text":"PyTorch DenseNet model pretrained on ImageNet Parameters: Name Type Description Default densenet_arch DenseNetArch Identifier for the DenseNet architecture. Must be one of: - densenet121 - densenet161 - densenet169 - densenet201 required device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description TorchDenseNetModule PyTorch DenseNet model pretrained on ImageNet","title":"torch_densenet_imagenet()"},{"location":"models/DenseNet/#mlmodule.models.densenet.pretrained.torch_densenet_places365","text":"PyTorch DenseNet model pretrained on Places365. See places365 documentation for more info. Parameters: Name Type Description Default device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description TorchDenseNetModule PyTorch DenseNet model pretrained on Places365","title":"torch_densenet_places365()"},{"location":"models/DenseNet/#base-model","text":"The DenseNet model is an implementation of a TorchMlModule .","title":"Base model"},{"location":"models/DenseNet/#mlmodule.models.densenet.modules.TorchDenseNetModule","text":"PyTorch implementation of DenseNet See TorchVision source code . Attributes: Name Type Description densenet_arch DenseNetArch Identifier for the DenseNet architecture. Must be one of: - densenet121 - densenet161 - densenet169 - densenet201 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights","title":"TorchDenseNetModule"},{"location":"models/DenseNet/#pre-trained-state-origins","text":"See the stores documentation for usage.","title":"Pre-trained state origins"},{"location":"models/DenseNet/#mlmodule.models.densenet.stores.DenseNetTorchVisionStore","text":"Model store to load DenseNet weights pretrained on ImageNet from TorchVision","title":"DenseNetTorchVisionStore"},{"location":"models/DenseNet/#mlmodule.models.densenet.stores.DenseNetPlaces365Store","text":"Model store to load DenseNet weights pretrained on Places365 See places365 documentation for more info. Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . July 2017. \u21a9","title":"DenseNetPlaces365Store"},{"location":"models/DistiluseMultilingual/","text":"Distiluse Multilingual Text model for computing sentence embeddings in multiple languages based on Sentence-Transformers framework 1 . Pre-trained models mlmodule . models . sentences . distilbert . pretrained . torch_distiluse_base_multilingual_v2 Multilingual model for semantic similarity See distiluse-base-multilingual-cased-v2 and sbert documentation for more information. Parameters: Name Type Description Default device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . required Base model This model is an implementation of a TorchMlModule . mlmodule.models.sentences.distilbert.modules.DistilUseBaseMultilingualCasedV2Module Multilingual model for semantic similarity See distiluse-base-multilingual-cased-v2 and sbert documentation for more information. Parameters: Name Type Description Default device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . device(type='cpu') Pre-trained state origins See the stores documentation for usage. mlmodule.models.sentences.distilbert.stores.SBERTDistiluseBaseMultilingualCasedV2Store Loads weights from SBERT's hugging face See Hugging face's documentation . Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 11 2020. URL: https://arxiv.org/abs/2004.09813 . \u21a9","title":"Distiluse Multilingual"},{"location":"models/DistiluseMultilingual/#distiluse-multilingual","text":"Text model for computing sentence embeddings in multiple languages based on Sentence-Transformers framework 1 .","title":"Distiluse Multilingual"},{"location":"models/DistiluseMultilingual/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/DistiluseMultilingual/#mlmodule.models.sentences.distilbert.pretrained.torch_distiluse_base_multilingual_v2","text":"Multilingual model for semantic similarity See distiluse-base-multilingual-cased-v2 and sbert documentation for more information. Parameters: Name Type Description Default device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . required","title":"torch_distiluse_base_multilingual_v2()"},{"location":"models/DistiluseMultilingual/#base-model","text":"This model is an implementation of a TorchMlModule .","title":"Base model"},{"location":"models/DistiluseMultilingual/#mlmodule.models.sentences.distilbert.modules.DistilUseBaseMultilingualCasedV2Module","text":"Multilingual model for semantic similarity See distiluse-base-multilingual-cased-v2 and sbert documentation for more information. Parameters: Name Type Description Default device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . device(type='cpu')","title":"DistilUseBaseMultilingualCasedV2Module"},{"location":"models/DistiluseMultilingual/#pre-trained-state-origins","text":"See the stores documentation for usage.","title":"Pre-trained state origins"},{"location":"models/DistiluseMultilingual/#mlmodule.models.sentences.distilbert.stores.SBERTDistiluseBaseMultilingualCasedV2Store","text":"Loads weights from SBERT's hugging face See Hugging face's documentation . Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 11 2020. URL: https://arxiv.org/abs/2004.09813 . \u21a9","title":"SBERTDistiluseBaseMultilingualCasedV2Store"},{"location":"models/MTCNN/","text":"MTCNN We are using facenet-pytorch to load pre-trained MTCNN model 1 , see https://github.com/timesler/facenet-pytorch . Pre-trained models mlmodule . models . mtcnn . pretrained . torch_mtcnn Pre-trained PyTorch's MTCNN face detection module by FaceNet Parameters: Name Type Description Default thresholds Tuple[float, float, float] MTCNN threshold hyperparameters required image_size Tuple[int, int] Image size after pre-preprocessing required min_face_size int Minimum face size in pixels required device torch.device Torch device to initialise the model weights required Returns: Type Description TorchMTCNNModule Pre-trained MTCNN model Base model The MTCNN model is an implementation of a TorchMlModule . mlmodule.models.mtcnn.modules.TorchMTCNNModule MTCNN face detection module Attributes: Name Type Description thresholds Tuple[float, float, float] MTCNN threshold hyperparameters image_size Tuple[int, int] Image size after pre-preprocessing min_face_size int Minimum face size in pixels device torch.device Torch device to initialise the model weights Provider store See the stores documentation for usage. mlmodule.models.mtcnn.stores.FaceNetMTCNNStore Pre-trained model states by Facenet for MTCNN These are identified by training_id=facenet . K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE Signal Processing Letters , 23(10):1499\u20131503, Oct 2016. doi:10.1109/LSP.2016.2603342 . \u21a9","title":"MTCNN"},{"location":"models/MTCNN/#mtcnn","text":"We are using facenet-pytorch to load pre-trained MTCNN model 1 , see https://github.com/timesler/facenet-pytorch .","title":"MTCNN"},{"location":"models/MTCNN/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/MTCNN/#mlmodule.models.mtcnn.pretrained.torch_mtcnn","text":"Pre-trained PyTorch's MTCNN face detection module by FaceNet Parameters: Name Type Description Default thresholds Tuple[float, float, float] MTCNN threshold hyperparameters required image_size Tuple[int, int] Image size after pre-preprocessing required min_face_size int Minimum face size in pixels required device torch.device Torch device to initialise the model weights required Returns: Type Description TorchMTCNNModule Pre-trained MTCNN model","title":"torch_mtcnn()"},{"location":"models/MTCNN/#base-model","text":"The MTCNN model is an implementation of a TorchMlModule .","title":"Base model"},{"location":"models/MTCNN/#mlmodule.models.mtcnn.modules.TorchMTCNNModule","text":"MTCNN face detection module Attributes: Name Type Description thresholds Tuple[float, float, float] MTCNN threshold hyperparameters image_size Tuple[int, int] Image size after pre-preprocessing min_face_size int Minimum face size in pixels device torch.device Torch device to initialise the model weights","title":"TorchMTCNNModule"},{"location":"models/MTCNN/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/MTCNN/#mlmodule.models.mtcnn.stores.FaceNetMTCNNStore","text":"Pre-trained model states by Facenet for MTCNN These are identified by training_id=facenet . K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE Signal Processing Letters , 23(10):1499\u20131503, Oct 2016. doi:10.1109/LSP.2016.2603342 . \u21a9","title":"FaceNetMTCNNStore"},{"location":"models/MagFace/","text":"MagFace We are using the official implementation of MagFace 1 in Pytorch. See https://github.com/IrvingMeng/MagFace . Pre-trained models mlmodule . models . magface . pretrained . torch_magface Pre-trained MagFace module Parameters: Name Type Description Default device torch.device Torch device to initialise the model weights required remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . required magnitude_threshold float Threshold to remove bad quality faces. The higher the stricter. Defaults to 22.5 . required Base models The MagFace model is an implementation of a TorchMlModule . mlmodule.models.magface.modules.TorchMagFaceModule MagFace face embeddings from MTCNN detected faces The input dataset should return a tuple of image data and bounding box information Attributes: Name Type Description device torch.device Torch device to initialise the model weights remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . magnitude_threshold float Threshold to remove bad quality faces. The higher the stricter. Defaults to 22.5 . Provider store See the stores documentation for usage. mlmodule.models.magface.stores.MagFaceStore Pre-trained model states by MagFace ( https://github.com/IrvingMeng/MagFace ) These are identified by training_id=magface . Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface: a universal representation for face recognition and quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 14225\u201314234. June 2021. \u21a9","title":"MagFace"},{"location":"models/MagFace/#magface","text":"We are using the official implementation of MagFace 1 in Pytorch. See https://github.com/IrvingMeng/MagFace .","title":"MagFace"},{"location":"models/MagFace/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/MagFace/#mlmodule.models.magface.pretrained.torch_magface","text":"Pre-trained MagFace module Parameters: Name Type Description Default device torch.device Torch device to initialise the model weights required remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . required magnitude_threshold float Threshold to remove bad quality faces. The higher the stricter. Defaults to 22.5 . required","title":"torch_magface()"},{"location":"models/MagFace/#base-models","text":"The MagFace model is an implementation of a TorchMlModule .","title":"Base models"},{"location":"models/MagFace/#mlmodule.models.magface.modules.TorchMagFaceModule","text":"MagFace face embeddings from MTCNN detected faces The input dataset should return a tuple of image data and bounding box information Attributes: Name Type Description device torch.device Torch device to initialise the model weights remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . magnitude_threshold float Threshold to remove bad quality faces. The higher the stricter. Defaults to 22.5 .","title":"TorchMagFaceModule"},{"location":"models/MagFace/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/MagFace/#mlmodule.models.magface.stores.MagFaceStore","text":"Pre-trained model states by MagFace ( https://github.com/IrvingMeng/MagFace ) These are identified by training_id=magface . Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface: a universal representation for face recognition and quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 14225\u201314234. June 2021. \u21a9","title":"MagFaceStore"},{"location":"models/ResNet/","text":"ResNet PyTorch implementation of ResNet 1 as defined in Torchvision . Pre-trained models mlmodule . models . resnet . pretrained . torch_resnet_imagenet TorchResNetModule model pre-trained on ImageNet Parameters: Name Type Description Default resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 required device torch.device Torch device to initialise the model weights device(type='cpu') training_mode TorchResNetTrainingMode | None Whether to return features or labels in the forward function. Used for training when computing the loss. None Returns: Type Description TorchResNetModule A PyTorch ResNet module pre-trained on ImageNet Base model The ResNet model is an implementation of a TorchMlModule . mlmodule.models.resnet.modules.TorchResNetModule PyTorch ResNet architecture. See PyTorch's documentation . Attributes: Name Type Description resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights training_mode TorchResNetTrainingMode | None Whether to return features or labels in the forward function. Used for training when computing the loss. Pre-trained state origins See the stores documentation for usage. mlmodule.models.resnet.stores.ResNetTorchVisionStore Model store to load ResNet weights pretrained on ImageNet from TorchVision Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , volume, 770\u2013778. 2016. doi:10.1109/CVPR.2016.90 . \u21a9","title":"ResNet"},{"location":"models/ResNet/#resnet","text":"PyTorch implementation of ResNet 1 as defined in Torchvision .","title":"ResNet"},{"location":"models/ResNet/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/ResNet/#mlmodule.models.resnet.pretrained.torch_resnet_imagenet","text":"TorchResNetModule model pre-trained on ImageNet Parameters: Name Type Description Default resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 required device torch.device Torch device to initialise the model weights device(type='cpu') training_mode TorchResNetTrainingMode | None Whether to return features or labels in the forward function. Used for training when computing the loss. None Returns: Type Description TorchResNetModule A PyTorch ResNet module pre-trained on ImageNet","title":"torch_resnet_imagenet()"},{"location":"models/ResNet/#base-model","text":"The ResNet model is an implementation of a TorchMlModule .","title":"Base model"},{"location":"models/ResNet/#mlmodule.models.resnet.modules.TorchResNetModule","text":"PyTorch ResNet architecture. See PyTorch's documentation . Attributes: Name Type Description resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights training_mode TorchResNetTrainingMode | None Whether to return features or labels in the forward function. Used for training when computing the loss.","title":"TorchResNetModule"},{"location":"models/ResNet/#pre-trained-state-origins","text":"See the stores documentation for usage.","title":"Pre-trained state origins"},{"location":"models/ResNet/#mlmodule.models.resnet.stores.ResNetTorchVisionStore","text":"Model store to load ResNet weights pretrained on ImageNet from TorchVision Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , volume, 770\u2013778. 2016. doi:10.1109/CVPR.2016.90 . \u21a9","title":"ResNetTorchVisionStore"},{"location":"models/VinVL/","text":"VinVL Pre-trained large-scale object-attribute detection (OD) model based on the ResNeXt-152 C4 architecture 1 . The OD model has been firstly trained on much larger amounts of data, combining multiple public object detection datasets, including COCO , OpenImages (OI) , Objects365 , and Visual Genome (VG) . Then it is fine-tuned on VG dataset alone, since VG is the only dataset with label attributes (see issue #120 ). It predicts objects from 1594 classes with attributes from 524 classes. See the code and the paper for details. Pre-trained models mlmodule . models . vinvl . pretrained . torch_vinvl_detector VinVL object detection model Parameters: Name Type Description Default score_threshold float required attr_score_threshold float required device torch.device PyTorch device attribute to initialise model. required Base model The VinVL model is an implementation of a TorchMlModule . mlmodule.models.vinvl.modules.TorchVinVLDetectorModule VinVL object detection model Attributes: Name Type Description score_threshold float attr_score_threshold float device torch.device PyTorch device attribute to initialise model. Provider store See the stores documentation for usage. mlmodule.models.vinvl.stores.VinVLStore Pre-trained model states for VinVL These are identified by training_id=vinvl . Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 5579\u20135588. June 2021. \u21a9","title":"VinVL"},{"location":"models/VinVL/#vinvl","text":"Pre-trained large-scale object-attribute detection (OD) model based on the ResNeXt-152 C4 architecture 1 . The OD model has been firstly trained on much larger amounts of data, combining multiple public object detection datasets, including COCO , OpenImages (OI) , Objects365 , and Visual Genome (VG) . Then it is fine-tuned on VG dataset alone, since VG is the only dataset with label attributes (see issue #120 ). It predicts objects from 1594 classes with attributes from 524 classes. See the code and the paper for details.","title":"VinVL"},{"location":"models/VinVL/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/VinVL/#mlmodule.models.vinvl.pretrained.torch_vinvl_detector","text":"VinVL object detection model Parameters: Name Type Description Default score_threshold float required attr_score_threshold float required device torch.device PyTorch device attribute to initialise model. required","title":"torch_vinvl_detector()"},{"location":"models/VinVL/#base-model","text":"The VinVL model is an implementation of a TorchMlModule .","title":"Base model"},{"location":"models/VinVL/#mlmodule.models.vinvl.modules.TorchVinVLDetectorModule","text":"VinVL object detection model Attributes: Name Type Description score_threshold float attr_score_threshold float device torch.device PyTorch device attribute to initialise model.","title":"TorchVinVLDetectorModule"},{"location":"models/VinVL/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/VinVL/#mlmodule.models.vinvl.stores.VinVLStore","text":"Pre-trained model states for VinVL These are identified by training_id=vinvl . Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 5579\u20135588. June 2021. \u21a9","title":"VinVLStore"},{"location":"models/keyframes/","text":"Video key-frames extractor This model implements two types of modules: a video frames encoder and the key-frames module. These models are an implementation of a TorchMlModule . Pre-trained models mlmodule . models . keyframes . pretrained . torch_keyframes_resnet_imagenet KeyFrames selector with PyTorch's ResNet pre-trained on ImageNet Parameters: Name Type Description Default resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 required fps float The number of frames per seconds to extract from the video. Defaults to 1. 1 device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description KeyFrameSelector Keyframes model with ResNet pre-trained on ImageNet encoder mlmodule . models . keyframes . pretrained . torch_keyframes_densenet_imagenet KeyFrames selector with PyTorch DenseNet model pretrained on ImageNet Parameters: Name Type Description Default densenet_arch DenseNetArch Identifier for the DenseNet architecture. Must be one of: - densenet121 - densenet161 - densenet169 - densenet201 required fps float The number of frames per seconds to extract from the video. Defaults to 1. 1 device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description KeyFrameSelector Keyframes model with DenseNet pre-trained on ImageNet encoder mlmodule . models . keyframes . pretrained . torch_keyframes_densenet_places365 KeyFrames selector with PyTorch DenseNet model pretrained on Places365. See places365 documentation for more info. Parameters: Name Type Description Default device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description KeyFrameSelector Keyframes model with DenseNet model pretrained on Places365 Base key-frames selector model These models allow to extract key-frames from a video. mlmodule.models.keyframes.selectors.KeyFrameSelector Video key-frames selector Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor, torch.Tensor] The PyTorch module to encode frames. fps float The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . Base video frames encoder model mlmodule.models.keyframes.encoders.VideoFramesEncoder Video frames encoder This module will extract and encode frames of a video using an image_encoder . Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor] The PyTorch module to encode frames fps float The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"Video key-frames extractor"},{"location":"models/keyframes/#video-key-frames-extractor","text":"This model implements two types of modules: a video frames encoder and the key-frames module. These models are an implementation of a TorchMlModule .","title":"Video key-frames extractor"},{"location":"models/keyframes/#pre-trained-models","text":"","title":"Pre-trained models"},{"location":"models/keyframes/#mlmodule.models.keyframes.pretrained.torch_keyframes_resnet_imagenet","text":"KeyFrames selector with PyTorch's ResNet pre-trained on ImageNet Parameters: Name Type Description Default resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 required fps float The number of frames per seconds to extract from the video. Defaults to 1. 1 device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description KeyFrameSelector Keyframes model with ResNet pre-trained on ImageNet encoder","title":"torch_keyframes_resnet_imagenet()"},{"location":"models/keyframes/#mlmodule.models.keyframes.pretrained.torch_keyframes_densenet_imagenet","text":"KeyFrames selector with PyTorch DenseNet model pretrained on ImageNet Parameters: Name Type Description Default densenet_arch DenseNetArch Identifier for the DenseNet architecture. Must be one of: - densenet121 - densenet161 - densenet169 - densenet201 required fps float The number of frames per seconds to extract from the video. Defaults to 1. 1 device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description KeyFrameSelector Keyframes model with DenseNet pre-trained on ImageNet encoder","title":"torch_keyframes_densenet_imagenet()"},{"location":"models/keyframes/#mlmodule.models.keyframes.pretrained.torch_keyframes_densenet_places365","text":"KeyFrames selector with PyTorch DenseNet model pretrained on Places365. See places365 documentation for more info. Parameters: Name Type Description Default device torch.device Torch device to initialise the model weights device(type='cpu') Returns: Type Description KeyFrameSelector Keyframes model with DenseNet model pretrained on Places365","title":"torch_keyframes_densenet_places365()"},{"location":"models/keyframes/#base-key-frames-selector-model","text":"These models allow to extract key-frames from a video.","title":"Base key-frames selector model"},{"location":"models/keyframes/#mlmodule.models.keyframes.selectors.KeyFrameSelector","text":"Video key-frames selector Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor, torch.Tensor] The PyTorch module to encode frames. fps float The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"KeyFrameSelector"},{"location":"models/keyframes/#base-video-frames-encoder-model","text":"","title":"Base video frames encoder model"},{"location":"models/keyframes/#mlmodule.models.keyframes.encoders.VideoFramesEncoder","text":"Video frames encoder This module will extract and encode frames of a video using an image_encoder . Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor] The PyTorch module to encode frames fps float The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"VideoFramesEncoder"},{"location":"references/callbacks/","text":"Callbacks Callbacks are used to control to control how a runner handles the results (features, labels or model weights). They are classes implementing a pre-defined set of functions: save_features save_label_scores save_bounding_boxes save_frames In memory callbacks These callbacks accumulate results in-memory. They expose their results via object attributes. mlmodule.callbacks.memory.CollectFeaturesInMemory dataclass Callback to collect features in memory Attributes: Name Type Description indices list List of dataset indices features numpy.ndarray Array of features. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_features ( self , model : Any , indices : Sequence , features : Union [ torch . Tensor , numpy . ndarray ]) -> None Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required mlmodule.callbacks.memory.CollectLabelsInMemory dataclass Callback to collect labels in memory Attributes: Name Type Description indices list List of dataset indices label_scores numpy.ndarray Array of label scores. The first dimension correspond to self.indices values. labels list[str] List of matching labels (label with maximum score) Note This callback works with any array-like features save_label_scores ( self , model : ModelWithLabels , indices : Sequence , labels_scores : Union [ torch . Tensor , numpy . ndarray ]) -> None Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required mlmodule.callbacks.memory.CollectBoundingBoxesInMemory dataclass Callback to collect bounding boxes predictions in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_bounding_boxes ( self , model : Any , indices : Sequence , bounding_boxes : Sequence [ mlmodule . predictions . BatchBoundingBoxesPrediction [ Union [ torch . Tensor , numpy . ndarray ]]]) -> None Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required mlmodule.callbacks.memory.CollectVideoFramesInMemory dataclass Callback to collect video frames in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_frames ( self , model : Any , indices : Sequence , frames : Sequence [ mlmodule . predictions . BatchVideoFramesPrediction [ Union [ torch . Tensor , numpy . ndarray ]]]) -> None Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required Callbacks for training mlmodule.callbacks.states.SaveModelState dataclass Simple callback to save model state during training. If state are saved during training (every X epochs, see TorchTrainingOptions ) the current epoch number is appended to the state_key.training_id in the following way: <state_key.training_id>-e<num_epoch> . When the training is complete, just the state_key.training_id is used. Attributes: Name Type Description store AbstractStateStore Object to handle model state saving state_key StateKey State identifier for the training activity. Warning This callback only saves the model state, thus does not create a whole training checkpoint (optimizer state, loss, etc..). save_model_state ( self , engine : Engine , model : Any ) -> None Save model state by calling the state store Parameters: Name Type Description Default model Any The MLModule model to save required Write your own callbacks mlmodule.callbacks.base.BaseSaveFeaturesCallback save_features ( self , model : Any , indices : Sequence , features : - _ContraArrayType ) -> None Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required mlmodule.callbacks.base.BaseSaveLabelsCallback save_label_scores ( self , model : ModelWithLabels , indices : Sequence , labels_scores : - _ContraArrayType ) -> None Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required mlmodule.callbacks.base.BaseSaveBoundingBoxCallback save_bounding_boxes ( self , model : Any , indices : Sequence , bounding_boxes : Sequence [ mlmodule . predictions . BatchBoundingBoxesPrediction [ - _ContraArrayType ]]) -> None Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required mlmodule.callbacks.base.BaseSaveVideoFramesCallback save_frames ( self , model : Any , indices : Sequence , frames : Sequence [ mlmodule . predictions . BatchVideoFramesPrediction [ - _ContraArrayType ]]) -> None Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required mlmodule.callbacks.base.BaseRunnerEndCallback on_runner_end ( self , model : Any ) -> None Called when the runner finishes This can be used to do clean up. For instance if the data is being processed by a thread, this function can wait for the thread to finish.$ Parameters: Name Type Description Default model Any The MLModule model required","title":"Callbacks"},{"location":"references/callbacks/#callbacks","text":"Callbacks are used to control to control how a runner handles the results (features, labels or model weights). They are classes implementing a pre-defined set of functions: save_features save_label_scores save_bounding_boxes save_frames","title":"Callbacks"},{"location":"references/callbacks/#in-memory-callbacks","text":"These callbacks accumulate results in-memory. They expose their results via object attributes.","title":"In memory callbacks"},{"location":"references/callbacks/#mlmodule.callbacks.memory.CollectFeaturesInMemory","text":"Callback to collect features in memory Attributes: Name Type Description indices list List of dataset indices features numpy.ndarray Array of features. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectFeaturesInMemory"},{"location":"references/callbacks/#mlmodule.callbacks.memory.CollectFeaturesInMemory.save_features","text":"Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required","title":"save_features()"},{"location":"references/callbacks/#mlmodule.callbacks.memory.CollectLabelsInMemory","text":"Callback to collect labels in memory Attributes: Name Type Description indices list List of dataset indices label_scores numpy.ndarray Array of label scores. The first dimension correspond to self.indices values. labels list[str] List of matching labels (label with maximum score) Note This callback works with any array-like features","title":"CollectLabelsInMemory"},{"location":"references/callbacks/#mlmodule.callbacks.memory.CollectLabelsInMemory.save_label_scores","text":"Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required","title":"save_label_scores()"},{"location":"references/callbacks/#mlmodule.callbacks.memory.CollectBoundingBoxesInMemory","text":"Callback to collect bounding boxes predictions in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectBoundingBoxesInMemory"},{"location":"references/callbacks/#mlmodule.callbacks.memory.CollectBoundingBoxesInMemory.save_bounding_boxes","text":"Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required","title":"save_bounding_boxes()"},{"location":"references/callbacks/#mlmodule.callbacks.memory.CollectVideoFramesInMemory","text":"Callback to collect video frames in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectVideoFramesInMemory"},{"location":"references/callbacks/#mlmodule.callbacks.memory.CollectVideoFramesInMemory.save_frames","text":"Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required","title":"save_frames()"},{"location":"references/callbacks/#callbacks-for-training","text":"","title":"Callbacks for training"},{"location":"references/callbacks/#mlmodule.callbacks.states.SaveModelState","text":"Simple callback to save model state during training. If state are saved during training (every X epochs, see TorchTrainingOptions ) the current epoch number is appended to the state_key.training_id in the following way: <state_key.training_id>-e<num_epoch> . When the training is complete, just the state_key.training_id is used. Attributes: Name Type Description store AbstractStateStore Object to handle model state saving state_key StateKey State identifier for the training activity. Warning This callback only saves the model state, thus does not create a whole training checkpoint (optimizer state, loss, etc..).","title":"SaveModelState"},{"location":"references/callbacks/#mlmodule.callbacks.states.SaveModelState.save_model_state","text":"Save model state by calling the state store Parameters: Name Type Description Default model Any The MLModule model to save required","title":"save_model_state()"},{"location":"references/callbacks/#write-your-own-callbacks","text":"","title":"Write your own callbacks"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseSaveFeaturesCallback","text":"","title":"BaseSaveFeaturesCallback"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseSaveFeaturesCallback.save_features","text":"Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required","title":"save_features()"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseSaveLabelsCallback","text":"","title":"BaseSaveLabelsCallback"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseSaveLabelsCallback.save_label_scores","text":"Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required","title":"save_label_scores()"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseSaveBoundingBoxCallback","text":"","title":"BaseSaveBoundingBoxCallback"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseSaveBoundingBoxCallback.save_bounding_boxes","text":"Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required","title":"save_bounding_boxes()"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseSaveVideoFramesCallback","text":"","title":"BaseSaveVideoFramesCallback"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseSaveVideoFramesCallback.save_frames","text":"Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required","title":"save_frames()"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseRunnerEndCallback","text":"","title":"BaseRunnerEndCallback"},{"location":"references/callbacks/#mlmodule.callbacks.base.BaseRunnerEndCallback.on_runner_end","text":"Called when the runner finishes This can be used to do clean up. For instance if the data is being processed by a thread, this function can wait for the thread to finish.$ Parameters: Name Type Description Default model Any The MLModule model required","title":"on_runner_end()"},{"location":"references/datasets/","text":"Datasets Torch datasets Torch dataset are implementing dataset as suggested in the Datasets & Dataloaders guide. These datasets are compatible with torch.utils.data.DataLoader . In-memory list datasets mlmodule.torch.datasets.ListDataset dataclass Simple dataset that contains a list of objects in memory Attributes: Name Type Description objects Sequence[_DatasetType] List of objects of the dataset mlmodule.torch.datasets.ListDatasetIndexed dataclass Simple dataset that contains a list of objects in memory with custom indices Attributes: Name Type Description indices Sequence[_IndicesType] Indices to track objects in results objects Sequence[_DatasetType] List of objects of the dataset Local files datasets mlmodule.torch.datasets.LocalBinaryFilesDataset dataclass Dataset that reads a list of local file names and returns their content as bytes Attributes: Name Type Description paths Sequence[_PathLike] List of paths to files Image datasets mlmodule.torch.datasets.ImageDataset dataclass Dataset that returns PIL.Image.Image from a dataset of images in bytes format Attributes: Name Type Description binary_files_dataset TorchDataset[_IndicesType, bytes] Dataset to load images. Usually a LocalBinaryFilesDataset . resize_image_size tuple[int, int] | None Optionally reduce the image size on load mode str | None Optional mode to apply when loading the image. See PIL Image.draft parameters. Bounding box datasets mlmodule.torch.datasets.ImageBoundingBoxDataset dataclass Dataset that returns tuple of Image and bounding box data from a list of file names and bounding box information Attributes: Name Type Description image_dataset TorchDataset[_IndicesType, Image] The dataset of images bounding_boxes Sequence[BatchBoundingBoxesPrediction[np.ndarray]] The bounding boxes predictions for all given images crop_image bool Whether to crop the image at the bounding box when loading it Training datasets mlmodule.torch.datasets.TorchTrainingDataset dataclass Dataset for training that returns a tuple (payload, target) where payload is the value returned by dataset and target the corrisponding element in targets . Attributes: Name Type Description dataset TorchDataset[_IndicesType, _DatasetType] A TorchDataset targets Sequence[_TargetsType] Training target for each element of the dataset Note Length of targets must match the size of the dataset . Warning TorchTrainingDataset doesn't work is with Torchvision datasets in torchvision.datasets . Write your own dataset Dataset types depends on the runner used, refer to the runner list to know which type to implement. Below is the list of dataset protocols that are specified by mlmodule mlmodule.torch.datasets.TorchDataset PyTorch dataset protocol In order to be used with the PyTorch runners, a TorchDataset should expose two functions __getitem__ and __len__ . __getitem__ ( self , index : int ) -> Tuple [ + _IndicesType , + _DatasetType ] special Get an item of the dataset by index Parameters: Name Type Description Default index int The index of the element to get required Returns: Type Description Tuple[_IndicesType, _DatasetType] A tuple of dataset index of the element and value of the element __len__ ( self ) -> int special Length of the dataset Returns: Type Description int The length of the dataset getitem_indices ( self , index : int ) -> + _IndicesType The value of dataset indices at index position Parameters: Name Type Description Default index int The position of the element to get required Returns: Type Description _IndicesType The value of the indices at the position","title":"Datasets"},{"location":"references/datasets/#datasets","text":"","title":"Datasets"},{"location":"references/datasets/#torch-datasets","text":"Torch dataset are implementing dataset as suggested in the Datasets & Dataloaders guide. These datasets are compatible with torch.utils.data.DataLoader .","title":"Torch datasets"},{"location":"references/datasets/#in-memory-list-datasets","text":"","title":"In-memory list datasets"},{"location":"references/datasets/#mlmodule.torch.datasets.ListDataset","text":"Simple dataset that contains a list of objects in memory Attributes: Name Type Description objects Sequence[_DatasetType] List of objects of the dataset","title":"ListDataset"},{"location":"references/datasets/#mlmodule.torch.datasets.ListDatasetIndexed","text":"Simple dataset that contains a list of objects in memory with custom indices Attributes: Name Type Description indices Sequence[_IndicesType] Indices to track objects in results objects Sequence[_DatasetType] List of objects of the dataset","title":"ListDatasetIndexed"},{"location":"references/datasets/#local-files-datasets","text":"","title":"Local files datasets"},{"location":"references/datasets/#mlmodule.torch.datasets.LocalBinaryFilesDataset","text":"Dataset that reads a list of local file names and returns their content as bytes Attributes: Name Type Description paths Sequence[_PathLike] List of paths to files","title":"LocalBinaryFilesDataset"},{"location":"references/datasets/#image-datasets","text":"","title":"Image datasets"},{"location":"references/datasets/#mlmodule.torch.datasets.ImageDataset","text":"Dataset that returns PIL.Image.Image from a dataset of images in bytes format Attributes: Name Type Description binary_files_dataset TorchDataset[_IndicesType, bytes] Dataset to load images. Usually a LocalBinaryFilesDataset . resize_image_size tuple[int, int] | None Optionally reduce the image size on load mode str | None Optional mode to apply when loading the image. See PIL Image.draft parameters.","title":"ImageDataset"},{"location":"references/datasets/#bounding-box-datasets","text":"","title":"Bounding box datasets"},{"location":"references/datasets/#mlmodule.torch.datasets.ImageBoundingBoxDataset","text":"Dataset that returns tuple of Image and bounding box data from a list of file names and bounding box information Attributes: Name Type Description image_dataset TorchDataset[_IndicesType, Image] The dataset of images bounding_boxes Sequence[BatchBoundingBoxesPrediction[np.ndarray]] The bounding boxes predictions for all given images crop_image bool Whether to crop the image at the bounding box when loading it","title":"ImageBoundingBoxDataset"},{"location":"references/datasets/#training-datasets","text":"","title":"Training datasets"},{"location":"references/datasets/#mlmodule.torch.datasets.TorchTrainingDataset","text":"Dataset for training that returns a tuple (payload, target) where payload is the value returned by dataset and target the corrisponding element in targets . Attributes: Name Type Description dataset TorchDataset[_IndicesType, _DatasetType] A TorchDataset targets Sequence[_TargetsType] Training target for each element of the dataset Note Length of targets must match the size of the dataset . Warning TorchTrainingDataset doesn't work is with Torchvision datasets in torchvision.datasets .","title":"TorchTrainingDataset"},{"location":"references/datasets/#write-your-own-dataset","text":"Dataset types depends on the runner used, refer to the runner list to know which type to implement. Below is the list of dataset protocols that are specified by mlmodule","title":"Write your own dataset"},{"location":"references/datasets/#mlmodule.torch.datasets.TorchDataset","text":"PyTorch dataset protocol In order to be used with the PyTorch runners, a TorchDataset should expose two functions __getitem__ and __len__ .","title":"TorchDataset"},{"location":"references/datasets/#mlmodule.torch.datasets.TorchDataset.__getitem__","text":"Get an item of the dataset by index Parameters: Name Type Description Default index int The index of the element to get required Returns: Type Description Tuple[_IndicesType, _DatasetType] A tuple of dataset index of the element and value of the element","title":"__getitem__()"},{"location":"references/datasets/#mlmodule.torch.datasets.TorchDataset.__len__","text":"Length of the dataset Returns: Type Description int The length of the dataset","title":"__len__()"},{"location":"references/datasets/#mlmodule.torch.datasets.TorchDataset.getitem_indices","text":"The value of dataset indices at index position Parameters: Name Type Description Default index int The position of the element to get required Returns: Type Description _IndicesType The value of the indices at the position","title":"getitem_indices()"},{"location":"references/labels/","text":"mlmodule.labels.base.LabelSet dataclass Label set is an ordered list of labels used for classification tasks Attributes: Name Type Description label_set_unique_id str Unique identifier for a label set label_list List[str] Ordered list of labels label_to_idx dict Dict with items (label_name, label_index) Examples: LabelSet objects are used as classic lists: animal_labels = LabelSet ( label_set_unique_id = \"animals\" , label_list = [ \"cat\" , \"dog\" ] ) print ( animal_labels . label_set_unique_id ) # \"animals\" print ( animal_labels [ 0 ]) # \"cat\" print ( animal_labels [ 1 ]) # \"dog\" print ( len ( animal_labels )) # 2 print ( animal_labels . get_label_ids ([ \"dog\" ])) # [1] get_label_ids ( self , labels : List [ str ]) -> List [ int ] Get the list of label indices for the provided labels. Parameters: Name Type Description Default labels List[str] list of labels of which to obtain indices required Exceptions: Type Description ValueError when a label in labels is not found in the label set Returns: Type Description indices (List[int]) list of indices Available label sets mlmodule.labels.imagenet.IMAGENET_LABELS mlmodule.labels.places_io.PLACES_IN_OUT_DOOR mlmodule.labels.places_io.PLACES_IO_LABELS mlmodule.labels.places.PLACES_LABELS mlmodule.labels.vinvl_attributes.VINVL_ATTRIBUTE_LABELS mlmodule.labels.vinvl.VINVL_LABELS","title":"Labels"},{"location":"references/labels/#mlmodule.labels.base.LabelSet","text":"Label set is an ordered list of labels used for classification tasks Attributes: Name Type Description label_set_unique_id str Unique identifier for a label set label_list List[str] Ordered list of labels label_to_idx dict Dict with items (label_name, label_index) Examples: LabelSet objects are used as classic lists: animal_labels = LabelSet ( label_set_unique_id = \"animals\" , label_list = [ \"cat\" , \"dog\" ] ) print ( animal_labels . label_set_unique_id ) # \"animals\" print ( animal_labels [ 0 ]) # \"cat\" print ( animal_labels [ 1 ]) # \"dog\" print ( len ( animal_labels )) # 2 print ( animal_labels . get_label_ids ([ \"dog\" ])) # [1]","title":"LabelSet"},{"location":"references/labels/#mlmodule.labels.base.LabelSet.get_label_ids","text":"Get the list of label indices for the provided labels. Parameters: Name Type Description Default labels List[str] list of labels of which to obtain indices required Exceptions: Type Description ValueError when a label in labels is not found in the label set Returns: Type Description indices (List[int]) list of indices","title":"get_label_ids()"},{"location":"references/labels/#available-label-sets","text":"mlmodule.labels.imagenet.IMAGENET_LABELS mlmodule.labels.places_io.PLACES_IN_OUT_DOOR mlmodule.labels.places_io.PLACES_IO_LABELS mlmodule.labels.places.PLACES_LABELS mlmodule.labels.vinvl_attributes.VINVL_ATTRIBUTE_LABELS mlmodule.labels.vinvl.VINVL_LABELS","title":"Available label sets"},{"location":"references/options/","text":"Options mlmodule.torch.options.TorchRunnerOptions dataclass Options for PyTorch runners Attributes: Name Type Description device torch.device Torch device data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar mlmodule.torch.options.TorchMultiGPURunnerOptions dataclass Options for PyTorch multi-gpu runners Attributes: Name Type Description data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar. Default, False. dist_options dict Options passed to ignite.distributed.Parallel . seed int random state seed to set. Default, 543. Note data_loader_options 's options batch_size and num_worker will be automatically scaled according to world_size and nprocs respectively. For more info visit auto_dataloader documentation . Note dist_options usually include backend and nproc_per_node parameters. For more info visit PyTorch Ignite's distributed documentation . mlmodule.torch.options.TorchTrainingOptions dataclass Options for PyTorch training runners Attributes: Name Type Description criterion Union[Callable, torch.nn.Module] the loss function to use during training. optimizer torch.optim.Optimizer Optimization strategy to use during training. num_epochs int number of epochs to train the model. validate_every int run model's validation every validate_every epochs. checkpoint_every Optional[int] store training checkpoint every checkpoint_every epochs. metrics Dict[str, Metric] Dictionary where values are Ignite's metrics to compute during evaluation. data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar. Default, False. dist_options dict Options passed to ignite.distributed.Parallel . seed int random state seed to set. Default, 543. Note data_loader_options 's options batch_size and num_worker will be automatically scaled according to world_size and nprocs respectively. For more info visit auto_dataloader documentation . Note dist_options usually include backend and nproc_per_node parameters. For more info visit PyTorch Ignite's distributed documentation .","title":"Options"},{"location":"references/options/#options","text":"","title":"Options"},{"location":"references/options/#mlmodule.torch.options.TorchRunnerOptions","text":"Options for PyTorch runners Attributes: Name Type Description device torch.device Torch device data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar","title":"TorchRunnerOptions"},{"location":"references/options/#mlmodule.torch.options.TorchMultiGPURunnerOptions","text":"Options for PyTorch multi-gpu runners Attributes: Name Type Description data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar. Default, False. dist_options dict Options passed to ignite.distributed.Parallel . seed int random state seed to set. Default, 543. Note data_loader_options 's options batch_size and num_worker will be automatically scaled according to world_size and nprocs respectively. For more info visit auto_dataloader documentation . Note dist_options usually include backend and nproc_per_node parameters. For more info visit PyTorch Ignite's distributed documentation .","title":"TorchMultiGPURunnerOptions"},{"location":"references/options/#mlmodule.torch.options.TorchTrainingOptions","text":"Options for PyTorch training runners Attributes: Name Type Description criterion Union[Callable, torch.nn.Module] the loss function to use during training. optimizer torch.optim.Optimizer Optimization strategy to use during training. num_epochs int number of epochs to train the model. validate_every int run model's validation every validate_every epochs. checkpoint_every Optional[int] store training checkpoint every checkpoint_every epochs. metrics Dict[str, Metric] Dictionary where values are Ignite's metrics to compute during evaluation. data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar. Default, False. dist_options dict Options passed to ignite.distributed.Parallel . seed int random state seed to set. Default, 543. Note data_loader_options 's options batch_size and num_worker will be automatically scaled according to world_size and nprocs respectively. For more info visit auto_dataloader documentation . Note dist_options usually include backend and nproc_per_node parameters. For more info visit PyTorch Ignite's distributed documentation .","title":"TorchTrainingOptions"},{"location":"references/runners/","text":"Runners Runners are used to execute inference or training of a model. They are initialised with a model , a dataset , callbacks and options . They are executed with the run function which takes no arguments. Inference mlmodule.torch.runners.TorchInferenceRunner Runner for inference tasks on PyTorch models Supports CPU or single GPU inference. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchRunnerOptions PyTorch options run ( self ) -> None Runs inference mlmodule.torch.runners.TorchInferenceMultiGPURunner Runner for inference tasks on PyTorch models Supports CPU and multi-GPU inference with native torch backends. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchMultiGPURunnerOptions PyTorch multi-gpu options run ( self ) -> None Runs inference Training mlmodule.torch.runners.TorchTrainingRunner dataclass Runner for training tasks on PyTorch models Supports CPU and multi-GPU training with multiple backends. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset Tuple[TorchTrainingDataset, TorchTrainingDataset] Train and test datasets for the runner callbacks List[Union[BaseRunnerEndCallback, SaveModelState]] Callback to save model weights plus one or more callbacks for when the runner ends. options TorchTrainingOptions PyTorch training options run ( self ) -> None Runs training Write your own runner mlmodule.runners.BaseRunner A runner takes a model and run an action on it (inference, training...) It must implement the run function and call the callbacks to save prediction or model weights. Attributes: Name Type Description model _ModelType The model object to run the action against dataset _DataType The input dataset callbacks List[_CallbackType] Callbacks to save model state or predictions options _OptionsType Options of the runner (devices...) Note The _ModelType , _DataType , _CallbackType and _OptionsType are generic types that should be specified when implementing a runner. Examples: This is an example of a runner that applies a function to each element of list dataset. It passes the returned data to the save_features callback. from mlmodule.callbacks.base import ( BaseSaveFeaturesCallback , callbacks_caller ) from mlmodule.runners import BaseRunner class NumpySumRunnerExample ( BaseRunner [ Callable , # _ModelType List , # _DataType BaseSaveFeaturesCallback , # _CallbackType None # _OptionType ]): def run ( self ): for index , data in enumerate ( self . dataset ): # Helper function to call all matching callbacks callbacks_caller ( self . callbacks , \"save_features\" , index , self . model ( data ) )","title":"Runners"},{"location":"references/runners/#runners","text":"Runners are used to execute inference or training of a model. They are initialised with a model , a dataset , callbacks and options . They are executed with the run function which takes no arguments.","title":"Runners"},{"location":"references/runners/#inference","text":"","title":"Inference"},{"location":"references/runners/#mlmodule.torch.runners.TorchInferenceRunner","text":"Runner for inference tasks on PyTorch models Supports CPU or single GPU inference. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchRunnerOptions PyTorch options","title":"TorchInferenceRunner"},{"location":"references/runners/#mlmodule.torch.runners.TorchInferenceRunner.run","text":"Runs inference","title":"run()"},{"location":"references/runners/#mlmodule.torch.runners.TorchInferenceMultiGPURunner","text":"Runner for inference tasks on PyTorch models Supports CPU and multi-GPU inference with native torch backends. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchMultiGPURunnerOptions PyTorch multi-gpu options","title":"TorchInferenceMultiGPURunner"},{"location":"references/runners/#mlmodule.torch.runners.TorchInferenceMultiGPURunner.run","text":"Runs inference","title":"run()"},{"location":"references/runners/#training","text":"","title":"Training"},{"location":"references/runners/#mlmodule.torch.runners.TorchTrainingRunner","text":"Runner for training tasks on PyTorch models Supports CPU and multi-GPU training with multiple backends. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset Tuple[TorchTrainingDataset, TorchTrainingDataset] Train and test datasets for the runner callbacks List[Union[BaseRunnerEndCallback, SaveModelState]] Callback to save model weights plus one or more callbacks for when the runner ends. options TorchTrainingOptions PyTorch training options","title":"TorchTrainingRunner"},{"location":"references/runners/#mlmodule.torch.runners.TorchTrainingRunner.run","text":"Runs training","title":"run()"},{"location":"references/runners/#write-your-own-runner","text":"","title":"Write your own runner"},{"location":"references/runners/#mlmodule.runners.BaseRunner","text":"A runner takes a model and run an action on it (inference, training...) It must implement the run function and call the callbacks to save prediction or model weights. Attributes: Name Type Description model _ModelType The model object to run the action against dataset _DataType The input dataset callbacks List[_CallbackType] Callbacks to save model state or predictions options _OptionsType Options of the runner (devices...) Note The _ModelType , _DataType , _CallbackType and _OptionsType are generic types that should be specified when implementing a runner. Examples: This is an example of a runner that applies a function to each element of list dataset. It passes the returned data to the save_features callback. from mlmodule.callbacks.base import ( BaseSaveFeaturesCallback , callbacks_caller ) from mlmodule.runners import BaseRunner class NumpySumRunnerExample ( BaseRunner [ Callable , # _ModelType List , # _DataType BaseSaveFeaturesCallback , # _CallbackType None # _OptionType ]): def run ( self ): for index , data in enumerate ( self . dataset ): # Helper function to call all matching callbacks callbacks_caller ( self . callbacks , \"save_features\" , index , self . model ( data ) )","title":"BaseRunner"},{"location":"references/states/","text":"States are managed with two concepts: StateType : Represents a family of states that are compatible with each other . In general, a model can be loaded with any pre-trained state if it matches its state_type attribute. StateKey : The identifier of a state instance, it should uniquely identify the result of a training activity for a model. mlmodule.states.StateType dataclass Definition for a type of state A state type is used to identify states that can be re-used accross models. For instance, the weights from ResNet18 with 1000 classes pretrained on ImageNet can be reused to initialised the weights of a ResNet18 for binary classification. In this scenario, the state type of the ResNet trained on ImageNet can be re-used to partially initialize the binary classification ResNet18. This is also used for models like key-frames extraction. Key-frames extraction does not define a new weights architecture, it is a wrapper around an image encoder. Therefore, any state that can be loaded into the image encoder, can also be loaded into the key-frame extractor. They share the same state type. As a convention, two state types are compatible when backend and architecture attributes are the same. This is implemented in the is_compatible_with method. Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description backend str The model backend. For instance pytorch . architecture str Identifier for the architecture (e.g. torchresnet18 ...). extra tuple[str, ...] Additional information to identify architecture variants (number of output classes...). is_compatible_with ( self , other : StateType ) -> bool Tells whether two architecture are compatible with each other. Parameters: Name Type Description Default other StateType The other architecture to compare required Returns: Type Description bool true if backend and architecture attributes match. mlmodule.states.StateKey dataclass Identifier for a state of a trained model Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description state_type StateType Identifies the type of state training_id str Identifies the training activity that was used to get to this state. mlmodule . states . VALID_NAMES The pattern for valid architecture name in StateType.architecture . It must be alphanumeric characters separated with dashes (i.e. [\\w\\-\\d] )","title":"States"},{"location":"references/states/#mlmodule.states.StateType","text":"Definition for a type of state A state type is used to identify states that can be re-used accross models. For instance, the weights from ResNet18 with 1000 classes pretrained on ImageNet can be reused to initialised the weights of a ResNet18 for binary classification. In this scenario, the state type of the ResNet trained on ImageNet can be re-used to partially initialize the binary classification ResNet18. This is also used for models like key-frames extraction. Key-frames extraction does not define a new weights architecture, it is a wrapper around an image encoder. Therefore, any state that can be loaded into the image encoder, can also be loaded into the key-frame extractor. They share the same state type. As a convention, two state types are compatible when backend and architecture attributes are the same. This is implemented in the is_compatible_with method. Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description backend str The model backend. For instance pytorch . architecture str Identifier for the architecture (e.g. torchresnet18 ...). extra tuple[str, ...] Additional information to identify architecture variants (number of output classes...).","title":"StateType"},{"location":"references/states/#mlmodule.states.StateType.is_compatible_with","text":"Tells whether two architecture are compatible with each other. Parameters: Name Type Description Default other StateType The other architecture to compare required Returns: Type Description bool true if backend and architecture attributes match.","title":"is_compatible_with()"},{"location":"references/states/#mlmodule.states.StateKey","text":"Identifier for a state of a trained model Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description state_type StateType Identifies the type of state training_id str Identifies the training activity that was used to get to this state.","title":"StateKey"},{"location":"references/states/#mlmodule.states.VALID_NAMES","text":"The pattern for valid architecture name in StateType.architecture . It must be alphanumeric characters separated with dashes (i.e. [\\w\\-\\d] )","title":"VALID_NAMES"},{"location":"references/stores/","text":"Stores Stores are used to load and save models state. They define 3 methods: get_state_keys , load and save . The get_state_keys method allows to list available pre-trained states for a given type of state. It usually called from a model . # List available model states in store for the model state type state_keys = store . get_state_keys ( model . state_type ) print ( state_keys ) # Prints for a ResNet pre-trained on ImageNet: # [ # StateKey( # state_type=StateType( # backend='pytorch', # architecture='resnet18', # extra=('cls1000',) # ), # training_id='imagenet' # ) # ] See states documentation for more information on how pre-trained states are identified. Then, a state can be loaded into the model . # Getting the first state key resnet_imagenet_state_key = state_keys [ 0 ] # Loading it into the model store . load ( model , state_key = resnet_imagenet_state_key ) A model can be saved by specifying a training_id which should uniquely identify the training activity that yielded this model's state. store . save ( model , training_id = \"2022-01-01-finetuned-imagenet\" ) See AbstractStateStore for more details on these methods. MLModule pre-trained models MLModule provides model weights for all defined models through the MlModule Store . mlmodule . stores . Store () -> GitHUBReleaseStore MlModule model state store. Examples: The store can be used to list available pre-trained states for a model store = Store () states = store . get_state_keys ( model . state_type ) And load a given state to a model store . load ( model , state_key = states [ 0 ]) Alternative stores These stores can be used if you want to store model states locally or on S3 storage. mlmodule.stores.local.LocalStateStore dataclass Local filebased store Attributes: Name Type Description folder str Path to the folder to save model's state mlmodule.stores.s3.S3StateStore dataclass State store on top of S3 object storage Given the state keys, states are organised in in folders with the following structure: base_path/ \u251c\u2500 { backend } / \u2502 \u251c\u2500 { architecture } . { extra1 } . { extra2 } . { training_id } .pt \u251c\u2500 pytorch/ # e.g. for torch models \u2502 \u251c\u2500 resnet18.cls1000.imagenet.pt \u2502 \u251c\u2500 clip-image-rn50.clip.pt Attributes: Name Type Description bucket str Bucket to use to store states session_kwargs dict Arguments passed to initialise boto3.session.Session s3_endpoint_url str To connect to S3 compatible storage base_path str The base path to store states mlmodule.stores.github.GitHUBReleaseStore dataclass Store implementation leveraging GitHUB releases Model weights are stored as assets in a release . We recommend setting up GitHUB authentication to use the get_state_keys and the save methods. These methods are calling the releases API and are limited to 60 requests by hour unauthenticated. This can be done with: Personal access token : The PAT and username need to be set in the environment variable GH_API_BASIC_AUTH={username}:{personal_access_token} GitHUB Token : Used in GitHUB Actions, needs to be set in a GH_TOKEN environment variable. Model states are organised in releases with the following convention: Release name and tags are constructed as {release_name_prefix}.{state_type.backend}.{state_type.architecture} Asset names within a release are constructed as {state_type.extra1}.{state_type.extra2}.{training_id}.state.gzip Attributes: Name Type Description repository_owner str The owner of the GitHUB repository repository_name str The name of the repository to use as a store branch_name str The branch used to create new releases holding model state. By default we recommend using an orphan branch named model-store . release_name_prefix str The prefix to identify releases containing model weights. Defaults to state , should not contain a dot. Write your own store A store should inherit AbstractStateStore and implement the save , load and get_state_keys methods. mlmodule.stores.abstract.AbstractStateStore Interface to handle model state loading and saving See states reference for more information on state management. exists ( self , state_key : StateKey ) -> bool Tests whether the state key exists in the current store Parameters: Name Type Description Default state_key StateKey The state key to test required Returns: Type Description bool True if state key exists or False otherwise get_state_keys ( self , state_type : StateType ) -> List [ mlmodule . states . StateKey ] Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type ) load ( self , model : ~ _ModelType , state_key : StateKey ) -> None Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load save ( self , model : ~ _ModelType , training_id : str ) -> StateKey Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created For stores used to download states of a single model, it can be useful to subclass the AbstractListStateStore directly. This makes it easier to define a store from a fix set of states as it is often the case when integrating the weights from external sources (pre-trained states for a paper, hugging face...). See SBERTDistiluseBaseMultilingualCasedV2Store for an example. mlmodule.stores.list.AbstractListStateStore Helper to define a store from a fixed list of state keys. The subclasses should implement the following: available_state_keys state_downloader available_state_keys : List [ mlmodule . states . StateKey ] property readonly List of available state keys for this store Returns: Type Description list(StateKey) All available state keys in the store get_state_keys ( self , state_type : StateType ) -> List [ mlmodule . states . StateKey ] Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type ) load ( self , model : ~ _ModelType , state_key : StateKey ) -> None Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load save ( self , model : ~ _ModelType , training_id : str ) -> NoReturn Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created state_downloader ( self , model : ~ _ModelType , state_key : StateKey ) -> None Downloads and applies a state to a model Parameters: Name Type Description Default model _ModelType The model that will be used to load the state required state_key StateKey The state key identifier required","title":"Stores"},{"location":"references/stores/#stores","text":"Stores are used to load and save models state. They define 3 methods: get_state_keys , load and save . The get_state_keys method allows to list available pre-trained states for a given type of state. It usually called from a model . # List available model states in store for the model state type state_keys = store . get_state_keys ( model . state_type ) print ( state_keys ) # Prints for a ResNet pre-trained on ImageNet: # [ # StateKey( # state_type=StateType( # backend='pytorch', # architecture='resnet18', # extra=('cls1000',) # ), # training_id='imagenet' # ) # ] See states documentation for more information on how pre-trained states are identified. Then, a state can be loaded into the model . # Getting the first state key resnet_imagenet_state_key = state_keys [ 0 ] # Loading it into the model store . load ( model , state_key = resnet_imagenet_state_key ) A model can be saved by specifying a training_id which should uniquely identify the training activity that yielded this model's state. store . save ( model , training_id = \"2022-01-01-finetuned-imagenet\" ) See AbstractStateStore for more details on these methods.","title":"Stores"},{"location":"references/stores/#mlmodule-pre-trained-models","text":"MLModule provides model weights for all defined models through the MlModule Store .","title":"MLModule pre-trained models"},{"location":"references/stores/#mlmodule.stores.Store","text":"MlModule model state store. Examples: The store can be used to list available pre-trained states for a model store = Store () states = store . get_state_keys ( model . state_type ) And load a given state to a model store . load ( model , state_key = states [ 0 ])","title":"Store()"},{"location":"references/stores/#alternative-stores","text":"These stores can be used if you want to store model states locally or on S3 storage.","title":"Alternative stores"},{"location":"references/stores/#mlmodule.stores.local.LocalStateStore","text":"Local filebased store Attributes: Name Type Description folder str Path to the folder to save model's state","title":"LocalStateStore"},{"location":"references/stores/#mlmodule.stores.s3.S3StateStore","text":"State store on top of S3 object storage Given the state keys, states are organised in in folders with the following structure: base_path/ \u251c\u2500 { backend } / \u2502 \u251c\u2500 { architecture } . { extra1 } . { extra2 } . { training_id } .pt \u251c\u2500 pytorch/ # e.g. for torch models \u2502 \u251c\u2500 resnet18.cls1000.imagenet.pt \u2502 \u251c\u2500 clip-image-rn50.clip.pt Attributes: Name Type Description bucket str Bucket to use to store states session_kwargs dict Arguments passed to initialise boto3.session.Session s3_endpoint_url str To connect to S3 compatible storage base_path str The base path to store states","title":"S3StateStore"},{"location":"references/stores/#mlmodule.stores.github.GitHUBReleaseStore","text":"Store implementation leveraging GitHUB releases Model weights are stored as assets in a release . We recommend setting up GitHUB authentication to use the get_state_keys and the save methods. These methods are calling the releases API and are limited to 60 requests by hour unauthenticated. This can be done with: Personal access token : The PAT and username need to be set in the environment variable GH_API_BASIC_AUTH={username}:{personal_access_token} GitHUB Token : Used in GitHUB Actions, needs to be set in a GH_TOKEN environment variable. Model states are organised in releases with the following convention: Release name and tags are constructed as {release_name_prefix}.{state_type.backend}.{state_type.architecture} Asset names within a release are constructed as {state_type.extra1}.{state_type.extra2}.{training_id}.state.gzip Attributes: Name Type Description repository_owner str The owner of the GitHUB repository repository_name str The name of the repository to use as a store branch_name str The branch used to create new releases holding model state. By default we recommend using an orphan branch named model-store . release_name_prefix str The prefix to identify releases containing model weights. Defaults to state , should not contain a dot.","title":"GitHUBReleaseStore"},{"location":"references/stores/#write-your-own-store","text":"A store should inherit AbstractStateStore and implement the save , load and get_state_keys methods.","title":"Write your own store"},{"location":"references/stores/#mlmodule.stores.abstract.AbstractStateStore","text":"Interface to handle model state loading and saving See states reference for more information on state management.","title":"AbstractStateStore"},{"location":"references/stores/#mlmodule.stores.abstract.AbstractStateStore.exists","text":"Tests whether the state key exists in the current store Parameters: Name Type Description Default state_key StateKey The state key to test required Returns: Type Description bool True if state key exists or False otherwise","title":"exists()"},{"location":"references/stores/#mlmodule.stores.abstract.AbstractStateStore.get_state_keys","text":"Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type )","title":"get_state_keys()"},{"location":"references/stores/#mlmodule.stores.abstract.AbstractStateStore.load","text":"Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load","title":"load()"},{"location":"references/stores/#mlmodule.stores.abstract.AbstractStateStore.save","text":"Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created For stores used to download states of a single model, it can be useful to subclass the AbstractListStateStore directly. This makes it easier to define a store from a fix set of states as it is often the case when integrating the weights from external sources (pre-trained states for a paper, hugging face...). See SBERTDistiluseBaseMultilingualCasedV2Store for an example.","title":"save()"},{"location":"references/stores/#mlmodule.stores.list.AbstractListStateStore","text":"Helper to define a store from a fixed list of state keys. The subclasses should implement the following: available_state_keys state_downloader","title":"AbstractListStateStore"},{"location":"references/stores/#mlmodule.stores.list.AbstractListStateStore.available_state_keys","text":"List of available state keys for this store Returns: Type Description list(StateKey) All available state keys in the store","title":"available_state_keys"},{"location":"references/stores/#mlmodule.stores.list.AbstractListStateStore.get_state_keys","text":"Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type )","title":"get_state_keys()"},{"location":"references/stores/#mlmodule.stores.list.AbstractListStateStore.load","text":"Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load","title":"load()"},{"location":"references/stores/#mlmodule.stores.list.AbstractListStateStore.save","text":"Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created","title":"save()"},{"location":"references/stores/#mlmodule.stores.list.AbstractListStateStore.state_downloader","text":"Downloads and applies a state to a model Parameters: Name Type Description Default model _ModelType The model that will be used to load the state required state_key StateKey The state key identifier required","title":"state_downloader()"}]}