{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction MLModule is a library containing a collection of machine learning models with standardised interface to run inference, train and manage model state files. It aims at providing high-level abstractions called runners on top of inference and training loops while allowing extensions via callbacks . These callbacks control the way the output of a runner is handled (i.e. features, labels, model weights...). We also try to keep as few dependencies as possible. Meaning models will be mostly implemented from modules available in deep learning frameworks (such as PyTorch or torchvision ). Go ahead to the installation and getting started guides for an overview of MLModule. Features Model zoo PyTorch inference PyTorch training Multi-GPU support","title":"Introduction"},{"location":"#introduction","text":"MLModule is a library containing a collection of machine learning models with standardised interface to run inference, train and manage model state files. It aims at providing high-level abstractions called runners on top of inference and training loops while allowing extensions via callbacks . These callbacks control the way the output of a runner is handled (i.e. features, labels, model weights...). We also try to keep as few dependencies as possible. Meaning models will be mostly implemented from modules available in deep learning frameworks (such as PyTorch or torchvision ). Go ahead to the installation and getting started guides for an overview of MLModule.","title":"Introduction"},{"location":"#features","text":"Model zoo PyTorch inference PyTorch training Multi-GPU support","title":"Features"},{"location":"0-installation/","text":"Installation We support Python >= 3.7 and PyTorch >= 1.9 . However, it is likely that MLModule can be run on previous versions of PyTorch, we are simply not testing it for versions before 1.9 . Pre-requisites First, you need to install PyTorch and TorchVision. See the official PyTorch's documentation for installation instructions. We recommend using the conda package manager to install PyTorch. Installing with pip Once PyTorch and TorchVision are installed, MLModule can be directly installed from the git repository using pip . # To install the latest developments pip install git+https://github.com/LSIR/mlmodule # Or to install a specific version X.Y.Z pip install git+https://github.com/LSIR/mlmodule@vX.Y.Z Using Docker For convenience, we also ship a base Docker image which contains dependencies that can be hard to install (for instance PyTorch). See MLModuleKit for usage documentation.","title":"Installation"},{"location":"0-installation/#installation","text":"We support Python >= 3.7 and PyTorch >= 1.9 . However, it is likely that MLModule can be run on previous versions of PyTorch, we are simply not testing it for versions before 1.9 .","title":"Installation"},{"location":"0-installation/#pre-requisites","text":"First, you need to install PyTorch and TorchVision. See the official PyTorch's documentation for installation instructions. We recommend using the conda package manager to install PyTorch.","title":"Pre-requisites"},{"location":"0-installation/#installing-with-pip","text":"Once PyTorch and TorchVision are installed, MLModule can be directly installed from the git repository using pip . # To install the latest developments pip install git+https://github.com/LSIR/mlmodule # Or to install a specific version X.Y.Z pip install git+https://github.com/LSIR/mlmodule@vX.Y.Z","title":"Installing with pip"},{"location":"0-installation/#using-docker","text":"For convenience, we also ship a base Docker image which contains dependencies that can be hard to install (for instance PyTorch). See MLModuleKit for usage documentation.","title":"Using Docker"},{"location":"1-getting-started/","text":"Getting started Code-based usage This guide runs through the inference of a PyTorch ResNet model pretrained on imagenet. First, we need to create a dataset of images, for this we will be using the OpenFileDataset . from mlmodule.v2.torch.datasets import OpenImageFileDataset # Getting a dataset of images (1) dataset = OpenImageFileDataset ( paths = [ \"tests/fixtures/cats_dogs/cat_0.jpg\" , \"tests/fixtures/cats_dogs/cat_90.jpg\" ] ) See Datasets for a list of available datasets. Next, we need to load the ResNet PyTorch module specifying the resnet18 architecture. The model is initialised with weights provided by the MLModuleModelStore . from mlmodule.contrib.resnet import TorchResNetModule from mlmodule.v2.states import StateKey from mlmodule.v2.stores import MLModuleModelStore # Model definition (1) resnet = TorchResNetModule ( \"resnet18\" ) # Getting pre-trained model (2) store = MLModuleModelStore () # Getting the state pre-trained on ImageNet (3) store . load ( resnet , StateKey ( state_type = resnet . state_type , training_id = \"imagenet\" ) ) List of all models List of all stores Description of how states are handled is available is state's reference Once we have a model initialized, we need to define what we want to do with it. In this case, we'll run an inference loop using the TorchInferenceRunner . Note that we pass two callbacks to the runner: CollectFeaturesInMemory and CollectLabelsInMemory . They will be called to collect the resulting features and labels for each batch. from mlmodule.v2.helpers.callbacks import ( CollectFeaturesInMemory , CollectLabelsInMemory ) from mlmodule.v2.torch.options import TorchRunnerOptions from mlmodule.v2.torch.runners import TorchInferenceRunner # Creating the callback to collect data (1) features = CollectFeaturesInMemory () labels = CollectLabelsInMemory () # Getting the torch runner for inference (2) runner = TorchInferenceRunner ( model = resnet , dataset = dataset , callbacks = [ features , labels ], options = TorchRunnerOptions ( tqdm_enabled = True ), ) List of available callbacks . List of available runners When defined, the runner can be run and the callback objects will contain the features and labels. # Executing inference runner . run () # Printing the features print ( features . indices , features . features ) # Printing labels print ( labels . indices , labels . labels ) Command-line interface MLModule exposes a command-line interface. See python -m mlmodule -h for a list of available commands. For instance, one can run a ResNet model against a list of local images with the following command: python -m mlmodule run \".resnet.modules.TorchResNetModule(resnet18)\" *.jpg It prints the results (features and labels) in JSON format. Similarly, we can extract the key-frames from videos: python -m mlmodule run \".keyframes.selectors.resnet_key_frame_selector(resnet18, 10)\" *.mp4 --file-type vi --batch-size 1","title":"Getting started"},{"location":"1-getting-started/#getting-started","text":"","title":"Getting started"},{"location":"1-getting-started/#code-based-usage","text":"This guide runs through the inference of a PyTorch ResNet model pretrained on imagenet. First, we need to create a dataset of images, for this we will be using the OpenFileDataset . from mlmodule.v2.torch.datasets import OpenImageFileDataset # Getting a dataset of images (1) dataset = OpenImageFileDataset ( paths = [ \"tests/fixtures/cats_dogs/cat_0.jpg\" , \"tests/fixtures/cats_dogs/cat_90.jpg\" ] ) See Datasets for a list of available datasets. Next, we need to load the ResNet PyTorch module specifying the resnet18 architecture. The model is initialised with weights provided by the MLModuleModelStore . from mlmodule.contrib.resnet import TorchResNetModule from mlmodule.v2.states import StateKey from mlmodule.v2.stores import MLModuleModelStore # Model definition (1) resnet = TorchResNetModule ( \"resnet18\" ) # Getting pre-trained model (2) store = MLModuleModelStore () # Getting the state pre-trained on ImageNet (3) store . load ( resnet , StateKey ( state_type = resnet . state_type , training_id = \"imagenet\" ) ) List of all models List of all stores Description of how states are handled is available is state's reference Once we have a model initialized, we need to define what we want to do with it. In this case, we'll run an inference loop using the TorchInferenceRunner . Note that we pass two callbacks to the runner: CollectFeaturesInMemory and CollectLabelsInMemory . They will be called to collect the resulting features and labels for each batch. from mlmodule.v2.helpers.callbacks import ( CollectFeaturesInMemory , CollectLabelsInMemory ) from mlmodule.v2.torch.options import TorchRunnerOptions from mlmodule.v2.torch.runners import TorchInferenceRunner # Creating the callback to collect data (1) features = CollectFeaturesInMemory () labels = CollectLabelsInMemory () # Getting the torch runner for inference (2) runner = TorchInferenceRunner ( model = resnet , dataset = dataset , callbacks = [ features , labels ], options = TorchRunnerOptions ( tqdm_enabled = True ), ) List of available callbacks . List of available runners When defined, the runner can be run and the callback objects will contain the features and labels. # Executing inference runner . run () # Printing the features print ( features . indices , features . features ) # Printing labels print ( labels . indices , labels . labels )","title":"Code-based usage"},{"location":"1-getting-started/#command-line-interface","text":"MLModule exposes a command-line interface. See python -m mlmodule -h for a list of available commands. For instance, one can run a ResNet model against a list of local images with the following command: python -m mlmodule run \".resnet.modules.TorchResNetModule(resnet18)\" *.jpg It prints the results (features and labels) in JSON format. Similarly, we can extract the key-frames from videos: python -m mlmodule run \".keyframes.selectors.resnet_key_frame_selector(resnet18, 10)\" *.mp4 --file-type vi --batch-size 1","title":"Command-line interface"},{"location":"2-develop/","text":"Developer guide Installation This section covers the installation of a development environment for contributing to MLModule. If you want to use MLModule in your project, see installation instructions instead. We are developing on Python 3.7 and PyTorch 1.9. Using tox (recommended) This method is the recommended method as it will install a complete environment with PyTorch. It requires conda and tox to be installed. Create a development environment: # CPU development tox --devenv venv -e py37-pt19 # or CUDA 11.1 tox --devenv venv -e py37-pt19-cuda The environment can be activated with: conda activate ./venv Using pip This method requires you to install PyTorch and TorchVision (see PyTorch documentation ). pip install -r requirements.txt # To install MLModule in development mode with all dependencies pip install -e . Testing Testing can be done using tox : # CPU testing tox -e py37-pt19 # or with GPU support tox -e py37-pt19-cuda or with directly using pytest on an environment with all dependencies installed pytest Code quality We use black as formatter. Install pre commit hooks for systematic styling on commits: pip install pre-commit pre-commit install Requirements Updating requirements should be done in setup.cfg . To update the requirement.txt file run: pip-compile --extra full --upgrade Publish a new version Push the new version to the master branch Test the release on all supported Python / PyTorch versions with the command. tox -e \"{py37,py38,py39}{-pt19,-pt110}{-cuda,}\" Create a GitHUB release on the branch master with format vX.Y.Z . For instance, v0.1.1 . Upload new model weights Note You will need to set the GH_TOKEN environment variable to the token given by gh auth status -t of the GitHUB CLI . Add the model and provider stores to the scripts/update_public_store.py . They should be added in a function that returns a list of tuple with model and provider store. Update the get_all_models_stores to iterate over your new function. Run the update_public_store scripts. This script accepts a --dry-run argument to see changes before actually uploading models. Update documentation Install requirements to build the documentation: pip install -r docs/requirements.txt We are using mkdocs-material . It allows for previewing the changes with hot-reload: mkdocs serve The documentation can then be deployed to GitHUB pages manually with: mkdocs gh-deploy Or automatically when merging a Pull Request into the main branch with GitHUB actions.","title":"Developer guide"},{"location":"2-develop/#developer-guide","text":"","title":"Developer guide"},{"location":"2-develop/#installation","text":"This section covers the installation of a development environment for contributing to MLModule. If you want to use MLModule in your project, see installation instructions instead. We are developing on Python 3.7 and PyTorch 1.9.","title":"Installation"},{"location":"2-develop/#using-tox-recommended","text":"This method is the recommended method as it will install a complete environment with PyTorch. It requires conda and tox to be installed. Create a development environment: # CPU development tox --devenv venv -e py37-pt19 # or CUDA 11.1 tox --devenv venv -e py37-pt19-cuda The environment can be activated with: conda activate ./venv","title":"Using tox (recommended)"},{"location":"2-develop/#using-pip","text":"This method requires you to install PyTorch and TorchVision (see PyTorch documentation ). pip install -r requirements.txt # To install MLModule in development mode with all dependencies pip install -e .","title":"Using pip"},{"location":"2-develop/#testing","text":"Testing can be done using tox : # CPU testing tox -e py37-pt19 # or with GPU support tox -e py37-pt19-cuda or with directly using pytest on an environment with all dependencies installed pytest","title":"Testing"},{"location":"2-develop/#code-quality","text":"We use black as formatter. Install pre commit hooks for systematic styling on commits: pip install pre-commit pre-commit install","title":"Code quality"},{"location":"2-develop/#requirements","text":"Updating requirements should be done in setup.cfg . To update the requirement.txt file run: pip-compile --extra full --upgrade","title":"Requirements"},{"location":"2-develop/#publish-a-new-version","text":"Push the new version to the master branch Test the release on all supported Python / PyTorch versions with the command. tox -e \"{py37,py38,py39}{-pt19,-pt110}{-cuda,}\" Create a GitHUB release on the branch master with format vX.Y.Z . For instance, v0.1.1 .","title":"Publish a new version"},{"location":"2-develop/#upload-new-model-weights","text":"Note You will need to set the GH_TOKEN environment variable to the token given by gh auth status -t of the GitHUB CLI . Add the model and provider stores to the scripts/update_public_store.py . They should be added in a function that returns a list of tuple with model and provider store. Update the get_all_models_stores to iterate over your new function. Run the update_public_store scripts. This script accepts a --dry-run argument to see changes before actually uploading models.","title":"Upload new model weights"},{"location":"2-develop/#update-documentation","text":"Install requirements to build the documentation: pip install -r docs/requirements.txt We are using mkdocs-material . It allows for previewing the changes with hot-reload: mkdocs serve The documentation can then be deployed to GitHUB pages manually with: mkdocs gh-deploy Or automatically when merging a Pull Request into the main branch with GitHUB actions.","title":"Update documentation"},{"location":"3-mlmodulekit/","text":"MLModule Kit MLModuleKit is a collection of docker images with a pre-configured environment to use MLModule. You can pull the image for a specific <version> with: docker pull lsirepfl/mlmodulekit:<version> Usage This example will go through the process to run a python script called main.py in the MLModuleKit docker image. As a first step, we need to create a Dockerfile that uses mlmodulekit, installs the latest version of MLModule and copies the script we want to run. FROM lsirepfl/mlmodulekit:3 WORKDIR /app RUN pip install git+https://github.com/LSIR/mlmodule ADD main.py . ENTRYPOINT [ \"conda\" , \"run\" , \"-n\" , \"app\" , \"--no-capture-output\" ] Then, we need to build a docker image from the Dockerfile : docker build . -t my-mlmodule-job The previous command has created a docker container that we can run with the following command: docker run my-mlmodule-job python main.py That's it ! You should see you script output in the terminal. Available versions Description of available image tags. 3 Python 3.7 CUDA 11.1 PyTorch 1.9.1 TorchVision 0.10.1","title":"MLModule Kit"},{"location":"3-mlmodulekit/#mlmodule-kit","text":"MLModuleKit is a collection of docker images with a pre-configured environment to use MLModule. You can pull the image for a specific <version> with: docker pull lsirepfl/mlmodulekit:<version>","title":"MLModule Kit"},{"location":"3-mlmodulekit/#usage","text":"This example will go through the process to run a python script called main.py in the MLModuleKit docker image. As a first step, we need to create a Dockerfile that uses mlmodulekit, installs the latest version of MLModule and copies the script we want to run. FROM lsirepfl/mlmodulekit:3 WORKDIR /app RUN pip install git+https://github.com/LSIR/mlmodule ADD main.py . ENTRYPOINT [ \"conda\" , \"run\" , \"-n\" , \"app\" , \"--no-capture-output\" ] Then, we need to build a docker image from the Dockerfile : docker build . -t my-mlmodule-job The previous command has created a docker container that we can run with the following command: docker run my-mlmodule-job python main.py That's it ! You should see you script output in the terminal.","title":"Usage"},{"location":"3-mlmodulekit/#available-versions","text":"Description of available image tags.","title":"Available versions"},{"location":"3-mlmodulekit/#3","text":"Python 3.7 CUDA 11.1 PyTorch 1.9.1 TorchVision 0.10.1","title":"3"},{"location":"models/","text":"Models Tip See the the menu on the left for a list of available models The sections below will help with defining a new model. Each section contains the functions a MLModule model class should define to implement a feature. Models state management A model with internal state (weights) should at least implement the ModelWithState protocol. mlmodule.v2.base.models.ModelWithState Protocol of a model with internal state (weights) It defines two functions set_state and get_state . Attributes: Name Type Description state_type StateType Type of the model state, see states for more information. state_type : StateType property readonly Type of the model state See states for more information. get_state ( self ) -> bytes Get the model internal state Returns: Type Description bytes Serialised state as bytes set_state ( self , state : bytes ) -> None Set the model internal state Parameters: Name Type Description Default state bytes Serialised state as bytes required Labels When a model returns label scores, it must define a LabelSet . This should be defined by implementing the ModelWithLabels protocol. mlmodule.v2.base.models.ModelWithLabels Model that predicts scores for labels It defines the get_labels function get_labels ( self ) -> LabelSet Getter for the model's LabelSet Returns: Type Description LabelSet The label set corresponding to returned label scores PyTorch models PyTorch models should be a subclass of TorchMlModule . Note PyTorch models already implement the ModelWithState protocol by default. mlmodule.v2.torch.modules.TorchMlModule Base torch.nn.Module for PyTorch models implemented in MLModule. A valid subclass of TorchMlModule must implement the following method: forward to_predictions state_type And can optionally implement: get_dataset_transforms get_dataloader_collate_fn Attributes: Name Type Description device torch.device Mandatory PyTorch device attribute to initialise model. is_trainable bool Flag which indicates if the model is trainable. Default, True. Examples: This would define a simple PyTorch model consisting of fully connected layer. from mlmodule.v2.states import StateType from mlmodule.v2.torch.modules import TorchMlModule from torchvision import transforms class FC ( TorchMlModule [ torch . Tensor , torch . Tensor ]): def __init__ ( self , device : torch . device = torch . device ( \"cpu\" )): super () . __init__ ( device = device ) self . fc = nn . Linear ( 512 , 512 ) def forward ( self , batch : torch . Tensor ) -> torch . Tensor : return self . fc ( batch ) def to_predictions ( self , forward_output : torch . Tensor ) -> BatchModelPrediction [ torch . Tensor ]: return BatchModelPrediction ( features = forward_output ) @property def state_type ( self ) -> StateType : return StateType ( backend = \"pytorch\" , architecture = \"fc512x512\" , ) def get_dataset_transforms ( self ) -> List [ Callable ]: return [ transforms . ToTensor ()] Note This is a generic class taking a _BatchType and _ForwardOutputType type argument. This corresponds respectively to the type of data the forward will take as argument and return. It is most likely torch.Tensor Note By default, MLModule models are trainable. Set the is_trainable parameter to False when creating a subclass if it shouldn't be trained. state_type : StateType property readonly Identifier for the current's model state architecture Note PyTorch's model architecture should have the pytorch backend Returns: Type Description StateType State architecture object Note This method must be implemented in subclasses forward ( self , batch : ~ _BatchType ) -> ~ _ForwardOutputType Forward pass of the model Applies the module on a batch and returns all potentially interesting data point (features, labels...) Parameters: Name Type Description Default batch _BatchType the batch of data to process required Returns: Type Description _ForwardOutputType A tensor or a sequence of tensor with relevant information (features, labels, bounding boxes...) Note This method must be implemented in subclasses get_dataloader_collate_fn ( self ) -> Optional [ Callable [[ Any ], Any ]] Optionally returns a collate function to be passed to the data loader Note This collate function will be wrapped in mlmodule.v2.torch.collate.TorchMlModuleCollateFn . This means that the first argument batch will not contain the indices of the dataset but only the data element. Returns: Type Description Callable[[Any], Any] | None The collate function to be passed to TorchMlModuleCollateFn . get_dataset_transforms ( self ) -> List [ Callable ] Transforms to apply to the input dataset . Note By default, this method returns an empty list (meaning no transformation) but in most cases, this will need to be overridden. Returns: Type Description List[Callable] A list of callables that will be used to transform the input data. to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) to_predictions ( self , forward_output : ~ _ForwardOutputType ) -> mlmodule . v2 . base . predictions . BatchModelPrediction [ torch . Tensor ] Modifies the output of the forward pass to create the standard BatchModelPrediction object Parameters: Name Type Description Default forward_output _ForwardOutputType the batch of data to process required Returns: Type Description BatchModelPrediction[torch.Tensor] Prediction object with the keys features , label_scores ... Note This method must be implemented in subclasses","title":"Models"},{"location":"models/#models","text":"Tip See the the menu on the left for a list of available models The sections below will help with defining a new model. Each section contains the functions a MLModule model class should define to implement a feature.","title":"Models"},{"location":"models/#models-state-management","text":"A model with internal state (weights) should at least implement the ModelWithState protocol.","title":"Models state management"},{"location":"models/#mlmodule.v2.base.models.ModelWithState","text":"Protocol of a model with internal state (weights) It defines two functions set_state and get_state . Attributes: Name Type Description state_type StateType Type of the model state, see states for more information.","title":"ModelWithState"},{"location":"models/#mlmodule.v2.base.models.ModelWithState.state_type","text":"Type of the model state See states for more information.","title":"state_type"},{"location":"models/#mlmodule.v2.base.models.ModelWithState.get_state","text":"Get the model internal state Returns: Type Description bytes Serialised state as bytes","title":"get_state()"},{"location":"models/#mlmodule.v2.base.models.ModelWithState.set_state","text":"Set the model internal state Parameters: Name Type Description Default state bytes Serialised state as bytes required","title":"set_state()"},{"location":"models/#labels","text":"When a model returns label scores, it must define a LabelSet . This should be defined by implementing the ModelWithLabels protocol.","title":"Labels"},{"location":"models/#mlmodule.v2.base.models.ModelWithLabels","text":"Model that predicts scores for labels It defines the get_labels function","title":"ModelWithLabels"},{"location":"models/#mlmodule.v2.base.models.ModelWithLabels.get_labels","text":"Getter for the model's LabelSet Returns: Type Description LabelSet The label set corresponding to returned label scores","title":"get_labels()"},{"location":"models/#pytorch-models","text":"PyTorch models should be a subclass of TorchMlModule . Note PyTorch models already implement the ModelWithState protocol by default.","title":"PyTorch models"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule","text":"Base torch.nn.Module for PyTorch models implemented in MLModule. A valid subclass of TorchMlModule must implement the following method: forward to_predictions state_type And can optionally implement: get_dataset_transforms get_dataloader_collate_fn Attributes: Name Type Description device torch.device Mandatory PyTorch device attribute to initialise model. is_trainable bool Flag which indicates if the model is trainable. Default, True. Examples: This would define a simple PyTorch model consisting of fully connected layer. from mlmodule.v2.states import StateType from mlmodule.v2.torch.modules import TorchMlModule from torchvision import transforms class FC ( TorchMlModule [ torch . Tensor , torch . Tensor ]): def __init__ ( self , device : torch . device = torch . device ( \"cpu\" )): super () . __init__ ( device = device ) self . fc = nn . Linear ( 512 , 512 ) def forward ( self , batch : torch . Tensor ) -> torch . Tensor : return self . fc ( batch ) def to_predictions ( self , forward_output : torch . Tensor ) -> BatchModelPrediction [ torch . Tensor ]: return BatchModelPrediction ( features = forward_output ) @property def state_type ( self ) -> StateType : return StateType ( backend = \"pytorch\" , architecture = \"fc512x512\" , ) def get_dataset_transforms ( self ) -> List [ Callable ]: return [ transforms . ToTensor ()] Note This is a generic class taking a _BatchType and _ForwardOutputType type argument. This corresponds respectively to the type of data the forward will take as argument and return. It is most likely torch.Tensor Note By default, MLModule models are trainable. Set the is_trainable parameter to False when creating a subclass if it shouldn't be trained.","title":"TorchMlModule"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.state_type","text":"Identifier for the current's model state architecture Note PyTorch's model architecture should have the pytorch backend Returns: Type Description StateType State architecture object Note This method must be implemented in subclasses","title":"state_type"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.forward","text":"Forward pass of the model Applies the module on a batch and returns all potentially interesting data point (features, labels...) Parameters: Name Type Description Default batch _BatchType the batch of data to process required Returns: Type Description _ForwardOutputType A tensor or a sequence of tensor with relevant information (features, labels, bounding boxes...) Note This method must be implemented in subclasses","title":"forward()"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.get_dataloader_collate_fn","text":"Optionally returns a collate function to be passed to the data loader Note This collate function will be wrapped in mlmodule.v2.torch.collate.TorchMlModuleCollateFn . This means that the first argument batch will not contain the indices of the dataset but only the data element. Returns: Type Description Callable[[Any], Any] | None The collate function to be passed to TorchMlModuleCollateFn .","title":"get_dataloader_collate_fn()"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.get_dataset_transforms","text":"Transforms to apply to the input dataset . Note By default, this method returns an empty list (meaning no transformation) but in most cases, this will need to be overridden. Returns: Type Description List[Callable] A list of callables that will be used to transform the input data.","title":"get_dataset_transforms()"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.to","text":"Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device class: torch.device ): the desired device of the parameters and buffers in this module required dtype class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module required tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module required memory_format class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) required Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)","title":"to()"},{"location":"models/#mlmodule.v2.torch.modules.TorchMlModule.to_predictions","text":"Modifies the output of the forward pass to create the standard BatchModelPrediction object Parameters: Name Type Description Default forward_output _ForwardOutputType the batch of data to process required Returns: Type Description BatchModelPrediction[torch.Tensor] Prediction object with the keys features , label_scores ... Note This method must be implemented in subclasses","title":"to_predictions()"},{"location":"models/ArcFace/","text":"ArcFace Implementation of ArcFace 1 in PyTorch by InsightFace . Models The MagFace model is an implementation of a TorchMlModule . mlmodule.contrib.arcface.modules.TorchArcFaceModule Creates face embeddings from MTCNN output Attributes: Name Type Description device torch.device Torch device to initialise the model weights remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . bad_faces_threshold float The cosine similarity distance to reference faces for which we consider the face is of bad quality. Provider store See the stores documentation for usage. mlmodule.contrib.arcface.stores.ArcFaceStore Gets the pretrained state dir from OneDrive URL: https://github.com/TreB1eN/InsightFace_Pytorch#2-pretrained-models--performance Model: IR-SE50 Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: additive angular margin loss for deep face recognition. In CVPR . 2019. \u21a9","title":"ArcFace"},{"location":"models/ArcFace/#arcface","text":"Implementation of ArcFace 1 in PyTorch by InsightFace .","title":"ArcFace"},{"location":"models/ArcFace/#models","text":"The MagFace model is an implementation of a TorchMlModule .","title":"Models"},{"location":"models/ArcFace/#mlmodule.contrib.arcface.modules.TorchArcFaceModule","text":"Creates face embeddings from MTCNN output Attributes: Name Type Description device torch.device Torch device to initialise the model weights remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . bad_faces_threshold float The cosine similarity distance to reference faces for which we consider the face is of bad quality.","title":"TorchArcFaceModule"},{"location":"models/ArcFace/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/ArcFace/#mlmodule.contrib.arcface.stores.ArcFaceStore","text":"Gets the pretrained state dir from OneDrive URL: https://github.com/TreB1eN/InsightFace_Pytorch#2-pretrained-models--performance Model: IR-SE50 Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: additive angular margin loss for deep face recognition. In CVPR . 2019. \u21a9","title":"ArcFaceStore"},{"location":"models/CLIP/","text":"CLIP See OpenAI/CLIP for the source code and original models. This model has extra requirements: pip install git+ssh://git@github.com/LSIR/mlmodule.git#egg = mlmodule [ torch,clip ] # or pip install mlmodule [ torch,clip ] Models CLIP comes with image ( CLIPImageModule ) and a text ( CLIPTextModule ) encoders. These modules are an implementation of TorchMlModule . mlmodule.contrib.clip.image.CLIPImageModule Image encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . mlmodule.contrib.clip.text.CLIPTextModule Text encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . Pre-trained states from CLIP See the stores documentation for usage. mlmodule.contrib.clip.stores.CLIPStore Pre-trained model states by OpenAI CLIP These are identified by training_id=clip . List models and parameters There is a command line utility to list all available models from CLIP with their associated parameters in JSON format: python -m mlmodule.contrib.clip.list The output is used to fill the mlmodule.contrib.clip.parameters file.","title":"CLIP"},{"location":"models/CLIP/#clip","text":"See OpenAI/CLIP for the source code and original models. This model has extra requirements: pip install git+ssh://git@github.com/LSIR/mlmodule.git#egg = mlmodule [ torch,clip ] # or pip install mlmodule [ torch,clip ]","title":"CLIP"},{"location":"models/CLIP/#models","text":"CLIP comes with image ( CLIPImageModule ) and a text ( CLIPTextModule ) encoders. These modules are an implementation of TorchMlModule .","title":"Models"},{"location":"models/CLIP/#mlmodule.contrib.clip.image.CLIPImageModule","text":"Image encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"CLIPImageModule"},{"location":"models/CLIP/#mlmodule.contrib.clip.text.CLIPTextModule","text":"Text encoder of the CLIP model Attributes: Name Type Description clip_model_name str Name of the model to load (see CLIP doc ) device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"CLIPTextModule"},{"location":"models/CLIP/#pre-trained-states-from-clip","text":"See the stores documentation for usage.","title":"Pre-trained states from CLIP"},{"location":"models/CLIP/#mlmodule.contrib.clip.stores.CLIPStore","text":"Pre-trained model states by OpenAI CLIP These are identified by training_id=clip .","title":"CLIPStore"},{"location":"models/CLIP/#list-models-and-parameters","text":"There is a command line utility to list all available models from CLIP with their associated parameters in JSON format: python -m mlmodule.contrib.clip.list The output is used to fill the mlmodule.contrib.clip.parameters file.","title":"List models and parameters"},{"location":"models/Classification/","text":"Classification Path: mlmodule.contrib.classification This module contains torch modules that can be used as classification heads on top of features. Models mlmodule.contrib.classification.modules.LinearClassifierTorchModule Linear classifier Attributes: Name Type Description in_features int Number of dimensions in the input label_set LabelSet The set of labels for this classifier mlmodule.contrib.classification.modules.MLPClassifierTorchModule Multi-layer perceptron classifier Attributes: Name Type Description in_features int The number of dimensions in the input hidden_layers Sequence[int] A sequence of width for the hidden layers label_set LabelSet The set of labels for this classifier activation str | None Name of an activation function in the torch.nn module. For instance ReLU . If None , no activation function is used.","title":"Classification"},{"location":"models/Classification/#classification","text":"Path: mlmodule.contrib.classification This module contains torch modules that can be used as classification heads on top of features.","title":"Classification"},{"location":"models/Classification/#models","text":"","title":"Models"},{"location":"models/Classification/#mlmodule.contrib.classification.modules.LinearClassifierTorchModule","text":"Linear classifier Attributes: Name Type Description in_features int Number of dimensions in the input label_set LabelSet The set of labels for this classifier","title":"LinearClassifierTorchModule"},{"location":"models/Classification/#mlmodule.contrib.classification.modules.MLPClassifierTorchModule","text":"Multi-layer perceptron classifier Attributes: Name Type Description in_features int The number of dimensions in the input hidden_layers Sequence[int] A sequence of width for the hidden layers label_set LabelSet The set of labels for this classifier activation str | None Name of an activation function in the torch.nn module. For instance ReLU . If None , no activation function is used.","title":"MLPClassifierTorchModule"},{"location":"models/DenseNet/","text":"DenseNet PyTorch implementation of DenseNet architecture. Model The DenseNet model is an implementation of a TorchMlModule . mlmodule.contrib.densenet.modules.TorchDenseNetModule PyTorch implementation of DenseNet See TorchVision source code . Attributes: Name Type Description densenet_arch DenseNetArch Identifier for the DenseNet architecture. Must be one of: - densenet121 - densenet161 - densenet169 - densenet201 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights Pre-trained state origins See the stores documentation for usage. mlmodule.contrib.densenet.stores.DenseNetTorchVisionStore Model store to load DenseNet weights pretrained on ImageNet from TorchVision mlmodule.contrib.densenet.stores.DenseNetPlaces365Store Model store to load DenseNet weights pretrained on Places365 See places365 documentation for more info.","title":"DenseNet"},{"location":"models/DenseNet/#densenet","text":"PyTorch implementation of DenseNet architecture.","title":"DenseNet"},{"location":"models/DenseNet/#model","text":"The DenseNet model is an implementation of a TorchMlModule .","title":"Model"},{"location":"models/DenseNet/#mlmodule.contrib.densenet.modules.TorchDenseNetModule","text":"PyTorch implementation of DenseNet See TorchVision source code . Attributes: Name Type Description densenet_arch DenseNetArch Identifier for the DenseNet architecture. Must be one of: - densenet121 - densenet161 - densenet169 - densenet201 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights","title":"TorchDenseNetModule"},{"location":"models/DenseNet/#pre-trained-state-origins","text":"See the stores documentation for usage.","title":"Pre-trained state origins"},{"location":"models/DenseNet/#mlmodule.contrib.densenet.stores.DenseNetTorchVisionStore","text":"Model store to load DenseNet weights pretrained on ImageNet from TorchVision","title":"DenseNetTorchVisionStore"},{"location":"models/DenseNet/#mlmodule.contrib.densenet.stores.DenseNetPlaces365Store","text":"Model store to load DenseNet weights pretrained on Places365 See places365 documentation for more info.","title":"DenseNetPlaces365Store"},{"location":"models/DistiluseMultilingual/","text":"Distiluse Multilingual Model This model is an implementation of a TorchMlModule . mlmodule.contrib.sentences.distilbert.modules.DistilUseBaseMultilingualCasedV2Module Multilingual model for semantic similarity See distiluse-base-multilingual-cased-v2 and sbert documentation for more information. Pre-trained state origins See the stores documentation for usage. mlmodule.contrib.sentences.distilbert.stores.SBERTDistiluseBaseMultilingualCasedV2Store Loads weights from SBERT's hugging face See Hugging face's documentation .","title":"Distiluse Multilingual"},{"location":"models/DistiluseMultilingual/#distiluse-multilingual","text":"","title":"Distiluse Multilingual"},{"location":"models/DistiluseMultilingual/#model","text":"This model is an implementation of a TorchMlModule .","title":"Model"},{"location":"models/DistiluseMultilingual/#mlmodule.contrib.sentences.distilbert.modules.DistilUseBaseMultilingualCasedV2Module","text":"Multilingual model for semantic similarity See distiluse-base-multilingual-cased-v2 and sbert documentation for more information.","title":"DistilUseBaseMultilingualCasedV2Module"},{"location":"models/DistiluseMultilingual/#pre-trained-state-origins","text":"See the stores documentation for usage.","title":"Pre-trained state origins"},{"location":"models/DistiluseMultilingual/#mlmodule.contrib.sentences.distilbert.stores.SBERTDistiluseBaseMultilingualCasedV2Store","text":"Loads weights from SBERT's hugging face See Hugging face's documentation .","title":"SBERTDistiluseBaseMultilingualCasedV2Store"},{"location":"models/MTCNN/","text":"MTCNN We are using facenet-pytorch to load pre-trained MTCNN model, see https://github.com/timesler/facenet-pytorch . Requirements This needs mlmodule with the mtcnn and torch extra requirements: pip install git+ssh://git@github.com/LSIR/mlmodule.git#egg = mlmodule [ torch,mtcnn ] # or pip install mlmodule [ torch,mtcnn ] Model The MTCNN model is an implementation of a TorchMlModule . mlmodule.contrib.mtcnn.modules.TorchMTCNNModule MTCNN face detection module Attributes: Name Type Description thresholds Tuple[float, float, float] MTCNN threshold hyperparameters image_size Tuple[int, int] Image size after pre-preprocessing min_face_size int Minimum face size in pixels device torch.device Torch device to initialise the model weights Provider store See the stores documentation for usage. mlmodule.contrib.mtcnn.stores.FaceNetMTCNNStore Pre-trained model states by Facenet for MTCNN These are identified by training_id=facenet .","title":"MTCNN"},{"location":"models/MTCNN/#mtcnn","text":"We are using facenet-pytorch to load pre-trained MTCNN model, see https://github.com/timesler/facenet-pytorch .","title":"MTCNN"},{"location":"models/MTCNN/#requirements","text":"This needs mlmodule with the mtcnn and torch extra requirements: pip install git+ssh://git@github.com/LSIR/mlmodule.git#egg = mlmodule [ torch,mtcnn ] # or pip install mlmodule [ torch,mtcnn ]","title":"Requirements"},{"location":"models/MTCNN/#model","text":"The MTCNN model is an implementation of a TorchMlModule .","title":"Model"},{"location":"models/MTCNN/#mlmodule.contrib.mtcnn.modules.TorchMTCNNModule","text":"MTCNN face detection module Attributes: Name Type Description thresholds Tuple[float, float, float] MTCNN threshold hyperparameters image_size Tuple[int, int] Image size after pre-preprocessing min_face_size int Minimum face size in pixels device torch.device Torch device to initialise the model weights","title":"TorchMTCNNModule"},{"location":"models/MTCNN/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/MTCNN/#mlmodule.contrib.mtcnn.stores.FaceNetMTCNNStore","text":"Pre-trained model states by Facenet for MTCNN These are identified by training_id=facenet .","title":"FaceNetMTCNNStore"},{"location":"models/MagFace/","text":"MagFace We are using the official implementation of MagFace in Pytorch (https://github.com/IrvingMeng/MagFace) Models The MagFace model is an implementation of a TorchMlModule . mlmodule.contrib.magface.modules.TorchMagFaceModule MagFace face embeddings from MTCNN detected faces The input dataset should return a tuple of image data and bounding box information Attributes: Name Type Description device torch.device Torch device to initialise the model weights remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . magnitude_threshold float Threshold to remove bad quality faces. The higher the stricter. Defaults to 22.5 . Provider store See the stores documentation for usage. mlmodule.contrib.magface.stores.MagFaceStore Pre-trained model states by MagFace ( https://github.com/IrvingMeng/MagFace ) These are identified by training_id=magface .","title":"MagFace"},{"location":"models/MagFace/#magface","text":"We are using the official implementation of MagFace in Pytorch (https://github.com/IrvingMeng/MagFace)","title":"MagFace"},{"location":"models/MagFace/#models","text":"The MagFace model is an implementation of a TorchMlModule .","title":"Models"},{"location":"models/MagFace/#mlmodule.contrib.magface.modules.TorchMagFaceModule","text":"MagFace face embeddings from MTCNN detected faces The input dataset should return a tuple of image data and bounding box information Attributes: Name Type Description device torch.device Torch device to initialise the model weights remove_bad_faces bool Whether to remove the faces with bad quality from the output. This will replace features of bad faces with float(\"nan\") . Defaults to False . magnitude_threshold float Threshold to remove bad quality faces. The higher the stricter. Defaults to 22.5 .","title":"TorchMagFaceModule"},{"location":"models/MagFace/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/MagFace/#mlmodule.contrib.magface.stores.MagFaceStore","text":"Pre-trained model states by MagFace ( https://github.com/IrvingMeng/MagFace ) These are identified by training_id=magface .","title":"MagFaceStore"},{"location":"models/ResNet/","text":"ResNet PyTorch implementation of ResNet as defined in Deep Residual Learning for Image Recognition and Torchvision . Model The ResNet model is an implementation of a TorchMlModule . mlmodule.contrib.resnet.modules.TorchResNetModule PyTorch ResNet architecture. See PyTorch's documentation . Attributes: Name Type Description resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights Pre-trained state origins See the stores documentation for usage. mlmodule.contrib.resnet.stores.ResNetTorchVisionStore Model store to load ResNet weights pretrained on ImageNet from TorchVision","title":"ResNet"},{"location":"models/ResNet/#resnet","text":"PyTorch implementation of ResNet as defined in Deep Residual Learning for Image Recognition and Torchvision .","title":"ResNet"},{"location":"models/ResNet/#model","text":"The ResNet model is an implementation of a TorchMlModule .","title":"Model"},{"location":"models/ResNet/#mlmodule.contrib.resnet.modules.TorchResNetModule","text":"PyTorch ResNet architecture. See PyTorch's documentation . Attributes: Name Type Description resnet_arch ResNetArchs Identifier for the ResNet architecture to load. Must be one of: resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 label_set LabelSet The output labels. Defaults to ImageNet 1000 labels. device torch.device Torch device to initialise the model weights","title":"TorchResNetModule"},{"location":"models/ResNet/#pre-trained-state-origins","text":"See the stores documentation for usage.","title":"Pre-trained state origins"},{"location":"models/ResNet/#mlmodule.contrib.resnet.stores.ResNetTorchVisionStore","text":"Model store to load ResNet weights pretrained on ImageNet from TorchVision","title":"ResNetTorchVisionStore"},{"location":"models/VinVL/","text":"VinVL Pre-trained large-scale object-attribute detection (OD) model is based on the ResNeXt-152 C4 architecture. The OD model has been firstly trained on much larger amounts of data, combining multiple public object detection datasets, including COCO , OpenImages (OI) , Objects365 , and Visual Genome (VG) . Then it is fine-tuned on VG dataset alone, since VG is the only dataset with label attributes (see issue #120 ). It predicts objects from 1594 classes with attributes from 524 classes. See the code and the paper for details. Requirements The model needs the configuration system YACS to be installed. pip install yacs == 0 .1.8 Models The VinVL model is an implementation of a TorchMlModule . mlmodule.contrib.vinvl.modules.TorchVinVLDetectorModule VinVL object detection model Attributes: Name Type Description score_threshold float attr_score_threshold float device torch.device PyTorch device attribute to initialise model. Provider store See the stores documentation for usage. mlmodule.contrib.vinvl.stores.VinVLStore Pre-trained model states for VinVL These are identified by training_id=vinvl .","title":"VinVL"},{"location":"models/VinVL/#vinvl","text":"Pre-trained large-scale object-attribute detection (OD) model is based on the ResNeXt-152 C4 architecture. The OD model has been firstly trained on much larger amounts of data, combining multiple public object detection datasets, including COCO , OpenImages (OI) , Objects365 , and Visual Genome (VG) . Then it is fine-tuned on VG dataset alone, since VG is the only dataset with label attributes (see issue #120 ). It predicts objects from 1594 classes with attributes from 524 classes. See the code and the paper for details.","title":"VinVL"},{"location":"models/VinVL/#requirements","text":"The model needs the configuration system YACS to be installed. pip install yacs == 0 .1.8","title":"Requirements"},{"location":"models/VinVL/#models","text":"The VinVL model is an implementation of a TorchMlModule .","title":"Models"},{"location":"models/VinVL/#mlmodule.contrib.vinvl.modules.TorchVinVLDetectorModule","text":"VinVL object detection model Attributes: Name Type Description score_threshold float attr_score_threshold float device torch.device PyTorch device attribute to initialise model.","title":"TorchVinVLDetectorModule"},{"location":"models/VinVL/#provider-store","text":"See the stores documentation for usage.","title":"Provider store"},{"location":"models/VinVL/#mlmodule.contrib.vinvl.stores.VinVLStore","text":"Pre-trained model states for VinVL These are identified by training_id=vinvl .","title":"VinVLStore"},{"location":"models/keyframes/","text":"Video key-frames extractor This model implements two types of modules: a video frames encoder and the key-frames module. These models are an implementation of a TorchMlModule . Key-frames selector models These models allow to extract key-frames from a video. mlmodule.contrib.keyframes.selectors.KeyFrameSelector Video key-frames selector Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor, torch.Tensor] The PyTorch module to encode frames. fps int The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") . Video frames encoder mlmodule.contrib.keyframes.encoders.VideoFramesEncoder Video frames encoder This module will extract and encode frames of a video using an image_encoder . Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor] The PyTorch module to encode frames fps float The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"Video key-frames extractor"},{"location":"models/keyframes/#video-key-frames-extractor","text":"This model implements two types of modules: a video frames encoder and the key-frames module. These models are an implementation of a TorchMlModule .","title":"Video key-frames extractor"},{"location":"models/keyframes/#key-frames-selector-models","text":"These models allow to extract key-frames from a video.","title":"Key-frames selector models"},{"location":"models/keyframes/#mlmodule.contrib.keyframes.selectors.KeyFrameSelector","text":"Video key-frames selector Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor, torch.Tensor] The PyTorch module to encode frames. fps int The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"KeyFrameSelector"},{"location":"models/keyframes/#video-frames-encoder","text":"","title":"Video frames encoder"},{"location":"models/keyframes/#mlmodule.contrib.keyframes.encoders.VideoFramesEncoder","text":"Video frames encoder This module will extract and encode frames of a video using an image_encoder . Attributes: Name Type Description image_encoder TorchMlModule[torch.Tensor] The PyTorch module to encode frames fps float The number of frames per seconds to extract from the video. Defaults to 1. device torch.device The PyTorch device to initialise the model weights. Defaults to torch.device(\"cpu\") .","title":"VideoFramesEncoder"},{"location":"references/callbacks/","text":"Callbacks Callbacks are used to control to control how a runner handles the results (features, labels or model weights). They are classes implementing a pre-defined set of functions: save_features save_label_scores save_bounding_boxes save_frames In memory callbacks These callbacks accumulate results in-memory. They expose their results via object attributes. mlmodule.v2.helpers.callbacks.CollectFeaturesInMemory dataclass Callback to collect features in memory Attributes: Name Type Description indices list List of dataset indices features numpy.ndarray Array of features. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_features ( self , model : Any , indices : Sequence , features : Union [ torch . Tensor , numpy . ndarray ]) -> None Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required mlmodule.v2.helpers.callbacks.CollectLabelsInMemory dataclass Callback to collect labels in memory Attributes: Name Type Description indices list List of dataset indices label_scores numpy.ndarray Array of label scores. The first dimension correspond to self.indices values. labels list[str] List of matching labels (label with maximum score) Note This callback works with any array-like features save_label_scores ( self , model : ModelWithLabels , indices : Sequence , labels_scores : Union [ torch . Tensor , numpy . ndarray ]) -> None Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required mlmodule.v2.helpers.callbacks.CollectBoundingBoxesInMemory dataclass Callback to collect bounding boxes predictions in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_bounding_boxes ( self , model : Any , indices : Sequence , bounding_boxes : Sequence [ mlmodule . v2 . base . predictions . BatchBoundingBoxesPrediction [ Union [ torch . Tensor , numpy . ndarray ]]]) -> None Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required mlmodule.v2.helpers.callbacks.CollectVideoFramesInMemory dataclass Callback to collect video frames in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features save_frames ( self , model : Any , indices : Sequence , frames : Sequence [ mlmodule . v2 . base . predictions . BatchVideoFramesPrediction [ Union [ torch . Tensor , numpy . ndarray ]]]) -> None Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required Callbacks for training mlmodule.v2.helpers.callbacks.SaveModelState dataclass Simple callback to save model state during training. If state are saved during traing (every X epochs, see TorchTrainingOptions ) the current epoch number is appended to the state_key.training_id in the following way: <state_key.training_id>-e<num_epoch> . When the training is complete, just the state_key.training_id is used. Attributes: Name Type Description store AbstractStateStore Object to handle model state saving state_key StateKey State identifier for the training activity. Warning This callback only saves the model state, thus does not create a whole training checkpoint (optimizer state, loss, etc..). save_model_state ( self , engine : Engine , model : Any ) -> None Save model state by calling the state store Parameters: Name Type Description Default model Any The MLModule model to save required Write your own callbacks mlmodule.v2.base.callbacks.BaseSaveFeaturesCallback save_features ( self , model : Any , indices : Sequence , features : - _ContraArrayType ) -> None Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required mlmodule.v2.base.callbacks.BaseSaveLabelsCallback save_label_scores ( self , model : ModelWithLabels , indices : Sequence , labels_scores : - _ContraArrayType ) -> None Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required mlmodule.v2.base.callbacks.BaseSaveBoundingBoxCallback save_bounding_boxes ( self , model : Any , indices : Sequence , bounding_boxes : Sequence [ mlmodule . v2 . base . predictions . BatchBoundingBoxesPrediction [ - _ContraArrayType ]]) -> None Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required mlmodule.v2.base.callbacks.BaseSaveVideoFramesCallback save_frames ( self , model : Any , indices : Sequence , frames : Sequence [ mlmodule . v2 . base . predictions . BatchVideoFramesPrediction [ - _ContraArrayType ]]) -> None Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required mlmodule.v2.base.callbacks.BaseRunnerEndCallback on_runner_end ( self , model : Any ) -> None Called when the runner finishes This can be used to do clean up. For instance if the data is being processed by a thread, this function can wait for the thread to finish.$ Parameters: Name Type Description Default model Any The MLModule model required","title":"Callbacks"},{"location":"references/callbacks/#callbacks","text":"Callbacks are used to control to control how a runner handles the results (features, labels or model weights). They are classes implementing a pre-defined set of functions: save_features save_label_scores save_bounding_boxes save_frames","title":"Callbacks"},{"location":"references/callbacks/#in-memory-callbacks","text":"These callbacks accumulate results in-memory. They expose their results via object attributes.","title":"In memory callbacks"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectFeaturesInMemory","text":"Callback to collect features in memory Attributes: Name Type Description indices list List of dataset indices features numpy.ndarray Array of features. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectFeaturesInMemory"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectFeaturesInMemory.save_features","text":"Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required","title":"save_features()"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectLabelsInMemory","text":"Callback to collect labels in memory Attributes: Name Type Description indices list List of dataset indices label_scores numpy.ndarray Array of label scores. The first dimension correspond to self.indices values. labels list[str] List of matching labels (label with maximum score) Note This callback works with any array-like features","title":"CollectLabelsInMemory"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectLabelsInMemory.save_label_scores","text":"Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required","title":"save_label_scores()"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectBoundingBoxesInMemory","text":"Callback to collect bounding boxes predictions in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectBoundingBoxesInMemory"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectBoundingBoxesInMemory.save_bounding_boxes","text":"Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required","title":"save_bounding_boxes()"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectVideoFramesInMemory","text":"Callback to collect video frames in memory Attributes: Name Type Description indices list List of dataset indices frames list[BatchVideoFramesPrediction[np.ndarray]] Sequence of video frames. The first dimension correspond to self.indices values. Note This callback works with any array-like features","title":"CollectVideoFramesInMemory"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.CollectVideoFramesInMemory.save_frames","text":"Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required","title":"save_frames()"},{"location":"references/callbacks/#callbacks-for-training","text":"","title":"Callbacks for training"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.SaveModelState","text":"Simple callback to save model state during training. If state are saved during traing (every X epochs, see TorchTrainingOptions ) the current epoch number is appended to the state_key.training_id in the following way: <state_key.training_id>-e<num_epoch> . When the training is complete, just the state_key.training_id is used. Attributes: Name Type Description store AbstractStateStore Object to handle model state saving state_key StateKey State identifier for the training activity. Warning This callback only saves the model state, thus does not create a whole training checkpoint (optimizer state, loss, etc..).","title":"SaveModelState"},{"location":"references/callbacks/#mlmodule.v2.helpers.callbacks.SaveModelState.save_model_state","text":"Save model state by calling the state store Parameters: Name Type Description Default model Any The MLModule model to save required","title":"save_model_state()"},{"location":"references/callbacks/#write-your-own-callbacks","text":"","title":"Write your own callbacks"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveFeaturesCallback","text":"","title":"BaseSaveFeaturesCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveFeaturesCallback.save_features","text":"Save features output returned by a module Parameters: Name Type Description Default model Any The MLModule model that produced the features required indices Sequence The list of indices as defined by the dataset required features ArrayLike The feature object as returned by the model required","title":"save_features()"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveLabelsCallback","text":"","title":"BaseSaveLabelsCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveLabelsCallback.save_label_scores","text":"Save labels scores returned by a module Parameters: Name Type Description Default model ModelWithLabels The MLModule model that produced the label scores required indices Sequence The list of indices as defined by the dataset required labels_scores ArrayLike Contains the output score/probability for each label required","title":"save_label_scores()"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveBoundingBoxCallback","text":"","title":"BaseSaveBoundingBoxCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveBoundingBoxCallback.save_bounding_boxes","text":"Save bounding boxes output of a module Parameters: Name Type Description Default model Any The MLModule model that produces the bounding boxes required indices Sequence The list of indices as defined by the dataset required bounding_boxes Sequence[BatchBoundingBoxesPrediction[_ContraArrayType]] The sequence bounding predictions required","title":"save_bounding_boxes()"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveVideoFramesCallback","text":"","title":"BaseSaveVideoFramesCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseSaveVideoFramesCallback.save_frames","text":"Save frames extracted from a video Parameters: Name Type Description Default model Any The MLModule model that produces the video frames encoding required indices Sequence The list of indices as defined by the dataset required frames Sequence[BatchVideoFramesPrediction[_ArrayType]] The sequence of frame features and indices required","title":"save_frames()"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseRunnerEndCallback","text":"","title":"BaseRunnerEndCallback"},{"location":"references/callbacks/#mlmodule.v2.base.callbacks.BaseRunnerEndCallback.on_runner_end","text":"Called when the runner finishes This can be used to do clean up. For instance if the data is being processed by a thread, this function can wait for the thread to finish.$ Parameters: Name Type Description Default model Any The MLModule model required","title":"on_runner_end()"},{"location":"references/datasets/","text":"Datasets Torch datasets Torch dataset are implementing dataset as suggested in the Datasets & Dataloaders guide. These datasets are compatible with torch.utils.data.DataLoader . In-memory list datasets mlmodule.v2.torch.datasets.ListDataset dataclass Simple dataset that contains a list of objects in memory Attributes: Name Type Description objects Sequence[_DatasetType] List of objects of the dataset mlmodule.v2.torch.datasets.ListDatasetIndexed dataclass Simple dataset that contains a list of objects in memory with custom indices Attributes: Name Type Description indices Sequence[_IndicesType] Indices to track objects in results objects Sequence[_DatasetType] List of objects of the dataset Local files datasets mlmodule.v2.torch.datasets.LocalBinaryFilesDataset dataclass Dataset that reads a list of local file names and returns their content as bytes Attributes: Name Type Description paths Sequence[_PathLike] List of paths to files Image datasets mlmodule.v2.torch.datasets.ImageDataset dataclass Dataset that returns PIL.Image.Image from a dataset of images in bytes format Attributes: Name Type Description binary_files_dataset TorchDataset[_IndicesType, bytes] Dataset to load images. Usually a LocalBinaryFilesDataset . resize_image_size tuple[int, int] | None Optionally reduce the image size on load mode str | None Optional mode to apply when loading the image. See PIL Image.draft parameters. Bounding box datasets mlmodule.v2.torch.datasets.ImageBoundingBoxDataset dataclass Dataset that returns tuple of Image and bounding box data from a list of file names and bounding box information Attributes: Name Type Description image_dataset TorchDataset[_IndicesType, Image] The dataset of images bounding_boxes Sequence[BatchBoundingBoxesPrediction[np.ndarray]] The bounding boxes predictions for all given images crop_image bool Whether to crop the image at the bounding box when loading it Training datasets mlmodule.v2.torch.datasets.TorchTrainingDataset dataclass Dataset for training that returns a tuple (payload, target) where payload is the value returned by dataset and target the corrisponding element in targets . Attributes: Name Type Description dataset TorchDataset[_IndicesType, _DatasetType] A TorchDataset targets Sequence[_TargetsType] Training target for each element of the dataset Note Length of targets must match the size of the dataset . Warning TorchTrainingDataset doesn't work is with Torchvision datasets in torchvision.datasets . Write your own dataset Dataset types depends on the runner used, refer to the runner list to know which type to implement. Below is the list of dataset protocols that are specified by mlmodule mlmodule.v2.torch.datasets.TorchDataset PyTorch dataset protocol In order to be used with the PyTorch runners, a TorchDataset should expose two functions __getitem__ and __len__ . __getitem__ ( self , index : int ) -> Tuple [ + _IndicesType , + _DatasetType ] special Get an item of the dataset by index Parameters: Name Type Description Default index int The index of the element to get required Returns: Type Description Tuple[_IndicesType, _DatasetType] A tuple of dataset index of the element and value of the element __len__ ( self ) -> int special Length of the dataset Returns: Type Description int The length of the dataset getitem_indices ( self , index : int ) -> + _IndicesType The value of dataset indices at index position Parameters: Name Type Description Default index int The position of the element to get required Returns: Type Description _IndicesType The value of the indices at the position","title":"Datasets"},{"location":"references/datasets/#datasets","text":"","title":"Datasets"},{"location":"references/datasets/#torch-datasets","text":"Torch dataset are implementing dataset as suggested in the Datasets & Dataloaders guide. These datasets are compatible with torch.utils.data.DataLoader .","title":"Torch datasets"},{"location":"references/datasets/#in-memory-list-datasets","text":"","title":"In-memory list datasets"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.ListDataset","text":"Simple dataset that contains a list of objects in memory Attributes: Name Type Description objects Sequence[_DatasetType] List of objects of the dataset","title":"ListDataset"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.ListDatasetIndexed","text":"Simple dataset that contains a list of objects in memory with custom indices Attributes: Name Type Description indices Sequence[_IndicesType] Indices to track objects in results objects Sequence[_DatasetType] List of objects of the dataset","title":"ListDatasetIndexed"},{"location":"references/datasets/#local-files-datasets","text":"","title":"Local files datasets"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.LocalBinaryFilesDataset","text":"Dataset that reads a list of local file names and returns their content as bytes Attributes: Name Type Description paths Sequence[_PathLike] List of paths to files","title":"LocalBinaryFilesDataset"},{"location":"references/datasets/#image-datasets","text":"","title":"Image datasets"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.ImageDataset","text":"Dataset that returns PIL.Image.Image from a dataset of images in bytes format Attributes: Name Type Description binary_files_dataset TorchDataset[_IndicesType, bytes] Dataset to load images. Usually a LocalBinaryFilesDataset . resize_image_size tuple[int, int] | None Optionally reduce the image size on load mode str | None Optional mode to apply when loading the image. See PIL Image.draft parameters.","title":"ImageDataset"},{"location":"references/datasets/#bounding-box-datasets","text":"","title":"Bounding box datasets"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.ImageBoundingBoxDataset","text":"Dataset that returns tuple of Image and bounding box data from a list of file names and bounding box information Attributes: Name Type Description image_dataset TorchDataset[_IndicesType, Image] The dataset of images bounding_boxes Sequence[BatchBoundingBoxesPrediction[np.ndarray]] The bounding boxes predictions for all given images crop_image bool Whether to crop the image at the bounding box when loading it","title":"ImageBoundingBoxDataset"},{"location":"references/datasets/#training-datasets","text":"","title":"Training datasets"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.TorchTrainingDataset","text":"Dataset for training that returns a tuple (payload, target) where payload is the value returned by dataset and target the corrisponding element in targets . Attributes: Name Type Description dataset TorchDataset[_IndicesType, _DatasetType] A TorchDataset targets Sequence[_TargetsType] Training target for each element of the dataset Note Length of targets must match the size of the dataset . Warning TorchTrainingDataset doesn't work is with Torchvision datasets in torchvision.datasets .","title":"TorchTrainingDataset"},{"location":"references/datasets/#write-your-own-dataset","text":"Dataset types depends on the runner used, refer to the runner list to know which type to implement. Below is the list of dataset protocols that are specified by mlmodule","title":"Write your own dataset"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.TorchDataset","text":"PyTorch dataset protocol In order to be used with the PyTorch runners, a TorchDataset should expose two functions __getitem__ and __len__ .","title":"TorchDataset"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.TorchDataset.__getitem__","text":"Get an item of the dataset by index Parameters: Name Type Description Default index int The index of the element to get required Returns: Type Description Tuple[_IndicesType, _DatasetType] A tuple of dataset index of the element and value of the element","title":"__getitem__()"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.TorchDataset.__len__","text":"Length of the dataset Returns: Type Description int The length of the dataset","title":"__len__()"},{"location":"references/datasets/#mlmodule.v2.torch.datasets.TorchDataset.getitem_indices","text":"The value of dataset indices at index position Parameters: Name Type Description Default index int The position of the element to get required Returns: Type Description _IndicesType The value of the indices at the position","title":"getitem_indices()"},{"location":"references/labels/","text":"mlmodule.labels.base.LabelSet dataclass Label set is an ordered list of labels used for classification tasks Attributes: Name Type Description label_set_unique_id str Unique identifier for a label set label_list List[str] Ordered list of labels label_to_idx dict Dict with items (label_name, label_index) Examples: LabelSet objects are used as classic lists: animal_labels = LabelSet ( label_set_unique_id = \"animals\" , label_list = [ \"cat\" , \"dog\" ] ) print ( animal_labels . label_set_unique_id ) # \"animals\" print ( animal_labels [ 0 ]) # \"cat\" print ( animal_labels [ 1 ]) # \"dog\" print ( len ( animal_labels )) # 2 print ( animal_labels . get_label_ids ([ \"dog\" ])) # [1] get_label_ids ( self , labels : List [ str ]) -> List [ int ] Get the list of label indices for the provided labels. Parameters: Name Type Description Default labels List[str] list of labels of which to obtain indices required Exceptions: Type Description ValueError when a label in labels is not found in the label set Returns: Type Description indices (List[int]) list of indices Available label sets mlmodule.labels.imagenet.IMAGENET_LABELS mlmodule.labels.places_io.PLACES_IN_OUT_DOOR mlmodule.labels.places_io.PLACES_IO_LABELS mlmodule.labels.places.PLACES_LABELS mlmodule.labels.vinvl_attributes.VINVL_ATTRIBUTE_LABELS mlmodule.labels.vinvl.VINVL_LABELS","title":"Labels"},{"location":"references/labels/#mlmodule.labels.base.LabelSet","text":"Label set is an ordered list of labels used for classification tasks Attributes: Name Type Description label_set_unique_id str Unique identifier for a label set label_list List[str] Ordered list of labels label_to_idx dict Dict with items (label_name, label_index) Examples: LabelSet objects are used as classic lists: animal_labels = LabelSet ( label_set_unique_id = \"animals\" , label_list = [ \"cat\" , \"dog\" ] ) print ( animal_labels . label_set_unique_id ) # \"animals\" print ( animal_labels [ 0 ]) # \"cat\" print ( animal_labels [ 1 ]) # \"dog\" print ( len ( animal_labels )) # 2 print ( animal_labels . get_label_ids ([ \"dog\" ])) # [1]","title":"LabelSet"},{"location":"references/labels/#mlmodule.labels.base.LabelSet.get_label_ids","text":"Get the list of label indices for the provided labels. Parameters: Name Type Description Default labels List[str] list of labels of which to obtain indices required Exceptions: Type Description ValueError when a label in labels is not found in the label set Returns: Type Description indices (List[int]) list of indices","title":"get_label_ids()"},{"location":"references/labels/#available-label-sets","text":"mlmodule.labels.imagenet.IMAGENET_LABELS mlmodule.labels.places_io.PLACES_IN_OUT_DOOR mlmodule.labels.places_io.PLACES_IO_LABELS mlmodule.labels.places.PLACES_LABELS mlmodule.labels.vinvl_attributes.VINVL_ATTRIBUTE_LABELS mlmodule.labels.vinvl.VINVL_LABELS","title":"Available label sets"},{"location":"references/options/","text":"Options mlmodule.v2.torch.options.TorchRunnerOptions dataclass Options for PyTorch runners Attributes: Name Type Description device torch.device Torch device data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar mlmodule.v2.torch.options.TorchMultiGPURunnerOptions dataclass Options for PyTorch multi-gpu runners Attributes: Name Type Description data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar. Default, False. dist_options dict Options passed to ignite.distributed.Parallel . seed int random state seed to set. Default, 543. Note data_loader_options 's options batch_size and num_worker will be automatically scaled according to world_size and nprocs respectively. For more info visit auto_dataloader documentation . Note dist_options usually include backend and nproc_per_node parameters. For more info visit PyTorch Ignite's distributed documentation . mlmodule.v2.torch.options.TorchTrainingOptions dataclass Options for PyTorch training runners Attributes: Name Type Description criterion Union[Callable, torch.nn.Module] the loss function to use during training. optimizer torch.optim.Optimizer Optimization strategy to use during training. num_epochs int number of epochs to train the model. validate_every int run model's validation every validate_every epochs. checkpoint_every Optional[int] store training checkpoint every checkpoint_every epochs. metrics Dict[str, Metric] Dictionary where values are Ignite's metrics to compute during evaluation. data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar. Default, False. dist_options dict Options passed to ignite.distributed.Parallel . seed int random state seed to set. Default, 543. Note data_loader_options 's options batch_size and num_worker will be automatically scaled according to world_size and nprocs respectively. For more info visit auto_dataloader documentation . Note dist_options usually include backend and nproc_per_node parameters. For more info visit PyTorch Ignite's distributed documentation .","title":"Options"},{"location":"references/options/#options","text":"","title":"Options"},{"location":"references/options/#mlmodule.v2.torch.options.TorchRunnerOptions","text":"Options for PyTorch runners Attributes: Name Type Description device torch.device Torch device data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar","title":"TorchRunnerOptions"},{"location":"references/options/#mlmodule.v2.torch.options.TorchMultiGPURunnerOptions","text":"Options for PyTorch multi-gpu runners Attributes: Name Type Description data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar. Default, False. dist_options dict Options passed to ignite.distributed.Parallel . seed int random state seed to set. Default, 543. Note data_loader_options 's options batch_size and num_worker will be automatically scaled according to world_size and nprocs respectively. For more info visit auto_dataloader documentation . Note dist_options usually include backend and nproc_per_node parameters. For more info visit PyTorch Ignite's distributed documentation .","title":"TorchMultiGPURunnerOptions"},{"location":"references/options/#mlmodule.v2.torch.options.TorchTrainingOptions","text":"Options for PyTorch training runners Attributes: Name Type Description criterion Union[Callable, torch.nn.Module] the loss function to use during training. optimizer torch.optim.Optimizer Optimization strategy to use during training. num_epochs int number of epochs to train the model. validate_every int run model's validation every validate_every epochs. checkpoint_every Optional[int] store training checkpoint every checkpoint_every epochs. metrics Dict[str, Metric] Dictionary where values are Ignite's metrics to compute during evaluation. data_loader_options dict Options passed to torch.utils.dataloader.DataLoader . tqdm_enabled bool Whether to print a tqdm progress bar. Default, False. dist_options dict Options passed to ignite.distributed.Parallel . seed int random state seed to set. Default, 543. Note data_loader_options 's options batch_size and num_worker will be automatically scaled according to world_size and nprocs respectively. For more info visit auto_dataloader documentation . Note dist_options usually include backend and nproc_per_node parameters. For more info visit PyTorch Ignite's distributed documentation .","title":"TorchTrainingOptions"},{"location":"references/runners/","text":"Runners Runners are used to execute inference or training of a model. They are initialised with a model , a dataset , callbacks and options . They are executed with the run function which takes no arguments. Inference mlmodule.v2.torch.runners.TorchInferenceRunner Runner for inference tasks on PyTorch models Supports CPU or single GPU inference. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchRunnerOptions PyTorch options run ( self ) -> None Runs inference mlmodule.v2.torch.runners.TorchInferenceMultiGPURunner Runner for inference tasks on PyTorch models Supports CPU and multi-GPU inference with native torch backends. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchMultiGPURunnerOptions PyTorch multi-gpu options run ( self ) -> None Runs inference Training mlmodule.v2.torch.runners.TorchTrainingRunner dataclass Runner for training tasks on PyTorch models Supports CPU and multi-GPU training with multiple backends. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset Tuple[TorchTrainingDataset, TorchTrainingDataset] Train and test datasets for the runner callbacks List[Union[BaseRunnerEndCallback, SaveModelState]] Callback to save model weights plus one or more callbacks for when the runner ends. options TorchTrainingOptions PyTorch training options run ( self ) -> None Runs training Write your own runner mlmodule.v2.base.runners.BaseRunner A runner takes a model and run an action on it (inference, training...) It must implement the run function and call the callbacks to save prediction or model weights. Attributes: Name Type Description model _ModelType The model object to run the action against dataset _DataType The input dataset callbacks List[_CallbackType] Callbacks to save model state or predictions options _OptionsType Options of the runner (devices...) Note The _ModelType , _DataType , _CallbackType and _OptionsType are generic types that should be specified when implementing a runner. Examples: This is an example of a runner that applies a function to each element of list dataset. It passes the returned data to the save_features callback. from mlmodule.v2.base.callbacks import ( BaseSaveFeaturesCallback , callbacks_caller ) from mlmodule.v2.base.runners import BaseRunner class NumpySumRunnerExample ( BaseRunner [ Callable , # _ModelType List , # _DataType BaseSaveFeaturesCallback , # _CallbackType None # _OptionType ]): def run ( self ): for index , data in enumerate ( self . dataset ): # Helper function to call all matching callbacks callbacks_caller ( self . callbacks , \"save_features\" , index , self . model ( data ) )","title":"Runners"},{"location":"references/runners/#runners","text":"Runners are used to execute inference or training of a model. They are initialised with a model , a dataset , callbacks and options . They are executed with the run function which takes no arguments.","title":"Runners"},{"location":"references/runners/#inference","text":"","title":"Inference"},{"location":"references/runners/#mlmodule.v2.torch.runners.TorchInferenceRunner","text":"Runner for inference tasks on PyTorch models Supports CPU or single GPU inference. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchRunnerOptions PyTorch options","title":"TorchInferenceRunner"},{"location":"references/runners/#mlmodule.v2.torch.runners.TorchInferenceRunner.run","text":"Runs inference","title":"run()"},{"location":"references/runners/#mlmodule.v2.torch.runners.TorchInferenceMultiGPURunner","text":"Runner for inference tasks on PyTorch models Supports CPU and multi-GPU inference with native torch backends. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset TorchDataset Input dataset for the runner callbacks List[TorchRunnerCallbackType] Callbacks to save features, labels or bounding boxes options TorchMultiGPURunnerOptions PyTorch multi-gpu options","title":"TorchInferenceMultiGPURunner"},{"location":"references/runners/#mlmodule.v2.torch.runners.TorchInferenceMultiGPURunner.run","text":"Runs inference","title":"run()"},{"location":"references/runners/#training","text":"","title":"Training"},{"location":"references/runners/#mlmodule.v2.torch.runners.TorchTrainingRunner","text":"Runner for training tasks on PyTorch models Supports CPU and multi-GPU training with multiple backends. Attributes: Name Type Description model TorchMlModule The PyTorch model to run inference dataset Tuple[TorchTrainingDataset, TorchTrainingDataset] Train and test datasets for the runner callbacks List[Union[BaseRunnerEndCallback, SaveModelState]] Callback to save model weights plus one or more callbacks for when the runner ends. options TorchTrainingOptions PyTorch training options","title":"TorchTrainingRunner"},{"location":"references/runners/#mlmodule.v2.torch.runners.TorchTrainingRunner.run","text":"Runs training","title":"run()"},{"location":"references/runners/#write-your-own-runner","text":"","title":"Write your own runner"},{"location":"references/runners/#mlmodule.v2.base.runners.BaseRunner","text":"A runner takes a model and run an action on it (inference, training...) It must implement the run function and call the callbacks to save prediction or model weights. Attributes: Name Type Description model _ModelType The model object to run the action against dataset _DataType The input dataset callbacks List[_CallbackType] Callbacks to save model state or predictions options _OptionsType Options of the runner (devices...) Note The _ModelType , _DataType , _CallbackType and _OptionsType are generic types that should be specified when implementing a runner. Examples: This is an example of a runner that applies a function to each element of list dataset. It passes the returned data to the save_features callback. from mlmodule.v2.base.callbacks import ( BaseSaveFeaturesCallback , callbacks_caller ) from mlmodule.v2.base.runners import BaseRunner class NumpySumRunnerExample ( BaseRunner [ Callable , # _ModelType List , # _DataType BaseSaveFeaturesCallback , # _CallbackType None # _OptionType ]): def run ( self ): for index , data in enumerate ( self . dataset ): # Helper function to call all matching callbacks callbacks_caller ( self . callbacks , \"save_features\" , index , self . model ( data ) )","title":"BaseRunner"},{"location":"references/states/","text":"States are managed with two concepts: StateType : Represents a family of states that are compatible with each other . In general, a model can be loaded with any pre-trained state if it matches its state_type attribute. StateKey : The identifier of a state instance, it should uniquely identify the result of a training activity for a model. mlmodule.v2.states.StateType dataclass Definition for a type of state A state type is used to identify states that can be re-used accross models. For instance, the weights from ResNet18 with 1000 classes pretrained on ImageNet can be reused to initialised the weights of a ResNet18 for binary classification. In this scenario, the state type of the ResNet trained on ImageNet can be re-used to partially initialize the binary classification ResNet18. This is also used for models like key-frames extraction. Key-frames extraction does not define a new weights architecture, it is a wrapper around an image encoder. Therefore, any state that can be loaded into the image encoder, can also be loaded into the key-frame extractor. They share the same state type. As a convention, two state types are compatible when backend and architecture attributes are the same. This is implemented in the is_compatible_with method. Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description backend str The model backend. For instance pytorch . architecture str Identifier for the architecture (e.g. torchresnet18 ...). extra tuple[str, ...] Additional information to identify architecture variants (number of output classes...). is_compatible_with ( self , other : StateType ) -> bool Tells whether two architecture are compatible with each other. Parameters: Name Type Description Default other StateType The other architecture to compare required Returns: Type Description bool true if backend and architecture attributes match. mlmodule.v2.states.StateKey dataclass Identifier for a state of a trained model Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description state_type StateType Identifies the type of state training_id str Identifies the training activity that was used to get to this state. mlmodule . v2 . states . VALID_NAMES The pattern for valid architecture name in StateType.architecture . It must be alphanumeric characters separated with dashes (i.e. [\\w\\-\\d] )","title":"States"},{"location":"references/states/#mlmodule.v2.states.StateType","text":"Definition for a type of state A state type is used to identify states that can be re-used accross models. For instance, the weights from ResNet18 with 1000 classes pretrained on ImageNet can be reused to initialised the weights of a ResNet18 for binary classification. In this scenario, the state type of the ResNet trained on ImageNet can be re-used to partially initialize the binary classification ResNet18. This is also used for models like key-frames extraction. Key-frames extraction does not define a new weights architecture, it is a wrapper around an image encoder. Therefore, any state that can be loaded into the image encoder, can also be loaded into the key-frame extractor. They share the same state type. As a convention, two state types are compatible when backend and architecture attributes are the same. This is implemented in the is_compatible_with method. Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description backend str The model backend. For instance pytorch . architecture str Identifier for the architecture (e.g. torchresnet18 ...). extra tuple[str, ...] Additional information to identify architecture variants (number of output classes...).","title":"StateType"},{"location":"references/states/#mlmodule.v2.states.StateType.is_compatible_with","text":"Tells whether two architecture are compatible with each other. Parameters: Name Type Description Default other StateType The other architecture to compare required Returns: Type Description bool true if backend and architecture attributes match.","title":"is_compatible_with()"},{"location":"references/states/#mlmodule.v2.states.StateKey","text":"Identifier for a state of a trained model Warning All string attributes must match the VALID_NAMES pattern. Attributes: Name Type Description state_type StateType Identifies the type of state training_id str Identifies the training activity that was used to get to this state.","title":"StateKey"},{"location":"references/states/#mlmodule.v2.states.VALID_NAMES","text":"The pattern for valid architecture name in StateType.architecture . It must be alphanumeric characters separated with dashes (i.e. [\\w\\-\\d] )","title":"VALID_NAMES"},{"location":"references/stores/","text":"Stores Stores are used to load and save models state. They define 3 methods: get_state_keys , load and save . The get_state_keys method allows to list available pre-trained states for a given type of state. It usually called from a model . # List available model states in store for the model state type state_keys = store . get_state_keys ( model . state_type ) print ( state_keys ) # Prints for a ResNet pre-trained on ImageNet: # [ # StateKey( # state_type=StateType( # backend='pytorch', # architecture='resnet18', # extra=('cls1000',) # ), # training_id='imagenet' # ) # ] See states documentation for more information on how pre-trained states are identified. Then, a state can be loaded into the model . # Getting the first state key resnet_imagenet_state_key = state_keys [ 0 ] # Loading it into the model store . load ( model , state_key = resnet_imagenet_state_key ) A model can be saved by specifying a training_id which should uniquely identify the training activity that yielded this model's state. store . save ( model , training_id = \"2022-01-01-finetuned-imagenet\" ) See AbstractStateStore for more details on these methods. MLModule pre-trained models MLModule provides model weights for all defined models through the MlModule Store . mlmodule . v2 . stores . Store () -> S3StateStore MlModule model state store. Examples: The store can be used to list available pre-trained states for a model store = Store () states = store . get_state_keys ( model . state_type ) And load a given state to a model store . load ( model , state_key = states [ 0 ]) Alternative stores These stores can be used if you want to store model states locally or on S3 storage. mlmodule.v2.stores.local.LocalStateStore dataclass Local filebased store Attributes: Name Type Description folder str Path to the folder to save model's state mlmodule.v2.stores.s3.S3StateStore dataclass State store on top of S3 object storage Given the state keys, states are organised in in folders with the following structure: base_path/ \u251c\u2500 { backend } / \u2502 \u251c\u2500 { architecture } . { extra1 } . { extra2 } . { training_id } .pt \u251c\u2500 pytorch/ # e.g. for torch models \u2502 \u251c\u2500 resnet18.cls1000.imagenet.pt \u2502 \u251c\u2500 clip-image-rn50.clip.pt Attributes: Name Type Description bucket str Bucket to use to store states session_kwargs dict Arguments passed to initialise boto3.session.Session s3_endpoint_url str To connect to S3 compatible storage base_path str The base path to store states mlmodule.v2.stores.github.GitHUBReleaseStore dataclass Store implementation leveraging GitHUB releases Model weights are stored as assets in a release . We recommend setting up GitHUB authentication to use the get_state_keys and the save methods. These methods are calling the releases API and are limited to 60 requests by hour unauthenticated. This can be done with: Personal access token : The PAT and username need to be set in the environment variable GH_API_BASIC_AUTH={username}:{personal_access_token} GitHUB Token : Used in GitHUB Actions, needs to be set in a GH_TOKEN environment variable. Model states are organised in releases with the following convention: Release name and tags are constructed as {release_name_prefix}.{state_type.backend}.{state_type.architecture} Asset names within a release are constructed as {state_type.extra1}.{state_type.extra2}.{training_id}.state.gzip Attributes: Name Type Description repository_owner str The owner of the GitHUB repository repository_name str The name of the repository to use as a store branch_name str The branch used to create new releases holding model state. By default we recommend using an orphan branch named model-store . release_name_prefix str The prefix to identify releases containing model weights. Defaults to state , should not contain a dot. Write your own store A store should inherit AbstractStateStore and implement the save , load and get_state_keys methods. mlmodule.v2.stores.abstract.AbstractStateStore Interface to handle model state loading and saving See states reference for more information on state management. exists ( self , state_key : StateKey ) -> bool Tests whether the state key exists in the current store Parameters: Name Type Description Default state_key StateKey The state key to test required Returns: Type Description bool True if state key exists or False otherwise get_state_keys ( self , state_type : StateType ) -> List [ mlmodule . v2 . states . StateKey ] Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type ) load ( self , model : ~ _ModelType , state_key : StateKey ) -> None Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load save ( self , model : ~ _ModelType , training_id : str ) -> StateKey Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created For stores used to download states of a single model, it can be useful to subclass the AbstractListStateStore directly. This makes it easier to define a store from a fix set of states as it is often the case when integrating the weights from external sources (pre-trained states for a paper, hugging face...). See SBERTDistiluseBaseMultilingualCasedV2Store for an example. mlmodule.v2.stores.list.AbstractListStateStore Helper to define a store from a fixed list of state keys. The subclasses should implement the following: available_state_keys state_downloader available_state_keys : List [ mlmodule . v2 . states . StateKey ] property readonly List of available state keys for this store Returns: Type Description list(StateKey) All available state keys in the store get_state_keys ( self , state_type : StateType ) -> List [ mlmodule . v2 . states . StateKey ] Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type ) load ( self , model : ~ _ModelType , state_key : StateKey ) -> None Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load save ( self , model : ~ _ModelType , training_id : str ) -> NoReturn Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created state_downloader ( self , model : ~ _ModelType , state_key : StateKey ) -> None Downloads and applies a state to a model Parameters: Name Type Description Default model _ModelType The model that will be used to load the state required state_key StateKey The state key identifier required","title":"Stores"},{"location":"references/stores/#stores","text":"Stores are used to load and save models state. They define 3 methods: get_state_keys , load and save . The get_state_keys method allows to list available pre-trained states for a given type of state. It usually called from a model . # List available model states in store for the model state type state_keys = store . get_state_keys ( model . state_type ) print ( state_keys ) # Prints for a ResNet pre-trained on ImageNet: # [ # StateKey( # state_type=StateType( # backend='pytorch', # architecture='resnet18', # extra=('cls1000',) # ), # training_id='imagenet' # ) # ] See states documentation for more information on how pre-trained states are identified. Then, a state can be loaded into the model . # Getting the first state key resnet_imagenet_state_key = state_keys [ 0 ] # Loading it into the model store . load ( model , state_key = resnet_imagenet_state_key ) A model can be saved by specifying a training_id which should uniquely identify the training activity that yielded this model's state. store . save ( model , training_id = \"2022-01-01-finetuned-imagenet\" ) See AbstractStateStore for more details on these methods.","title":"Stores"},{"location":"references/stores/#mlmodule-pre-trained-models","text":"MLModule provides model weights for all defined models through the MlModule Store .","title":"MLModule pre-trained models"},{"location":"references/stores/#mlmodule.v2.stores.Store","text":"MlModule model state store. Examples: The store can be used to list available pre-trained states for a model store = Store () states = store . get_state_keys ( model . state_type ) And load a given state to a model store . load ( model , state_key = states [ 0 ])","title":"Store()"},{"location":"references/stores/#alternative-stores","text":"These stores can be used if you want to store model states locally or on S3 storage.","title":"Alternative stores"},{"location":"references/stores/#mlmodule.v2.stores.local.LocalStateStore","text":"Local filebased store Attributes: Name Type Description folder str Path to the folder to save model's state","title":"LocalStateStore"},{"location":"references/stores/#mlmodule.v2.stores.s3.S3StateStore","text":"State store on top of S3 object storage Given the state keys, states are organised in in folders with the following structure: base_path/ \u251c\u2500 { backend } / \u2502 \u251c\u2500 { architecture } . { extra1 } . { extra2 } . { training_id } .pt \u251c\u2500 pytorch/ # e.g. for torch models \u2502 \u251c\u2500 resnet18.cls1000.imagenet.pt \u2502 \u251c\u2500 clip-image-rn50.clip.pt Attributes: Name Type Description bucket str Bucket to use to store states session_kwargs dict Arguments passed to initialise boto3.session.Session s3_endpoint_url str To connect to S3 compatible storage base_path str The base path to store states","title":"S3StateStore"},{"location":"references/stores/#mlmodule.v2.stores.github.GitHUBReleaseStore","text":"Store implementation leveraging GitHUB releases Model weights are stored as assets in a release . We recommend setting up GitHUB authentication to use the get_state_keys and the save methods. These methods are calling the releases API and are limited to 60 requests by hour unauthenticated. This can be done with: Personal access token : The PAT and username need to be set in the environment variable GH_API_BASIC_AUTH={username}:{personal_access_token} GitHUB Token : Used in GitHUB Actions, needs to be set in a GH_TOKEN environment variable. Model states are organised in releases with the following convention: Release name and tags are constructed as {release_name_prefix}.{state_type.backend}.{state_type.architecture} Asset names within a release are constructed as {state_type.extra1}.{state_type.extra2}.{training_id}.state.gzip Attributes: Name Type Description repository_owner str The owner of the GitHUB repository repository_name str The name of the repository to use as a store branch_name str The branch used to create new releases holding model state. By default we recommend using an orphan branch named model-store . release_name_prefix str The prefix to identify releases containing model weights. Defaults to state , should not contain a dot.","title":"GitHUBReleaseStore"},{"location":"references/stores/#write-your-own-store","text":"A store should inherit AbstractStateStore and implement the save , load and get_state_keys methods.","title":"Write your own store"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore","text":"Interface to handle model state loading and saving See states reference for more information on state management.","title":"AbstractStateStore"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore.exists","text":"Tests whether the state key exists in the current store Parameters: Name Type Description Default state_key StateKey The state key to test required Returns: Type Description bool True if state key exists or False otherwise","title":"exists()"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore.get_state_keys","text":"Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type )","title":"get_state_keys()"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore.load","text":"Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load","title":"load()"},{"location":"references/stores/#mlmodule.v2.stores.abstract.AbstractStateStore.save","text":"Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created For stores used to download states of a single model, it can be useful to subclass the AbstractListStateStore directly. This makes it easier to define a store from a fix set of states as it is often the case when integrating the weights from external sources (pre-trained states for a paper, hugging face...). See SBERTDistiluseBaseMultilingualCasedV2Store for an example.","title":"save()"},{"location":"references/stores/#mlmodule.v2.stores.list.AbstractListStateStore","text":"Helper to define a store from a fixed list of state keys. The subclasses should implement the following: available_state_keys state_downloader","title":"AbstractListStateStore"},{"location":"references/stores/#mlmodule.v2.stores.list.AbstractListStateStore.available_state_keys","text":"List of available state keys for this store Returns: Type Description list(StateKey) All available state keys in the store","title":"available_state_keys"},{"location":"references/stores/#mlmodule.v2.stores.list.AbstractListStateStore.get_state_keys","text":"Lists the available states that are compatible with the given state type. Attributes: Name Type Description state_type StateType Used to filter the compatible state keys Examples: This is used to list the pretrained weights for a given model. The following code gives all available state keys in store for the model . keys = store . get_state_keys ( model . state_type )","title":"get_state_keys()"},{"location":"references/stores/#mlmodule.v2.stores.list.AbstractListStateStore.load","text":"Loads the models weights from the store Attributes: Name Type Description model ModelWithState Model to update state_key StateKey The identifier for the state to load","title":"load()"},{"location":"references/stores/#mlmodule.v2.stores.list.AbstractListStateStore.save","text":"Saves the model state to the store Attributes: Name Type Description model ModelWithState Model to save training_id str Identifier for the training activity Returns: Type Description StateKey The identifier for the state that has been created","title":"save()"},{"location":"references/stores/#mlmodule.v2.stores.list.AbstractListStateStore.state_downloader","text":"Downloads and applies a state to a model Parameters: Name Type Description Default model _ModelType The model that will be used to load the state required state_key StateKey The state key identifier required","title":"state_downloader()"}]}